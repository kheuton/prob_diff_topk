{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_K(y_pred_BD, y_true_BD, K=4, perturbed_top_K_func=None):\n",
    "\n",
    "    top_K_ids_BKD = perturbed_top_K_func(y_pred_BD)\n",
    "    # Sum over k dim\n",
    "    top_K_ids_BD = top_K_ids_BKD.sum(dim=-2)\n",
    "\n",
    "    true_top_K_val_BD, _  = torch.topk(y_true_BD, K) \n",
    "    denominator_B = torch.sum(true_top_K_val_BD, dim=-1)\n",
    "    numerator_B = torch.sum(top_K_ids_BD * y_true_BD, dim=-1)\n",
    "    bpr_B = numerator_B/denominator_B\n",
    "\n",
    "    return bpr_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedTopK(nn.Module):\n",
    "\n",
    "    def __init__(self, k: int, num_samples: int = 500, sigma: float = 0.05):\n",
    "        super(PerturbedTopK, self).__init__()\n",
    "    \n",
    "        self.num_samples = num_samples\n",
    "        self.sigma = sigma\n",
    "        self.k = k\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Return the output of the PerturbedTopKFunction, applied to the input tensor\n",
    "        # using the k, num_samples, and sigma attributes as arguments\n",
    "        return PerturbedTopKFunction.apply(x, self.k, self.num_samples, self.sigma)\n",
    "\n",
    "\n",
    "class PerturbedTopKFunction(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, k: int, num_samples: int = 500, sigma: float = 0.05):\n",
    "    \n",
    "        b, d = x.shape\n",
    "        \n",
    "        # Generate Gaussian noise with specified number of samples and standard deviation\n",
    "        noise = torch.normal(mean=0.0, std=1.0, size=(b, num_samples, d)).to(x.device)\n",
    "\n",
    "        # Add noise to the input tensor\n",
    "        perturbed_x = x[:, None, :] + noise * sigma # b, nS, d\n",
    "        \n",
    "        # Perform top-k pooling on the perturbed tensor\n",
    "        topk_results = torch.topk(perturbed_x, k=k, dim=-1, sorted=False)\n",
    "        \n",
    "        # Get the indices of the top k elements\n",
    "        indices = topk_results.indices # b, nS, k\n",
    "        \n",
    "        # Sort the indices in ascending order\n",
    "        indices = torch.sort(indices, dim=-1).values # b, nS, k\n",
    "\n",
    "        # Convert the indices to one-hot tensors\n",
    "        perturbed_output = torch.nn.functional.one_hot(indices, num_classes=d).float()\n",
    "        \n",
    "        # Average the one-hot tensors to get the final output\n",
    "        indicators = perturbed_output.mean(dim=1) # b, k, d\n",
    "\n",
    "        # Save constants and tensors for backward pass\n",
    "        ctx.k = k\n",
    "        ctx.num_samples = num_samples\n",
    "        ctx.sigma = sigma\n",
    "\n",
    "        ctx.perturbed_output = perturbed_output\n",
    "        ctx.noise = noise\n",
    "\n",
    "        return indicators\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # If there is no gradient to backpropagate, return tuple of None values\n",
    "        if grad_output is None:\n",
    "            return tuple([None] * 5)\n",
    "\n",
    "        noise_gradient = ctx.noise\n",
    "        \n",
    "        # Calculate expected gradient\n",
    "        expected_gradient = (\n",
    "            torch.einsum(\"bnkd,bne->bkde\", ctx.perturbed_output, noise_gradient)\n",
    "            / ctx.num_samples\n",
    "            / ctx.sigma\n",
    "        ) * float(ctx.k)\n",
    "        \n",
    "        grad_input = torch.einsum(\"bkd,bkde->be\", grad_output, expected_gradient)\n",
    "        \n",
    "        return (grad_input,) + tuple([None] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dimension\n",
    "B = 100\n",
    "# feature dimension\n",
    "D = 10\n",
    "# top-K\n",
    "K=4\n",
    "rating_BD = torch.rand(B, D)\n",
    "rating_BD.requires_grad_(True)\n",
    "labels_BD = torch.rand(B, D)\n",
    "labels_BD.requires_grad_(True)\n",
    "perturbed_top_K_func = PerturbedTopK(k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(rating_BD):\n",
    "    \"\"\"Compute loss. Labels are not an input so we can just get gradient of rating\"\"\"\n",
    "    return -bpr_K(rating_BD, labels_BD, perturbed_top_K_func=perturbed_top_K_func, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tensor = loss(rating_BD)\n",
    "loss_val = torch.sum(loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_loss_TD = rating_BD.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_loss_TD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "vmap: We do not yet support calling random operations inside of vmap. Please perform random operations outside of vmap as a workaround",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Goal: gradient of loss w.r.t. rating_BD for each B separately\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m grad_loss_TD \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrating_BD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward-mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:668\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_graph:\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.autograd.functional.jacobian: `create_graph=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand `strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` are not supported together (yet). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either set `create_graph=False` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jacfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    671\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:545\u001b[0m, in \u001b[0;36m_jacfwd\u001b[0;34m(func, inputs, strict, vectorize)\u001b[0m\n\u001b[1;32m    542\u001b[0m         output_info\u001b[38;5;241m.\u001b[39mappend(primal_outs)\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(jv)\n\u001b[0;32m--> 545\u001b[0m outputs_before_split \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m output_info\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Step 3: for each of the output tangents, split along dim 0\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/_vmap_internals.py:223\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    221\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    225\u001b[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:530\u001b[0m, in \u001b[0;36m_jacfwd.<locals>.jvp\u001b[0;34m(tangents)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fwAD\u001b[38;5;241m.\u001b[39mdual_level():\n\u001b[1;32m    525\u001b[0m     dual_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    526\u001b[0m         fwAD\u001b[38;5;241m.\u001b[39mmake_dual(\u001b[38;5;28minput\u001b[39m, tangent\u001b[38;5;241m.\u001b[39mview_as(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, tangent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, tangents)\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m     _is_outputs_tuple, dual_outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[0;32m--> 530\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdual_inputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m     output_info\u001b[38;5;241m.\u001b[39mappend(_is_outputs_tuple)\n\u001b[1;32m    533\u001b[0m     jv \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(rating_BD)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(rating_BD):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss. Labels are not an input so we can just get gradient of rating\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43mbpr_K\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrating_BD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_BD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed_top_K_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturbed_top_K_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mbpr_K\u001b[0;34m(y_pred_BD, y_true_BD, K, perturbed_top_K_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbpr_K\u001b[39m(y_pred_BD, y_true_BD, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, perturbed_top_K_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     top_K_ids_BKD \u001b[38;5;241m=\u001b[39m \u001b[43mperturbed_top_K_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_BD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Sum over k dim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     top_K_ids_BD \u001b[38;5;241m=\u001b[39m top_K_ids_BKD\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mPerturbedTopK.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Return the output of the PerturbedTopKFunction, applied to the input tensor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# using the k, num_samples, and sigma attributes as arguments\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPerturbedTopKFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mPerturbedTopKFunction.forward\u001b[0;34m(ctx, x, k, num_samples, sigma)\u001b[0m\n\u001b[1;32m     21\u001b[0m b, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate Gaussian noise with specified number of samples and standard deviation\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Add noise to the input tensor\u001b[39;00m\n\u001b[1;32m     27\u001b[0m perturbed_x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m+\u001b[39m noise \u001b[38;5;241m*\u001b[39m sigma \u001b[38;5;66;03m# b, nS, d\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vmap: We do not yet support calling random operations inside of vmap. Please perform random operations outside of vmap as a workaround"
     ]
    }
   ],
   "source": [
    "# Goal: gradient of loss w.r.t. rating_BD for each B separately\n",
    "grad_loss_TD = torch.autograd.functional.jacobian(loss, (rating_BD), vectorize=True,  strategy='forward-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
