{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 17:16:25.900711: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-07 17:16:25.942833: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-07 17:16:25.942858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-07 17:16:25.943863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 17:16:25.951315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-07 17:16:27.455063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "# add code directory to path\n",
    "import sys\n",
    "sys.path.append('/cluster/home/kheuto01/code/prob_diff_topk')\n",
    "\n",
    "from metrics import top_k_onehot_indicator\n",
    "from torch_perturb.perturbations import perturbed\n",
    "from torch_models import NegativeBinomialRegressionModel,NegativeBinomialDebug, torch_bpr_uncurried, deterministic_bpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_df_to_3d_array(df):\n",
    "    # Ensure the DataFrame has a MultiIndex with 'geoid' and 'timestep'\n",
    "    if not isinstance(df.index, pd.MultiIndex) or set(df.index.names) != {'geoid', 'timestep'}:\n",
    "        raise ValueError(\"DataFrame must have a MultiIndex with levels 'geoid' and 'timestep'\")\n",
    "\n",
    "    # Get unique geoids and timesteps, sorted\n",
    "    geoids = sorted(df.index.get_level_values('geoid').unique())\n",
    "    timesteps = sorted(df.index.get_level_values('timestep').unique())\n",
    "\n",
    "    # Create a mapping of geoids to indices\n",
    "    geoid_to_idx = {geoid: idx for idx, geoid in enumerate(geoids)}\n",
    "\n",
    "    # Initialize the 3D array\n",
    "    num_timesteps = len(timesteps)\n",
    "    num_locations = len(geoids)\n",
    "    num_features = len(df.columns)\n",
    "    X = np.zeros((num_timesteps, num_locations, num_features))\n",
    "\n",
    "    # Fill the 3D array\n",
    "    for (geoid, timestep), row in df.iterrows():\n",
    "        t_idx = timesteps.index(timestep)\n",
    "        g_idx = geoid_to_idx[geoid]\n",
    "        X[t_idx, g_idx, :] = row.values\n",
    "\n",
    "    return X, geoids, timesteps\n",
    "\n",
    "def convert_y_df_to_2d_array(y_df, geoids, timesteps):\n",
    "    # Ensure the DataFrame has a MultiIndex with 'geoid' and 'timestep'\n",
    "    if not isinstance(y_df.index, pd.MultiIndex) or set(y_df.index.names) != {'geoid', 'timestep'}:\n",
    "        raise ValueError(\"DataFrame must have a MultiIndex with levels 'geoid' and 'timestep'\")\n",
    "\n",
    "    # Initialize the 2D array\n",
    "    num_timesteps = len(timesteps)\n",
    "    num_locations = len(geoids)\n",
    "    y = np.zeros((num_timesteps, num_locations))\n",
    "\n",
    "    # Create a mapping of geoids to indices\n",
    "    geoid_to_idx = {geoid: idx for idx, geoid in enumerate(geoids)}\n",
    "\n",
    "    # Fill the 2D array\n",
    "    for (geoid, timestep), value in y_df.iloc[:, 0].items():\n",
    "        t_idx = timesteps.index(timestep)\n",
    "        g_idx = geoid_to_idx[geoid]\n",
    "        y[t_idx, g_idx] = value\n",
    "\n",
    "    return y\n",
    "\n",
    "def evaluate_model(model, X, y, time, K, M_score_func, perturbed_top_K_func):\n",
    "    \"\"\"Evaluate model on given data and return metrics.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        dist = model(X, time)\n",
    "        \n",
    "        # Sample and calculate ratio ratings\n",
    "        y_sample_TMS = dist.sample((M_score_func,)).permute(1, 0, 2)\n",
    "        ratio_rating_TMS = y_sample_TMS/y_sample_TMS.sum(dim=-1, keepdim=True)\n",
    "        ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        nll = -model.log_likelihood(y, X, time)\n",
    "        perturbed_bpr_T = torch_bpr_uncurried(ratio_rating_TS, y, K=K, \n",
    "                                             perturbed_top_K_func=perturbed_top_K_func)\n",
    "        deterministic_bpr_T = deterministic_bpr(ratio_rating_TS, y, K=K)\n",
    "        \n",
    "        metrics = {\n",
    "            'nll': nll.item(),\n",
    "            'perturbed_bpr': torch.mean(perturbed_bpr_T).item(),\n",
    "            'deterministic_bpr': torch.mean(deterministic_bpr_T).item()\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "def train_epoch_neg_binom(model, optimizer, K, threshold,\n",
    "                         M_score_func, feat_TSF,\n",
    "                         time_T, train_y_TS,\n",
    "                         perturbed_top_K_func, bpr_weight, nll_weight, update=True):\n",
    "    \"\"\"Train one epoch of the negative binomial model.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    dist = model(feat_TSF, time_T)\n",
    "    \n",
    "    y_sample_TMS = dist.sample((M_score_func,)).permute(1, 0, 2)\n",
    "    y_sample_action_TMS = y_sample_TMS\n",
    "\n",
    "    ratio_rating_TMS = y_sample_action_TMS/y_sample_action_TMS.sum(dim=-1, keepdim=True)\n",
    "    ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "    ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "    def get_log_probs_baked(param):\n",
    "        distribution = model.build_from_single_tensor(param, feat_TSF, time_T)\n",
    "        log_probs_TMS = distribution.log_prob(y_sample_TMS.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "        return log_probs_TMS\n",
    "    \n",
    "    jac_TMSP = torch.autograd.functional.jacobian(get_log_probs_baked, \n",
    "                                                (model.params_to_single_tensor()), \n",
    "                                                strategy='forward-mode', \n",
    "                                                vectorize=True)\n",
    "\n",
    "    score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "    score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)    \n",
    "\n",
    "    positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS), \n",
    "                                        K=K, perturbed_top_K_func=perturbed_top_K_func)\n",
    "    \n",
    "    if nll_weight > 0:\n",
    "        bpr_threshold_diff_T = positive_bpr_T - threshold\n",
    "        violate_threshold_flag = bpr_threshold_diff_T < 0\n",
    "        negative_bpr_loss = torch.mean(-bpr_threshold_diff_T*violate_threshold_flag)\n",
    "    else:\n",
    "        negative_bpr_loss = torch.mean(-positive_bpr_T)\n",
    "    \n",
    "    nll = -model.log_likelihood(train_y_TS, feat_TSF, time_T)\n",
    "    loss = bpr_weight*negative_bpr_loss + nll_weight*nll\n",
    "    loss.backward()\n",
    "\n",
    "    loss_grad_TS = ratio_rating_TS.grad\n",
    "    print(f'Params: {[param for param in model.parameters()]}')\n",
    "    print(f'Score func estimator: {score_func_estimator_TSP}')\n",
    "    print(f'Loss grad: {loss_grad_TS}')\n",
    "\n",
    "    gradient_TSP = score_func_estimator_TSP * torch.unsqueeze(loss_grad_TS, -1)\n",
    "    gradient_P = torch.sum(gradient_TSP, dim=[0,1])\n",
    "    gradient_tuple = model.single_tensor_to_params(gradient_P)\n",
    "\n",
    "    for param, gradient in zip(model.parameters(), gradient_tuple):\n",
    "        if nll_weight > 0:\n",
    "            gradient = gradient + param.grad\n",
    "        param.grad = gradient\n",
    "        \n",
    "    if update:\n",
    "        optimizer.step()\n",
    "\n",
    "    deterministic_bpr_T = deterministic_bpr(ratio_rating_TS, torch.tensor(train_y_TS), K=K)\n",
    "    det_bpr = torch.mean(deterministic_bpr_T)\n",
    "\n",
    "    metrics = {\n",
    "        'loss': loss.detach().item(),\n",
    "        'deterministic_bpr': det_bpr.item(),\n",
    "        'perturbed_bpr': torch.mean(positive_bpr_T).item(),\n",
    "        'nll': nll.item()\n",
    "    }\n",
    "\n",
    "    return metrics, model\n",
    "\n",
    "def main(K=None, step_size=None, epochs=None, bpr_weight=None,\n",
    "         nll_weight=None, seed=None, outdir=None, threshold=None,\n",
    "         perturbed_noise=None, num_score_samples=None, num_pert_samples=None,\n",
    "         data_dir=None, device='cuda', val_freq=10):\n",
    "    \"\"\"Main training loop with command line arguments.\"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Load training data\n",
    "    train_X_df = pd.read_csv(os.path.join(data_dir, 'train_x.csv'), index_col=[0,1])\n",
    "    train_Y_df = pd.read_csv(os.path.join(data_dir, 'train_y.csv'), index_col=[0,1])\n",
    "    \n",
    "    # Load validation data\n",
    "    val_X_df = pd.read_csv(os.path.join(data_dir, 'valid_x.csv'), index_col=[0,1])\n",
    "    val_Y_df = pd.read_csv(os.path.join(data_dir, 'valid_y.csv'), index_col=[0,1])\n",
    "    \n",
    "    # Process training data\n",
    "    train_X, geoids, timesteps = convert_df_to_3d_array(train_X_df)#.drop(columns='timestep.1'))\n",
    "    train_time_arr = np.array([timesteps] * len(geoids)).T\n",
    "    train_y = convert_y_df_to_2d_array(train_Y_df, geoids, timesteps)\n",
    "\n",
    "    # Process validation data\n",
    "    val_X, _, val_timesteps = convert_df_to_3d_array(val_X_df)#.drop(columns='timestep.1'))\n",
    "    val_time_arr = np.array([val_timesteps] * len(geoids)).T\n",
    "    val_y = convert_y_df_to_2d_array(val_Y_df, geoids, val_timesteps)\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    X_train = torch.tensor(train_X, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_y, dtype=torch.float32).to(device)\n",
    "    time_train = torch.tensor(train_time_arr, dtype=torch.float32).to(device)\n",
    "    \n",
    "    X_val = torch.tensor(val_X, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(val_y, dtype=torch.float32).to(device)\n",
    "    time_val = torch.tensor(val_time_arr, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = NegativeBinomialRegressionModel(\n",
    "        num_locations=len(geoids),\n",
    "        num_fixed_effects=train_X.shape[2], device=device\n",
    "    ).to(device)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "    # Setup top-k function\n",
    "    top_k_func = partial(top_k_onehot_indicator, k=K)\n",
    "    perturbed_top_K_func = perturbed(top_k_func, sigma=perturbed_noise, num_samples=num_pert_samples)\n",
    "\n",
    "    # Initialize metric tracking with separate epoch tracking for validation\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'epochs': [], \n",
    "            'loss': [], \n",
    "            'nll': [], \n",
    "            'perturbed_bpr': [], \n",
    "            'deterministic_bpr': []\n",
    "        },\n",
    "        'val': {\n",
    "            'epochs': [], \n",
    "            'nll': [], \n",
    "            'perturbed_bpr': [], \n",
    "            'deterministic_bpr': []\n",
    "        },\n",
    "        'times': []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'EPOCH: {epoch}')\n",
    "        start = time.time()\n",
    "        \n",
    "        # Training step\n",
    "        train_metrics, model = train_epoch_neg_binom(\n",
    "            model, optimizer, K, threshold,\n",
    "            num_score_samples, X_train, time_train,\n",
    "            y_train, perturbed_top_K_func,\n",
    "            bpr_weight, nll_weight, device\n",
    "        )\n",
    "        \n",
    "        # Update training metrics\n",
    "        metrics['train']['epochs'].append(epoch)\n",
    "        for metric, value in train_metrics.items():\n",
    "            metrics['train'][metric].append(value)\n",
    "        \n",
    "        # Validation step (every val_freq epochs)\n",
    "        if epoch % val_freq == 0:\n",
    "            model.eval()\n",
    "            val_metrics = evaluate_model(\n",
    "                model, X_val, y_val, time_val,\n",
    "                K, num_score_samples, perturbed_top_K_func\n",
    "            )\n",
    "            \n",
    "            # Update validation metrics\n",
    "            metrics['val']['epochs'].append(epoch)\n",
    "            for metric, value in val_metrics.items():\n",
    "                metrics['val'][metric].append(value)\n",
    "            \n",
    "            # Save best model\n",
    "            val_loss = val_metrics['nll'] * nll_weight\n",
    "            if bpr_weight > 0:\n",
    "                val_loss -= val_metrics['perturbed_bpr'] * bpr_weight\n",
    "                \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.exists(outdir):\n",
    "                    os.makedirs(outdir)\n",
    "                torch.save(model.state_dict(), f'{outdir}/best_model.pth')\n",
    "        \n",
    "        end = time.time()\n",
    "        metrics['times'].append(end - start)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, NLL: {train_metrics['nll']:.4f}, \"\n",
    "              f\"BPR: {train_metrics['deterministic_bpr']:.4f}\")\n",
    "        if epoch % val_freq == 0:\n",
    "            print(f\"Val - NLL: {val_metrics['nll']:.4f}, \"\n",
    "                  f\"BPR: {val_metrics['deterministic_bpr']:.4f}\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 100 == 0:\n",
    "            if not os.path.exists(outdir):\n",
    "                os.makedirs(outdir)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, f'{outdir}/checkpoint.pth')\n",
    "            \n",
    "            # Save metrics separately for easier analysis\n",
    "            # Create DataFrames with proper indexing\n",
    "            train_df = pd.DataFrame(metrics['train']).set_index('epochs')\n",
    "            val_df = pd.DataFrame(metrics['val']).set_index('epochs')\n",
    "            times_df = pd.DataFrame({'times': metrics['times']}, index=range(len(metrics['times'])))\n",
    "            \n",
    "            train_df.to_csv(f'{outdir}/train_metrics.csv')\n",
    "            val_df.to_csv(f'{outdir}/val_metrics.csv')\n",
    "            times_df.to_csv(f'{outdir}/time_metrics.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_nll_model = '/cluster/tufts/hugheslab/kheuto01/opioid_grid_try_fix_params/MA/K100_bw30_nw1_ss0.001_nss100_nps100_seed123_sig0.001_tr0.5'\n",
    "data_dir = '/cluster/tufts/hugheslab/datasets/NSF_OD/cleaned/MA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "bpr_weight = 30\n",
    "nll_weight = 1\n",
    "step_size = 0.001\n",
    "num_score_samples = 100\n",
    "num_pert_samples = 100\n",
    "seed = 123\n",
    "perturbed_noise = 0.001\n",
    "threshold = 0.5\n",
    "epochs = 1\n",
    "outdir = '/cluster/tufts/hugheslab/kheuto01/debug'\n",
    "data_dir = '/cluster/tufts/hugheslab/datasets/NSF_OD/cleaned/MA'\n",
    "device = 'cuda'\n",
    "val_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_X_df = pd.read_csv(os.path.join(data_dir, 'train_x.csv'), index_col=[0,1])\n",
    "train_Y_df = pd.read_csv(os.path.join(data_dir, 'train_y.csv'), index_col=[0,1])\n",
    "#temp mem fix\n",
    "#train_X_df = train_X_df.iloc[:1620]\n",
    "#train_Y_df = train_Y_df.iloc[:1620]\n",
    "\n",
    "# Load validation data\n",
    "val_X_df = pd.read_csv(os.path.join(data_dir, 'valid_x.csv'), index_col=[0,1])\n",
    "val_Y_df = pd.read_csv(os.path.join(data_dir, 'valid_y.csv'), index_col=[0,1])\n",
    "\n",
    "# Process training data\n",
    "train_X, geoids, timesteps = convert_df_to_3d_array(train_X_df)#.drop(columns='timestep.1'))\n",
    "train_time_arr = np.array([timesteps] * len(geoids)).T\n",
    "train_y = convert_y_df_to_2d_array(train_Y_df, geoids, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6480, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = os.path.join(good_nll_model, 'best_model.pth')\n",
    "\n",
    "    \n",
    "# Initialize model with correct parameters\n",
    "\n",
    "\n",
    "# Load saved weights\n",
    "#model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train_X, dtype=torch.float32).to(device)\n",
    "# temp mem fix\n",
    "#X_train = X_train[:2,:,:]\n",
    "#timesteps = X_train.shape[0]\n",
    "train_time_arr = np.array([timesteps] * len(geoids)).T\n",
    "time_train = torch.tensor(train_time_arr, dtype=torch.float32).to(device)\n",
    "num_score_samples = 100\n",
    "num_pert_samples = 100\n",
    "\n",
    "model = NegativeBinomialDebug(\n",
    "    num_locations=len(geoids),\n",
    "    num_fixed_effects=X_train.shape[2], device=device\n",
    ").to(device)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1620])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unconstrained theta: Parameter containing:\n",
      "tensor([-1.8628], device='cuda:0', requires_grad=True)\n",
      "Theta: tensor([0.1344], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dist = model(X_train, time_train)\n",
    "\n",
    "y_sample_TMS = dist.sample((num_score_samples,)).permute(1, 0, 2)\n",
    "y_sample_action_TMS = y_sample_TMS\n",
    "\n",
    "ratio_rating_TMS = y_sample_action_TMS/y_sample_action_TMS.sum(dim=-1, keepdim=True)\n",
    "ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "def get_log_probs_baked(param):\n",
    "    distribution = model.build_from_single_tensor(param, X_train, time_train)\n",
    "    log_probs_TMS = distribution.log_prob(y_sample_TMS.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "    return log_probs_TMS\n",
    "\n",
    "jac_TMSP = torch.autograd.functional.jacobian(get_log_probs_baked, \n",
    "                                            (model.params_to_single_tensor()), \n",
    "                                            strategy='forward-mode', \n",
    "                                            vectorize=True)\n",
    "\n",
    "score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-28.1058, device='cuda:0', grad_fn=<MinBackward1>),\n",
       " tensor(35.1680, device='cuda:0', grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac_TMSP.min(), jac_TMSP.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample_TMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample_action_TMS.sum(dim=-1, keepdim=True).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
