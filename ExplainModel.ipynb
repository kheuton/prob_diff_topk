{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 14:13:12.451792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 14:13:12.609203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-13 14:13:12.609246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-13 14:13:12.610512: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-13 14:13:12.634727: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 14:13:19.435034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "\n",
    "from datasets import example_datasets, to_numpy\n",
    "from models import mixture_poissons,location_specific_linear, CustomPenalizedMixtureDecisionModel, get_mixture\n",
    "from metrics import mixture_poi_loss, get_bpr_loss_func, mix_bpr, get_penalized_bpr_loss_func_mix, cross_ratio_decision, get_perturbed_bpr_func\n",
    "from experiments import training_loop, training_loop_score_function_trick, score_function_trick, overall_gradient_calculation\n",
    "from plotting_funcs import plot_losses, plot_frontier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import resource\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=360\n",
    "num_components=4\n",
    "learning_rate = 0.05\n",
    "epochs=200\n",
    "outdir = '/cluster/home/kheuto01/testdir'\n",
    "penalty = 5000\n",
    "threshold = 0.55\n",
    "K=4\n",
    "do_only=True\n",
    "# tracts/distributions\n",
    "S=12\n",
    "# history/features\n",
    "H = 3\n",
    "# total timepoints\n",
    "T= 500\n",
    "perturbed_sigma=0.3\n",
    "num_score_func_samples=10\n",
    "batch_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = example_datasets(H, T, seed=seed, batch_size=batch_size)\n",
    "train_X_THS, train_y_TS = to_numpy(train_dataset)\n",
    "val_X_THS, val_y_TS = to_numpy(val_dataset)\n",
    "input_shape = (H,S)\n",
    "\n",
    "bpr_K = get_perturbed_bpr_func(K, sigma=perturbed_sigma)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5561111], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPR should be (1+2+3+4)/(3+4+5+6) = 10/18 = 0.5555\n",
    "bpr_K(np.array([[1,2,3,4,5,6]], dtype=np.float32), np.array([[6,5,4,3,2,1]], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_specific_linear(input_shape, seed=360, activation='softplus'):\n",
    "    keras.utils.set_random_seed(seed)\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(name='linear_input', shape=input_shape),\n",
    "            # convolution to turn H,S into 1,S\n",
    "            # Filters = size oute convolutiput space\n",
    "            # kernel_size = size of thon window\n",
    "            # dataformat = channels_first means that the input shape is (batch_size, features, time)\n",
    "            keras.layers.Conv1D(name='linear_convolution', filters=1, kernel_size=1, activation=activation, data_format='channels_first'),\n",
    "            keras.layers.Flatten(name='linnear_flatten'),\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureWeights(keras.layers.Layer):\n",
    "    \"\"\"Dumb layer that just returns mixture weights\n",
    "    Constrained to unit norm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_locations, num_components=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = self.add_weight(name='shared_mix_weights',\n",
    "            shape=(num_locations, num_components ),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        self.softmax = keras.layers.Softmax(axis=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.softmax(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixture(model, input_shape, num_components=2, seed=360):\n",
    "\n",
    "    num_features, num_locations = input_shape\n",
    "\n",
    "    member_models = []\n",
    "    for c in range(num_components):\n",
    "        member_models.append(model(input_shape, seed=seed+1000*c))\n",
    "\n",
    "    # Define layers\n",
    "    inputs = keras.Input(shape=input_shape, name='mix_input')\n",
    "    reshape_layer = keras.layers.Reshape(name='mix_reshape', target_shape=(-1,1))\n",
    "    concat_layer = keras.layers.Concatenate(name='mix_concat',axis=-1)\n",
    "    add_layer = keras.layers.Add(name='add_const')\n",
    "\n",
    "    mixture_weight_layer = MixtureWeights(num_locations, name='mixture_weights', num_components=num_components)\n",
    "\n",
    "    # Reshape results of member models to add component dimension\n",
    "    reshaped = [reshape_layer(member(inputs)) for member in member_models]\n",
    "    # Concatenate along component dimension\n",
    "    concatted = concat_layer(reshaped)\n",
    "    # add a constant so rate is always positive\n",
    "    added = add_layer([concatted, tf.constant([1e-13])])\n",
    "    \n",
    "    # obtain mixture weights\n",
    "    mixture_weights = mixture_weight_layer(inputs)\n",
    "\n",
    "    # Get a mixture of poissons\n",
    "    mixture_distribution_layer = tfp.layers.DistributionLambda(lambda params: \n",
    "        tfp.distributions.MixtureSameFamily(mixture_distribution=tfp.distributions.Categorical(probs=params[0]),\n",
    "                                            components_distribution = tfp.distributions.Poisson(rate=params[1], validate_args=True)))\n",
    "    \n",
    "    outputs = mixture_distribution_layer([mixture_weights, added])\n",
    "\n",
    "    model = keras.Model(name='mix_model',inputs=inputs, outputs=[outputs])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = get_mixture(location_specific_linear, input_shape, num_components=num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3, 12)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just 50 rows\n",
    "example_X = train_X_THS[:50]\n",
    "example_y = train_y_TS[:50]\n",
    "example_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_distribution = model(example_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape (10, 50, 12) is sample-by-batch-by-location\n"
     ]
    }
   ],
   "source": [
    "sample_y = mixture_distribution.sample(10)\n",
    "print(f'Sample shape {sample_y.shape} is sample-by-batch-by-location')\n",
    "log_probs = mixture_distribution.log_prob(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function_trick(jacobian_MBSp, decision_MBS):\n",
    "\n",
    "    # every parameter has some shape in addition to their batch and location dimension.\n",
    "    # Find that shape, and add axes to the decision gradient to match\n",
    "    num_param_dims = tf.rank(jacobian_MBSp)-3\n",
    "    # expand decision to match jacobian\n",
    "    num_param_dims_tf = tf.cast(num_param_dims, tf.int32)\n",
    "    new_shape = tf.concat([tf.shape(decision_MBS), tf.ones([num_param_dims_tf], tf.int32)], axis=0)\n",
    "    decision_MBSp=tf.reshape(decision_MBS, new_shape)\n",
    "\n",
    "    # scale gradient by decision\n",
    "    scaled_jacobian_MBSp = jacobian_MBSp*decision_MBSp\n",
    "\n",
    "    # average over sample dims\n",
    "    param_gradient_BSp = tf.reduce_mean(scaled_jacobian_MBSp, axis=0)\n",
    "\n",
    "    return param_gradient_BSp\n",
    "\n",
    "\n",
    "\n",
    "def overall_gradient_calculation(gradient_BSp, decision_gradient_BS):\n",
    "\n",
    "    # every parameter has some shape in addition to their batch and location dimension.\n",
    "    # Find that shape, and add axes to the decision gradient to match\n",
    "    num_param_dims = tf.rank(gradient_BSp)-2\n",
    "    num_param_dims_tf = tf.cast(num_param_dims, tf.int32)\n",
    "    new_shape = tf.concat([tf.shape(decision_gradient_BS), tf.ones([num_param_dims_tf], tf.int32)], axis=0)\n",
    "    decision_gradient_BSp=tf.reshape(decision_gradient_BS, new_shape)\n",
    "\n",
    "    # scale gradient by decision gradient\n",
    "    overall_gradient_BSp = gradient_BSp*decision_gradient_BSp\n",
    "\n",
    "    # sum over batch and location\n",
    "    overall_gradient = tf.reduce_sum(overall_gradient_BSp, axis=[0,1])\n",
    "\n",
    "    return overall_gradient\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 2 tapes, 1 to track jacobian of samples w.r.t model\n",
    "# other for gradient of loss w.r.t decision\n",
    "with tf.GradientTape() as jacobian_tape, tf.GradientTape() as loss_tape:\n",
    "    mixture = model(example_X)\n",
    "\n",
    "    sample_y_MBS = mixture.sample(num_score_func_samples)+1e-9\n",
    "    sample_log_probs_MBS = mixture.log_prob(sample_y_MBS)\n",
    "\n",
    "    sample_decisions_MBS = cross_ratio_decision(sample_y_MBS)\n",
    "    expected_decisions_BS = tf.reduce_mean(sample_decisions_MBS, axis=0)\n",
    "\n",
    "    bpr_B = bpr_K(example_y, expected_decisions_BS)\n",
    "    observed_log_prob_BS = mixture.log_prob(example_y)\n",
    "\n",
    "    loss_B = -tf.reduce_sum(observed_log_prob_BS, axis=-1)\n",
    "    violate_threshold_flag_B = tf.cast(tf.greater(threshold, bpr_B),\n",
    "                                       tf.float32)\n",
    "    loss_B += penalty * violate_threshold_flag_B *(threshold - bpr_B)\n",
    "\n",
    "jacobian_pMBS = jacobian_tape.jacobian(sample_log_probs_MBS, model.trainable_weights)\n",
    "param_gradient_pBS = [score_function_trick(j, sample_decisions_MBS) for j in jacobian_pMBS]\n",
    "\n",
    "loss_gradients_BS = loss_tape.gradient(loss_B, expected_decisions_BS)\n",
    "overall_gradient = [overall_gradient_calculation(g, loss_gradients_BS) for g in param_gradient_pBS]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
       " array([[[1525.3921  ],\n",
       "         [  13.128357],\n",
       "         [ -82.121086]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([84.285675], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
       " array([[[-1427.94 ],\n",
       "         [-4536.369],\n",
       "         [-2829.64 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([59.452374], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
       " array([[[-2886.3953],\n",
       "         [-1164.0221],\n",
       "         [ 4933.247 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-85.699356], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 3, 1), dtype=float32, numpy=\n",
       " array([[[-7.950815e-02],\n",
       "         [-6.704994e+02],\n",
       "         [-9.544615e-03]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([44.94523], dtype=float32)>,\n",
       " <tf.Tensor: shape=(12, 4), dtype=float32, numpy=\n",
       " array([[ 129.37689 , -217.15352 ,  -42.850838,  130.62746 ],\n",
       "        [  86.048386, -199.55031 ,   25.239403,   88.26255 ],\n",
       "        [ 109.23879 ,  -89.27162 , -129.62595 ,  109.65878 ],\n",
       "        [ 128.6852  ,  -79.743256, -176.84067 ,  127.89873 ],\n",
       "        [ -66.85319 ,  273.41144 , -106.31441 , -100.243835],\n",
       "        [-132.27605 ,  318.67007 ,  -62.5112  , -123.88291 ],\n",
       "        [ 253.43181 , -281.2907  , -220.09123 ,  247.95026 ],\n",
       "        [ -33.24097 ,  239.55492 , -163.83824 ,  -42.47567 ],\n",
       "        [-298.10788 ,  609.68207 ,  -35.782368, -275.79193 ],\n",
       "        [-303.06757 ,  638.7958  ,  175.70935 , -511.43768 ],\n",
       "        [-268.90674 ,  451.43726 ,  152.78674 , -335.31726 ],\n",
       "        [-355.44217 ,  858.23413 ,   12.555189, -515.34717 ]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k2_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
