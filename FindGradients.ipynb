{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:16:35.456275: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-12 14:16:35.499980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-12 14:16:35.500009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-12 14:16:35.501076: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-12 14:16:35.508539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-12 14:16:37.937597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "\n",
    "from datasets import example_datasets, to_numpy\n",
    "from models import mixture_poissons,location_specific_linear, CustomPenalizedMixtureDecisionModel\n",
    "from metrics import mixture_poi_loss, get_bpr_loss_func, mix_bpr, get_penalized_bpr_loss_func_mix, cross_ratio_decision, get_perturbed_bpr_func\n",
    "from experiments import training_loop, training_loop_score_function_trick, score_function_trick, overall_gradient_calculation\n",
    "from plotting_funcs import plot_losses, plot_frontier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import resource\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=360\n",
    "num_components=4\n",
    "learning_rate = 0.05\n",
    "epochs=200\n",
    "outdir = '/cluster/home/kheuto01/testdir'\n",
    "penalty = 5000\n",
    "threshold = 0.55\n",
    "K=4\n",
    "do_only=True\n",
    "# tracts/distributions\n",
    "S=12\n",
    "# history/features\n",
    "H = 3\n",
    "# total timepoints\n",
    "T= 500\n",
    "perturbed_sigma=0.1\n",
    "num_score_func_samples=5\n",
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:16:43.938036: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-05-12 14:16:43.938070: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: s1cmp008.pax.tufts.edu\n",
      "2024-05-12 14:16:43.938078: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: s1cmp008.pax.tufts.edu\n",
      "2024-05-12 14:16:43.938143: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.129.3\n",
      "2024-05-12 14:16:43.938167: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.129.3\n",
      "2024-05-12 14:16:43.938172: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.129.3\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = example_datasets(H, T, seed=seed, batch_size=batch_size)\n",
    "train_X_THS, train_y_TS = to_numpy(train_dataset)\n",
    "val_X_THS, val_y_TS = to_numpy(val_dataset)\n",
    "input_shape = (H,S)\n",
    "\n",
    "bpr_K = get_perturbed_bpr_func(K, sigma=perturbed_sigma)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, mix_weights  = mixture_poissons(location_specific_linear, input_shape, num_components=num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def build_distribution(prob_params_BSK, mixture_weights_KS):\n",
    "    # my fault that model returns KS instead of SK\n",
    "    mixture_weights_SK = tf.transpose(mixture_weights_KS, perm=[1,0])\n",
    "\n",
    "    #  could create a custom model class that returns \n",
    "    # the appropriate tfp.dist given outputs\n",
    "    mix = tfp.distributions.MixtureSameFamily(\n",
    "        mixture_distribution=tfp.distributions.Categorical(probs=mixture_weights_SK),#+1e-13),\n",
    "        components_distribution = tfp.distributions.Poisson(rate=prob_params_BSK+1e-13))\n",
    "\n",
    "    return mix\n",
    "\n",
    "@tf.function\n",
    "def get_samples(mix, num_score_func_samples):\n",
    "    sample_y_MBS = mix.sample(num_score_func_samples)\n",
    "    return sample_y_MBS\n",
    "\n",
    "@tf.function\n",
    "def get_sample_log_probs(mix, sample_y_MBS):\n",
    "    sample_log_probs_MBS = mix.log_prob(sample_y_MBS)\n",
    "    return sample_log_probs_MBS\n",
    "\n",
    "@tf.function\n",
    "def get_expected_decision(sample_decisions_MBS):\n",
    "    expected_decisions_BS = tf.reduce_mean(sample_decisions_MBS, axis=0)\n",
    "    return expected_decisions_BS\n",
    "\n",
    "@tf.function\n",
    "def get_bpr_B(y_BS, expected_decisions_BS):\n",
    "    bpr_B = bpr_K(y_BS, expected_decisions_BS)\n",
    "    return bpr_B\n",
    "\n",
    "@tf.function\n",
    "def get_observed_log_prob(mix, y_BS):\n",
    "    observed_log_prob_BS = mix.log_prob(y_BS)\n",
    "    return observed_log_prob_BS\n",
    "\n",
    "@tf.function\n",
    "def get_loss(observed_log_prob_BS, threshold, bpr_B):\n",
    "\n",
    "    loss_B = -tf.reduce_sum(observed_log_prob_BS, axis=-1)\n",
    "\n",
    "    violate_threshold_flag_B = tf.cast(tf.greater(threshold,\n",
    "                                                bpr_B),\n",
    "                                        tf.float32)\n",
    "    loss_B += penalty * violate_threshold_flag_B *(threshold - bpr_B)\n",
    "    return loss_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def do_it_all(prob_params_BSK, mixture_weights_KS):\n",
    "    # my fault that model returns KS instead of SK\n",
    "    mixture_weights_SK = tf.transpose(mixture_weights_KS, perm=[1,0])\n",
    "\n",
    "    #  could create a custom model class that returns \n",
    "    # the appropriate tfp.dist given outputs\n",
    "    mix = tfp.distributions.MixtureSameFamily(\n",
    "        mixture_distribution=tfp.distributions.Categorical(probs=mixture_weights_SK),#+1e-13),\n",
    "        components_distribution = tfp.distributions.Poisson(rate=prob_params_BSK+1e-13))\n",
    "\n",
    "    # add constant to avoid log 0\n",
    "    sample_y_MBS = mix.sample(num_score_func_samples)#+1e-13\n",
    "\n",
    "    sample_log_probs_MBS = mix.log_prob(sample_y_MBS)\n",
    "    sample_decisions_MBS = cross_ratio_decision(sample_y_MBS)\n",
    "    expected_decisions_BS = tf.reduce_mean(sample_decisions_MBS, axis=0)\n",
    "    bpr_B = bpr_K(y_BS, expected_decisions_BS)\n",
    "    observed_log_prob_BS = mix.log_prob(y_BS)\n",
    "\n",
    "\n",
    "    loss_B = -tf.reduce_sum(observed_log_prob_BS, axis=-1)\n",
    "\n",
    "    violate_threshold_flag_B = tf.cast(tf.greater(threshold,\n",
    "                                                bpr_B),\n",
    "                                        tf.float32)\n",
    "    loss_B += penalty * violate_threshold_flag_B *(threshold - bpr_B)\n",
    "\n",
    "    return loss_B, sample_log_probs_MBS, sample_decisions_MBS, expected_decisions_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "0\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "WARNING:tensorflow:AutoGraph could not transform <function perturbed.<locals>.wrapper at 0x2aedcaf85f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got partial\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function perturbed.<locals>.wrapper at 0x2aedcaf85f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got partial\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1140668\n",
      "1\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1289532\n",
      "2\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1366392\n",
      "3\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1372984\n",
      "4\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x2aedceba2f20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1380624\n",
      "5\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x2aedcfeb0860> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1386368\n",
      "Loss: 644.0394897460938\n",
      "NLL: -135.14707946777344\n",
      "BPR: 0.5009053349494934\n",
      "Epoch 1\n",
      "0\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1393332\n",
      "1\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1400100\n",
      "2\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1406988\n",
      "3\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1413692\n",
      "4\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1420700\n",
      "5\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1427300\n",
      "Loss: 619.4752807617188\n",
      "NLL: -132.9781951904297\n",
      "BPR: 0.5040485262870789\n",
      "Epoch 2\n",
      "0\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1434024\n",
      "1\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1440648\n",
      "2\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1447604\n",
      "3\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1454748\n",
      "4\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1461380\n",
      "5\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1468172\n",
      "Loss: 560.1986694335938\n",
      "NLL: -129.64108276367188\n",
      "BPR: 0.5236735343933105\n",
      "Epoch 3\n",
      "0\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1475392\n",
      "1\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1482012\n",
      "2\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1488620\n",
      "3\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1495508\n",
      "4\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1502216\n",
      "5\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1508800\n",
      "Loss: 555.2611694335938\n",
      "NLL: -128.12828063964844\n",
      "BPR: 0.5233525633811951\n",
      "Epoch 4\n",
      "0\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1515212\n",
      "1\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1521876\n",
      "2\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1528676\n",
      "3\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1535360\n",
      "4\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1542156\n",
      "5\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1548760\n",
      "Loss: 560.7413940429688\n",
      "NLL: -127.4324951171875\n",
      "BPR: 0.5295074582099915\n",
      "Epoch 5\n",
      "0\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1555672\n",
      "1\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1562300\n",
      "2\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1569464\n",
      "3\n",
      "model evaled\n",
      "got mix\n",
      "got samples\n",
      "got probs of samples\n",
      "got decisions\n",
      "got expected decisions\n",
      "print got bpr\n",
      "before jac\n",
      "after jac\n",
      "before grad\n",
      "after grad\n",
      "grads combined\n",
      "1576352\n",
      "4\n",
      "model evaled\n",
      "got mix\n"
     ]
    }
   ],
   "source": [
    "#check numerics\n",
    "tf.debugging.disable_check_numerics()\n",
    "\n",
    "losses = {}\n",
    "losses['train'] = {}\n",
    "losses['val'] ={}\n",
    "losses['train']['loss']=[]\n",
    "losses['train']['nll']=[]\n",
    "losses['train']['bpr']=[]\n",
    "losses['val']['loss']=[]\n",
    "losses['val']['nll']=[]\n",
    "losses['val']['bpr']=[]\n",
    "verbose=True\n",
    "for epoch in range(10):\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch}')\n",
    "        else:\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}')\n",
    "\n",
    "        batch_losses = {'train': {'loss': [], 'nll': [], 'bpr': []}}\n",
    "\n",
    "        for step, (x_BHS, y_BS) in enumerate(train_dataset):\n",
    "            print(step)\n",
    "            \n",
    "            with tf.GradientTape() as jacobian_tape, tf.GradientTape() as loss_tape:\n",
    "                \n",
    "                prob_params_BSK, mixture_weights_KS = model(x_BHS, training=True)\n",
    "                print('model evaled')\n",
    "\n",
    "                mix = build_distribution(prob_params_BSK, mixture_weights_KS)\n",
    "                print('got mix')\n",
    "                sample_y_MBS = get_samples(mix, num_score_func_samples)\n",
    "                print('got samples')\n",
    "                sample_log_probs_MBS = get_sample_log_probs(mix, sample_y_MBS)\n",
    "                print('got probs of samples')\n",
    "                sample_decisions_MBS = cross_ratio_decision(sample_y_MBS)\n",
    "                print('got decisions')\n",
    "                expected_decisions_BS = get_expected_decision(sample_decisions_MBS)\n",
    "                print('got expected decisions')\n",
    "\n",
    "\n",
    "                bpr_B = get_bpr_B(y_BS, expected_decisions_BS)\n",
    "                print('print got bpr')\n",
    "                observed_log_prob_BS = get_observed_log_prob(mix, y_BS)\n",
    "                loss_B = get_loss(observed_log_prob_BS, threshold, bpr_B)\n",
    "\n",
    "                '''\n",
    "                # my fault that model returns KS instead of SK\n",
    "                mixture_weights_SK = tf.transpose(mixture_weights_KS, perm=[1,0])\n",
    "\n",
    "                #  could create a custom model class that returns \n",
    "                # the appropriate tfp.dist given outputs\n",
    "                mix = tfp.distributions.MixtureSameFamily(\n",
    "                    mixture_distribution=tfp.distributions.Categorical(probs=mixture_weights_SK),#+1e-13),\n",
    "                    components_distribution = tfp.distributions.Poisson(rate=prob_params_BSK+1e-13))\n",
    "\n",
    "                # add constant to avoid log 0\n",
    "                sample_y_MBS = mix.sample(num_score_func_samples)#+1e-13\n",
    "\n",
    "                sample_log_probs_MBS = mix.log_prob(sample_y_MBS)\n",
    "                sample_decisions_MBS = cross_ratio_decision(sample_y_MBS)\n",
    "                expected_decisions_BS = tf.reduce_mean(sample_decisions_MBS, axis=0)\n",
    "                bpr_B = bpr_K(y_BS, expected_decisions_BS)\n",
    "                observed_log_prob_BS = mix.log_prob(y_BS)\n",
    "\n",
    "                loss_B = tf.zeros_like(bpr_B)\n",
    "\n",
    "                if True:\n",
    "                    loss_B -= tf.reduce_sum(observed_log_prob_BS, axis=-1)\n",
    "                if True:\n",
    "                    violate_threshold_flag_B = tf.cast(tf.greater(threshold,\n",
    "                                                                bpr_B),\n",
    "                                                        tf.float32)\n",
    "                    loss_B += penalty * violate_threshold_flag_B *(threshold - bpr_B)\n",
    "                '''\n",
    "\n",
    "            # The lowercase \"p\" signifies that these are lists of length P, P = number of trainable variables\n",
    "            print('before jac')\n",
    "            jacobian_pMBS = jacobian_tape.jacobian(sample_log_probs_MBS, model.trainable_weights)\n",
    "            print('after jac')\n",
    "            param_gradient_pBS = [score_function_trick(j, sample_decisions_MBS) for j in jacobian_pMBS]\n",
    "            print('before grad')\n",
    "            loss_gradients_BS = loss_tape.gradient(loss_B, expected_decisions_BS)\n",
    "            print('after grad')\n",
    "            overall_gradient = [overall_gradient_calculation(g, loss_gradients_BS) for g in param_gradient_pBS]\n",
    "            print('grads combined')\n",
    "\n",
    "            # Run one step of gradient descent by updating\n",
    "            # the value of the variables to minimize the loss.\n",
    "            optimizer.apply_gradients(zip(overall_gradient, model.trainable_weights))\n",
    "            \n",
    "\n",
    "\n",
    "            batch_losses['train']['loss'].append(tf.reduce_mean(loss_B))\n",
    "            batch_losses['train']['nll'].append(tf.reduce_mean(tf.reduce_sum(observed_log_prob_BS, axis=-1)))\n",
    "            batch_losses['train']['bpr'].append(tf.reduce_mean(bpr_B))\n",
    "            print(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "            \n",
    "            \n",
    "        losses['train']['loss'].append(tf.reduce_mean(batch_losses['train']['loss']))\n",
    "        losses['train']['nll'].append(tf.reduce_mean(batch_losses['train']['nll']))\n",
    "        losses['train']['bpr'].append(tf.reduce_mean(batch_losses['train']['bpr']))\n",
    "\n",
    "        if verbose:\n",
    "            # print all metrics\n",
    "            print(f'Loss: {losses[\"train\"][\"loss\"][-1]}')\n",
    "            print(f'NLL: {losses[\"train\"][\"nll\"][-1]}')\n",
    "            print(f'BPR: {losses[\"train\"][\"bpr\"][-1]}')\n",
    "\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "                \n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:26:46.462884: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-12 14:26:46.507144: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-12 14:26:46.507170: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-12 14:26:46.508242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-12 14:26:46.515890: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-12 14:26:48.375811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:26:54.816543: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-05-12 14:26:54.816577: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: s1cmp008.pax.tufts.edu\n",
      "2024-05-12 14:26:54.816584: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: s1cmp008.pax.tufts.edu\n",
      "2024-05-12 14:26:54.816647: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.129.3\n",
      "2024-05-12 14:26:54.816671: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.129.3\n",
      "2024-05-12 14:26:54.816676: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.129.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7334\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.7260\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.7180\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.7110\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7039\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6962\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6887\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6814\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6733\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6666\n",
      "tf.Tensor([[183.6322]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([8.803156], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Define a simple model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tfp.layers.DistributionLambda(lambda t: tfp.distributions.Normal(loc=t, scale=1)),\n",
    "])\n",
    "\n",
    "# Define some data\n",
    "x = tf.random.normal(shape=(100, 1))\n",
    "y = tf.random.normal(shape=(100, 1))\n",
    "\n",
    "# Define a loss function\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=nll)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x, y, epochs=10)\n",
    "\n",
    "# Now, let's differentiate through samples and log probabilities\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    # Compute loss\n",
    "    loss = nll(y, y_pred)\n",
    "\n",
    "# Get the gradients of the loss with respect to the model's trainable variables\n",
    "gradients = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "# Print the gradients\n",
    "for grad in gradients:\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1), dtype=float32, numpy=\n",
       "array([[-1.484607  ],\n",
       "       [ 0.08382666],\n",
       "       [ 3.205427  ],\n",
       "       [ 1.8569267 ],\n",
       "       [ 1.0521313 ],\n",
       "       [ 3.26027   ],\n",
       "       [ 0.8267242 ],\n",
       "       [-0.04156196],\n",
       "       [ 3.305962  ],\n",
       "       [ 0.19652712],\n",
       "       [ 2.3469286 ],\n",
       "       [-0.21311045],\n",
       "       [ 1.7456309 ],\n",
       "       [ 0.1669746 ],\n",
       "       [-0.92454946],\n",
       "       [ 2.629715  ],\n",
       "       [ 2.7455034 ],\n",
       "       [ 0.55555665],\n",
       "       [ 2.3121119 ],\n",
       "       [ 2.9785523 ],\n",
       "       [-0.94783545],\n",
       "       [-0.26307166],\n",
       "       [ 3.2826483 ],\n",
       "       [ 2.024968  ],\n",
       "       [ 1.230127  ],\n",
       "       [-1.3259962 ],\n",
       "       [-0.6261064 ],\n",
       "       [-3.4701219 ],\n",
       "       [ 4.0360813 ],\n",
       "       [ 3.9916093 ],\n",
       "       [ 2.1007195 ],\n",
       "       [ 3.0485828 ],\n",
       "       [ 3.3723094 ],\n",
       "       [ 0.7909816 ],\n",
       "       [ 2.1264493 ],\n",
       "       [-2.4550061 ],\n",
       "       [ 3.5258813 ],\n",
       "       [ 3.9761531 ],\n",
       "       [ 1.7272794 ],\n",
       "       [ 0.49896324],\n",
       "       [-0.35074925],\n",
       "       [ 1.4300237 ],\n",
       "       [ 3.670957  ],\n",
       "       [ 1.0924287 ],\n",
       "       [-1.8517761 ],\n",
       "       [ 0.93293583],\n",
       "       [ 1.2917633 ],\n",
       "       [ 4.544841  ],\n",
       "       [ 0.28673398],\n",
       "       [ 2.872809  ],\n",
       "       [-1.202699  ],\n",
       "       [ 0.29167545],\n",
       "       [ 3.074149  ],\n",
       "       [-0.03546894],\n",
       "       [ 3.8204415 ],\n",
       "       [-0.2524662 ],\n",
       "       [ 1.512827  ],\n",
       "       [-0.28525043],\n",
       "       [ 0.04124725],\n",
       "       [-0.1773467 ],\n",
       "       [-0.0141561 ],\n",
       "       [-0.5299674 ],\n",
       "       [ 0.452677  ],\n",
       "       [-1.7178466 ],\n",
       "       [-2.246979  ],\n",
       "       [ 0.57849604],\n",
       "       [ 3.8957777 ],\n",
       "       [ 2.3903487 ],\n",
       "       [ 1.4568646 ],\n",
       "       [ 0.06257749],\n",
       "       [ 1.6522149 ],\n",
       "       [ 1.3669056 ],\n",
       "       [ 0.3263892 ],\n",
       "       [ 4.096521  ],\n",
       "       [-0.846359  ],\n",
       "       [ 0.41295195],\n",
       "       [-3.7939496 ],\n",
       "       [-2.8633199 ],\n",
       "       [ 1.4838665 ],\n",
       "       [ 0.39521253],\n",
       "       [ 0.12739646],\n",
       "       [-0.01436329],\n",
       "       [ 2.9013627 ],\n",
       "       [-3.5357752 ],\n",
       "       [ 0.02843094],\n",
       "       [ 1.2717683 ],\n",
       "       [ 0.8594543 ],\n",
       "       [ 1.7664915 ],\n",
       "       [ 0.44305074],\n",
       "       [ 4.3477397 ],\n",
       "       [ 1.1784422 ],\n",
       "       [ 0.03370237],\n",
       "       [ 0.32466125],\n",
       "       [ 4.341321  ],\n",
       "       [ 0.02719235],\n",
       "       [-0.847955  ],\n",
       "       [ 2.5278075 ],\n",
       "       [ 0.45125282],\n",
       "       [ 1.2990053 ],\n",
       "       [ 1.6172445 ]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k2_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
