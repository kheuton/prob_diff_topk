{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 00:27:54.280582: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-28 00:27:54.298216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738042074.318786   54124 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738042074.325159   54124 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-28 00:27:54.347179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# add code directory to path\n",
    "import sys\n",
    "sys.path.append('/cluster/home/fmuenc01/code/prob_diff_topk')\n",
    "\n",
    "from torch_models import NegativeBinomialRegressionModel\n",
    "from metrics import top_k_onehot_indicator\n",
    "from functools import partial\n",
    "from torch_models import NegativeBinomialRegressionModel\n",
    "from metrics import top_k_onehot_indicator\n",
    "from torch_perturb.perturbations import perturbed\n",
    "from torch_models import torch_bpr_uncurried, deterministic_bpr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Load and process training, validation, and test data.\"\"\"\n",
    "    # Load data\n",
    "    train_X_df = pd.read_csv(os.path.join(data_dir, 'bird_train_x.csv'), index_col=[0,1])\n",
    "    train_Y_df = pd.read_csv(os.path.join(data_dir, 'bird_train_y.csv'), index_col=[0,1])\n",
    "    val_X_df = pd.read_csv(os.path.join(data_dir, 'bird_valid_x.csv'), index_col=[0,1])\n",
    "    val_Y_df = pd.read_csv(os.path.join(data_dir, 'bird_valid_y.csv'), index_col=[0,1])\n",
    "    test_X_df = pd.read_csv(os.path.join(data_dir, 'bird_test_x.csv'), index_col=[0,1])\n",
    "    test_Y_df = pd.read_csv(os.path.join(data_dir, 'bird_test_y.csv'), index_col=[0,1])\n",
    "    \n",
    "    def convert_df_to_3d_array(df):\n",
    "        geoids = sorted(df.index.get_level_values('geoid').unique())\n",
    "        timesteps = sorted(df.index.get_level_values('timestep').unique())\n",
    "        geoid_to_idx = {geoid: idx for idx, geoid in enumerate(geoids)}\n",
    "        \n",
    "        num_timesteps = len(timesteps)\n",
    "        num_locations = len(geoids)\n",
    "        num_features = len(df.columns)\n",
    "        X = np.zeros((num_timesteps, num_locations, num_features))\n",
    "        \n",
    "        for (geoid, timestep), row in df.iterrows():\n",
    "            t_idx = timesteps.index(timestep)\n",
    "            g_idx = geoid_to_idx[geoid]\n",
    "            X[t_idx, g_idx, :] = row.values\n",
    "            \n",
    "        return X, geoids, timesteps\n",
    "\n",
    "    def convert_y_df_to_2d_array(y_df, geoids, timesteps):\n",
    "        num_timesteps = len(timesteps)\n",
    "        num_locations = len(geoids)\n",
    "        y = np.zeros((num_timesteps, num_locations))\n",
    "        geoid_to_idx = {geoid: idx for idx, geoid in enumerate(geoids)}\n",
    "        \n",
    "        for (geoid, timestep), value in y_df.iloc[:, 0].items():\n",
    "            t_idx = timesteps.index(timestep)\n",
    "            g_idx = geoid_to_idx[geoid]\n",
    "            y[t_idx, g_idx] = value\n",
    "            \n",
    "        return y\n",
    "\n",
    "    # Process training data\n",
    "    print(train_X_df.columns, train_X_df.index)\n",
    "    train_X, geoids, timesteps = convert_df_to_3d_array(train_X_df)#.drop(columns='timestep'))\n",
    "    train_time = np.array([timesteps] * len(geoids)).T\n",
    "    train_y = convert_y_df_to_2d_array(train_Y_df, geoids, timesteps)\n",
    "\n",
    "    # Process validation data\n",
    "    val_X, val_geoids, val_timesteps = convert_df_to_3d_array(val_X_df)#.drop(columns='timestep.1'))\n",
    "    val_time = np.array([val_timesteps] * len(val_geoids)).T\n",
    "    val_y = convert_y_df_to_2d_array(val_Y_df, val_geoids, val_timesteps)\n",
    "\n",
    "    # Process test data\n",
    "    test_X, test_geoids, test_timesteps = convert_df_to_3d_array(test_X_df)#.drop(columns='timestep.1'))\n",
    "    test_time = np.array([test_timesteps] * len(test_geoids)).T\n",
    "    test_y = convert_y_df_to_2d_array(test_Y_df, test_geoids, test_timesteps)\n",
    "\n",
    "    return {\n",
    "        'train': (torch.tensor(train_X, dtype=torch.float32), \n",
    "                 torch.tensor(train_time, dtype=torch.float32),\n",
    "                 torch.tensor(train_y, dtype=torch.float32)),\n",
    "        'val': (torch.tensor(val_X, dtype=torch.float32),\n",
    "               torch.tensor(val_time, dtype=torch.float32),\n",
    "               torch.tensor(val_y, dtype=torch.float32)),\n",
    "        'test': (torch.tensor(test_X, dtype=torch.float32),\n",
    "                torch.tensor(test_time, dtype=torch.float32),\n",
    "                torch.tensor(test_y, dtype=torch.float32))\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, X, time_T, y, K, perturbed_noise=0.01, num_pert_samples=50, num_score_samples=20):\n",
    "    \"\"\"Evaluate model using the same metrics as in training.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        dist = model(X, time_T)\n",
    "        \n",
    "        # Get samples for ratio calculation\n",
    "        y_sample_TMS = dist.sample((num_score_samples,)).permute(1, 0, 2)\n",
    "        ratio_rating_TMS = y_sample_TMS/y_sample_TMS.sum(dim=-1, keepdim=True)\n",
    "        ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "        \n",
    "        # Calculate deterministic BPR\n",
    "        det_bpr = deterministic_bpr(ratio_rating_TS, y, K=K)\n",
    "        det_bpr = torch.mean(det_bpr)\n",
    "        \n",
    "        # Calculate perturbed BPR\n",
    "        top_k_func = partial(top_k_onehot_indicator, k=K)\n",
    "        perturbed_top_K_func = perturbed(top_k_func, sigma=perturbed_noise, num_samples=num_pert_samples)\n",
    "        pert_bpr = torch_bpr_uncurried(ratio_rating_TS, y, K=K, perturbed_top_K_func=perturbed_top_K_func)\n",
    "        pert_bpr = torch.mean(pert_bpr)\n",
    "        \n",
    "        # Calculate NLL\n",
    "        nll = -model.log_likelihood(y, X, time_T)\n",
    "        \n",
    "        return {\n",
    "            'det_bpr': det_bpr.item(),\n",
    "            'pert_bpr': pert_bpr.item(),\n",
    "            'nll': nll.item()\n",
    "        }\n",
    "\n",
    "def find_best_model_and_evaluate(base_dir, data_dir, device='cuda', K=100):\n",
    "    \"\"\"Find the best model based on validation performance and evaluate on test set.\"\"\"\n",
    "    # Load all data\n",
    "    data = load_data(data_dir)\n",
    "    \n",
    "    # Move data to device\n",
    "    val_data = tuple(x.to(device) for x in data['val'])\n",
    "    test_data = tuple(x.to(device) for x in data['test'])\n",
    "    # Find all experiment directories\n",
    "    exp_dirs = glob.glob(os.path.join(base_dir, \"K*\"))\n",
    "    \n",
    "    best_val_score = float('-inf')\n",
    "    best_model_info = None\n",
    "    \n",
    "    results = []\n",
    "    print(exp_dirs)\n",
    "    for exp_dir in exp_dirs:\n",
    "        # Extract hyperparameters from directory name\n",
    "        dir_name = os.path.basename(exp_dir)\n",
    "        params = [param.split('_') for param in dir_name.split('_')]\n",
    "        \n",
    "        # Load model\n",
    "        model_path = os.path.join(exp_dir, 'best_model.pth')\n",
    "        if not os.path.exists(model_path):\n",
    "            continue\n",
    "            \n",
    "        # Initialize model with correct parameters\n",
    "        model = NegativeBinomialRegressionModel(\n",
    "            num_locations=data['train'][0].shape[1],\n",
    "            num_fixed_effects=data['train'][0].shape[2]\n",
    "        ).to(device)\n",
    "        \n",
    "        \n",
    "        # Load saved weights\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_performance = evaluate_model(\n",
    "            model, *val_data, K=K,\n",
    "        )\n",
    "        \n",
    "        # Track results\n",
    "        result = {\n",
    "            'dir': exp_dir,\n",
    "            **val_performance,\n",
    "            'params':params}\n",
    "\n",
    "        results.append(result)\n",
    "        \n",
    "        # Update best model if this one is better\n",
    "        if val_performance['det_bpr'] > best_val_score:\n",
    "            best_val_score = val_performance['det_bpr']\n",
    "            best_model_info = {\n",
    "                'model': model,\n",
    "                'dir': exp_dir,\n",
    "                'params': params,\n",
    "                'val_performance': val_performance\n",
    "            }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Evaluate best model on test set\n",
    "    if best_model_info is not None:\n",
    "        test_performance = evaluate_model(\n",
    "            best_model_info['model'], *test_data, K=K,\n",
    "        )\n",
    "        \n",
    "        print(\"\\nBest Model Information:\")\n",
    "        print(f\"Directory: {best_model_info['dir']}\")\n",
    "        print(f\"Parameters: {best_model_info['params']}\")\n",
    "        print(\"\\nValidation Performance:\")\n",
    "        print(f\"Deterministic BPR: {best_model_info['val_performance']['det_bpr']:.4f}\")\n",
    "        print(f\"Perturbed BPR: {best_model_info['val_performance']['pert_bpr']:.4f}\")\n",
    "        print(f\"NLL: {best_model_info['val_performance']['nll']:.4f}\")\n",
    "        print(\"\\nTest Performance:\")\n",
    "        print(f\"Deterministic BPR: {test_performance['det_bpr']:.4f}\")\n",
    "        print(f\"Perturbed BPR: {test_performance['pert_bpr']:.4f}\")\n",
    "        print(f\"NLL: {test_performance['nll']:.4f}\")\n",
    "    \n",
    "    return results_df, best_model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/cluster/tufts/hugheslab/fmuenc01/code/prob_diff_topk/data_dir/asurv/2monthly_ctxtSize5_small/'\n",
    "base_dir = '/cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K75_8000_bigFix/asurv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_negative_binomial_report(exp_dir, title=None):\n",
    "    \"\"\"\n",
    "    Generate a report with training metrics from saved CSV files.\n",
    "    \n",
    "    Args:\n",
    "        exp_dir: Directory containing experiment metrics files\n",
    "        title: Title for the report\n",
    "    \"\"\"\n",
    "    # Load metrics from CSV files\n",
    "    train_metrics = pd.read_csv(os.path.join(exp_dir, 'train_metrics.csv'), index_col='epochs')\n",
    "    val_metrics = pd.read_csv(os.path.join(exp_dir, 'val_metrics.csv'), index_col='epochs')\n",
    "    times = pd.read_csv(os.path.join(exp_dir, 'time_metrics.csv'))\n",
    "\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    else:\n",
    "        fig.suptitle(os.path.basename(exp_dir), fontsize=16)\n",
    "\n",
    "    # Plot training and validation metrics\n",
    "    epochs = train_metrics.index\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(epochs, train_metrics['loss'], 'b-', label='Train Loss')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # BPR plot\n",
    "    axes[0, 1].plot(epochs, train_metrics['deterministic_bpr'], 'g-', label='Train BPR')\n",
    "    if 'deterministic_bpr' in val_metrics.columns:\n",
    "        val_epochs = val_metrics.index\n",
    "        axes[0, 1].plot(val_epochs, val_metrics['deterministic_bpr'], 'g--', label='Val BPR')\n",
    "    axes[0, 1].set_title('Deterministic BPR')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('BPR')\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # NLL plot\n",
    "    axes[1, 0].plot(epochs, train_metrics['nll'], 'r-', label='Train NLL')\n",
    "    if 'nll' in val_metrics.columns:\n",
    "        axes[1, 0].plot(val_epochs, val_metrics['nll'], 'r--', label='Val NLL')\n",
    "    axes[1, 0].set_title('Negative Log Likelihood (NLL)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('NLL')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Time plot\n",
    "    cumulative_time = np.cumsum(times['times'])\n",
    "    axes[1, 1].plot(np.arange(len(cumulative_time)), cumulative_time, 'm-', label='Cumulative Time')\n",
    "    axes[1, 1].set_title('Cumulative Training Time')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Time (seconds)')\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    report_path = os.path.join(exp_dir, 'training_report.png')\n",
    "    plt.savefig(report_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Calculate summary metrics\n",
    "    summary = {\n",
    "        'final_loss': train_metrics['loss'].iloc[-1],\n",
    "        'final_train_bpr': train_metrics['deterministic_bpr'].iloc[-1],\n",
    "        'final_train_nll': train_metrics['nll'].iloc[-1],\n",
    "        'best_train_bpr': train_metrics['deterministic_bpr'].max(),\n",
    "        'best_train_bpr_epoch': train_metrics['deterministic_bpr'].idxmax(),\n",
    "        'min_train_nll': train_metrics['nll'].min(),\n",
    "        'min_train_nll_epoch': train_metrics['nll'].idxmin(),\n",
    "        'total_time': cumulative_time,\n",
    "        'total_epochs': len(epochs)\n",
    "    }\n",
    "\n",
    "    # Add validation metrics if available\n",
    "    if not val_metrics.empty:\n",
    "        summary.update({\n",
    "            'final_val_bpr': val_metrics['deterministic_bpr'].iloc[-1],\n",
    "            'final_val_nll': val_metrics['nll'].iloc[-1],\n",
    "            'best_val_bpr': val_metrics['deterministic_bpr'].max(),\n",
    "            'best_val_bpr_epoch': val_metrics['deterministic_bpr'].idxmax(),\n",
    "            'min_val_nll': val_metrics['nll'].min(),\n",
    "            'min_val_nll_epoch': val_metrics['nll'].idxmin()\n",
    "        })\n",
    "\n",
    "    return summary\n",
    "\n",
    "def generate_reports_for_all_experiments(base_dir):\n",
    "    \"\"\"Generate reports for all experiments in the base directory.\"\"\"\n",
    "    exp_dirs = glob.glob(os.path.join(base_dir, \"K*\"))\n",
    "    summaries = []\n",
    "    \n",
    "    for exp_dir in exp_dirs:\n",
    "        print\n",
    "        try:\n",
    "            # Extract parameters from directory name\n",
    "            dir_name = os.path.basename(exp_dir)\n",
    "            params = {}\n",
    "            for param in dir_name.split('_'):\n",
    "                if len(param) >= 2:\n",
    "                    key = param[:2]  # Take first two characters as key\n",
    "                    value = param[2:]  # Take rest as value\n",
    "                    params[key] = value\n",
    "            \n",
    "            # Generate report\n",
    "            title = f\"Experiment Results\\n{dir_name}\"\n",
    "            summary = make_negative_binomial_report(exp_dir, title=title)\n",
    "            \n",
    "            if summary is not None:\n",
    "                # Add experiment parameters to summary\n",
    "                summary.update(params)\n",
    "                summary['exp_dir'] = exp_dir\n",
    "                summaries.append(summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {exp_dir}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    # Create summary DataFrame and save\n",
    "    if summaries:\n",
    "        summary_df = pd.DataFrame(summaries)\n",
    "        summary_df.to_csv(os.path.join(base_dir, 'all_experiments_summary.csv'), index=False)\n",
    "        \n",
    "        # Print best models according to different metrics\n",
    "        print(\"\\nBest Models:\")\n",
    "        print(\"\\nBy Best Validation BPR:\")\n",
    "        best_val_bpr_idx = summary_df['best_val_bpr'].idxmax()\n",
    "        print(summary_df.loc[best_val_bpr_idx, ['exp_dir', 'best_val_bpr', 'best_val_bpr_epoch']])\n",
    "        \n",
    "        print(\"\\nBy Best Validation NLL:\")\n",
    "        best_val_nll_idx = summary_df['min_val_nll'].idxmin()\n",
    "        print(summary_df.loc[best_val_nll_idx, ['exp_dir', 'min_val_nll', 'min_val_nll_epoch']])\n",
    "        \n",
    "        return summary_df\n",
    "    else:\n",
    "        print(\"No valid summaries generated\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generate_reports_for_all_experiments(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('asurv_init_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['final_loss', 'final_train_bpr', 'final_train_nll', 'best_train_bpr',\n",
       "       'best_train_bpr_epoch', 'min_train_nll', 'min_train_nll_epoch',\n",
       "       'total_time', 'total_epochs', 'final_val_bpr', 'final_val_nll',\n",
       "       'best_val_bpr', 'best_val_bpr_epoch', 'min_val_nll',\n",
       "       'min_val_nll_epoch', 'K7', 'bw', 'nw', 'ss', 'ns', 'np', 'se', 'st',\n",
       "       'ti', 'ct', 'sm', 'exp_dir', 'si', 'tr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th># of models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.85</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  threshold  # of models\n",
       "0       0.7            6\n",
       "1      0.75            2\n",
       "2       0.8            6\n",
       "3      0.85            6\n",
       "4         1           22"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby('tr').count()[['final_loss']].reset_index().rename({'tr': 'threshold', 'final_loss': '# of models'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['final_loss', 'final_train_bpr', 'final_train_nll', 'best_train_bpr',\n",
       "       'best_train_bpr_epoch', 'min_train_nll', 'min_train_nll_epoch',\n",
       "       'total_time', 'total_epochs', 'final_val_bpr', 'final_val_nll',\n",
       "       'best_val_bpr', 'best_val_bpr_epoch', 'min_val_nll',\n",
       "       'min_val_nll_epoch', 'K2', 'bw', 'nw', 'ss', 'ns', 'np', 'se', 'st',\n",
       "       'ti', 'ct', 'sm', 'exp_dir', 'si', 'tr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.0001_nss50_nps50_seed360_sig0.001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4303235411643982\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.001_nss50_nps50_seed360_sig0.1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.429681658744812\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.001_nss50_nps50_seed360_sig0.01_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4265941977500915\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.0001_nss50_nps50_seed360_sig0.1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4251540899276733\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.0001_nss50_nps50_seed360_sig0.01_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4237491190433502\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.0001_nss50_nps50_seed360_sig0.001_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4234586060047149\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.001_nss50_nps50_seed360_sig0.001_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4234439134597778\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.001_nss50_nps50_seed360_sig0.001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4189121425151825\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.0001_nss50_nps50_seed360_sig0.01_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4188794195652008\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.0001_nss50_nps50_seed360_sig0.0001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4146189093589782\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.001_nss50_nps50_seed360_sig0.01_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4045617282390594\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.0001_nss50_nps50_seed360_sig0.1_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.4000198543071747\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.001_nss50_nps50_seed360_sig0.1_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3939779102802276\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.01_nss50_nps50_seed360_sig0.1_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3930628895759582\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.0001_nss50_nps50_seed360_sig0.0001_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.39178666472435\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.001_nss50_nps50_seed360_sig0.0001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3856854736804962\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw0_nw1_ss0.01_nss50_nps50_seed360_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3826450109481811\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw0_nw1_ss0.001_nss50_nps50_seed360_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3800811171531677\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw0_ss0.001_nss50_nps50_seed360_sig0.0001_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3777991235256195\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw0_nw1_ss0.0001_nss50_nps50_seed360_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3602181375026703\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.01_nss50_nps50_seed360_sig0.01_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3212219774723053\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.01_nss50_nps50_seed360_sig0.0001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3068277835845947\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.01_nss50_nps50_seed360_sig0.001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.3032692670822143\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.1_nss50_nps50_seed360_sig0.0001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.2606113851070404\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.1_nss50_nps50_seed360_sig0.001_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.2424157112836837\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw0_nw1_ss0.1_nss50_nps50_seed360_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.1902299374341964\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.1_nss50_nps50_seed360_sig0.1_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.1902299374341964\n",
      "\n",
      "folder: /cluster/tufts/hugheslab/fmuenc01/bird_zero_rand_K25_8000_bigFix/K25_bw30_nw1_ss0.1_nss50_nps50_seed360_sig0.01_tr1_studyasurv_timescale2monthly_ctxtSize5_small\n",
      "Val BPR: 0.1344156265258789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in results.sort_values(by='best_val_bpr', ascending=False).iterrows():\n",
    "    print(f\"folder: {row['exp_dir']}\")\n",
    "    print(f\"Val BPR: {row['best_val_bpr']}\")\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'long', 'prev_counts_05back', 'prev_counts_04back',\n",
      "       'prev_counts_03back', 'prev_counts_02back', 'prev_counts_01back'],\n",
      "      dtype='object') MultiIndex([(   0,   9),\n",
      "            (   1,   9),\n",
      "            (   2,   9),\n",
      "            (   3,   9),\n",
      "            (   4,   9),\n",
      "            (   5,   9),\n",
      "            (   6,   9),\n",
      "            (   7,   9),\n",
      "            (   8,   9),\n",
      "            (   9,   9),\n",
      "            ...\n",
      "            (2146, 131),\n",
      "            (2147, 131),\n",
      "            (2148, 131),\n",
      "            (2149, 131),\n",
      "            (2150, 131),\n",
      "            (2151, 131),\n",
      "            (2152, 131),\n",
      "            (2153, 131),\n",
      "            (2154, 131),\n",
      "            (2155, 131)],\n",
      "           names=['geoid', 'timestep'], length=19404)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_df, best_model_info \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_model_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 124\u001b[0m, in \u001b[0;36mfind_best_model_and_evaluate\u001b[0;34m(base_dir, data_dir, device, K)\u001b[0m\n\u001b[1;32m    121\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data(data_dir)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Find all experiment directories\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 124\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m data \u001b[38;5;241m=\u001b[39m load_data(data_dir)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    125\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Find all experiment directories\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/fmuenc01/miniforge3/envs/birds/lib/python3.11/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "results_df, best_model_info = find_best_model_and_evaluate(base_dir, data_dir, device='cuda', K=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the final test_det_bpr and -test_nll for all models, use a different color and marker for\n",
    "# bw = 30, nw=0 call this bpr only\n",
    "# bw = 30, nw=1 call this hybrid\n",
    "# bw = 0, nw=1 call this nll only\n",
    "# don't do 0 and 0\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Extract 'tr' values from results_df['param_8']\n",
    "results_df['tr'] = results_df['param_8'].apply(lambda x: float(x[0][2:]) if isinstance(x, list) and len(x) > 0 else np.nan)\n",
    "\n",
    "# Extract 'bw' and 'nw' values from param_1 and param_2\n",
    "results_df['bw'] = results_df['param_1'].apply(lambda x: int(x[0][2:]) if isinstance(x, list) and len(x) > 0 else np.nan)\n",
    "results_df['nw'] = results_df['param_2'].apply(lambda x: int(x[0][2:]) if isinstance(x, list) and len(x) > 0 else np.nan)\n",
    "\n",
    "# Define colors and markers\n",
    "colors = { (30, 0): 'blue', (30, 1): 'green', (0, 1): 'red' }\n",
    "markers = { (30, 0): 'o', (30, 1): '*', (0, 1): 'D' }\n",
    "labelslist = { (30, 0): 'bpr only', (30, 1): 'hybrid', (0, 1): 'nll only' }\n",
    "\n",
    "# Generate shades of green for different thresholds\n",
    "thresholds = sorted(results_df['tr'].dropna().unique())\n",
    "shades_of_green = list(mcolors.LinearSegmentedColormap.from_list(\"\", [\"lightgreen\", \"darkgreen\"])(np.linspace(0, 1, len(thresholds))))\n",
    "\n",
    "# Plot data points\n",
    "for bw, nw in [(30, 0), (30, 1), (0, 1)]:\n",
    "    mask = (results_df['bw'] == bw) & (results_df['nw'] == nw)\n",
    "    if bw == 30 and nw == 0:\n",
    "        idx = results_df[mask]['test_det_bpr'].idxmax()\n",
    "        plt.scatter(results_df.loc[idx, 'test_det_bpr'], -results_df.loc[idx, 'test_nll'], \n",
    "                    color=colors[(bw, nw)], marker=markers[(bw, nw)], label=labelslist[(bw, nw)])\n",
    "    elif bw == 30 and nw == 1:\n",
    "        for i, tr in enumerate(thresholds):\n",
    "            tr_mask = mask & (results_df['tr'] == tr)\n",
    "            idx = results_df[tr_mask]['test_det_bpr'].idxmax()\n",
    "            plt.scatter(results_df.loc[idx, 'test_det_bpr'], -results_df.loc[idx, 'test_nll'], \n",
    "                        color=shades_of_green[i], marker=markers[(bw, nw)], label=f\"{labelslist[(bw, nw)]} (tr={tr})\")\n",
    "    elif bw == 0 and nw == 1:\n",
    "        idx = results_df[mask]['test_det_bpr'].idxmax()\n",
    "        plt.scatter(results_df.loc[idx, 'test_det_bpr'], -results_df.loc[idx, 'test_nll'], \n",
    "                    color=colors[(bw, nw)], marker=markers[(bw, nw)], label=labelslist[(bw, nw)])\n",
    "\n",
    "# Set labels and legend\n",
    "plt.title('Mass County')\n",
    "plt.xlabel('Test Deterministic BPR')\n",
    "plt.ylabel('Test Negative Log Likelihood (NLL)')\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(sorted(zip(labels, handles), key=lambda x: x[0]))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "plt.show()\n",
    "# Plot the test nll and bpr, but choose models based on the best val_det_bpr\n",
    "for bw, nw in [(30, 0), (30, 1), (0, 1)]:\n",
    "    mask = (results_df['bw'] == bw) & (results_df['nw'] == nw)\n",
    "    if bw == 30 and nw == 0:\n",
    "        idx = results_df[mask]['det_bpr'].idxmax()\n",
    "        plt.scatter(results_df.loc[idx, 'test_det_bpr'], -results_df.loc[idx, 'test_nll'], \n",
    "                    color=colors[(bw, nw)], marker=markers[(bw, nw)], label=labelslist[(bw, nw)])\n",
    "    elif bw == 30 and nw == 1:\n",
    "        for i, tr in enumerate(thresholds):\n",
    "            tr_mask = mask & (results_df['tr'] == tr)\n",
    "            idx = results_df[tr_mask]['det_bpr'].idxmax()\n",
    "            plt.scatter(results_df.loc[idx, 'test_det_bpr'], -results_df.loc[idx, 'test_nll'], \n",
    "                        color=shades_of_green[i], marker=markers[(bw, nw)], label=f\"{labelslist[(bw, nw)]} (tr={tr})\")\n",
    "    elif bw == 0 and nw == 1:\n",
    "        idx = results_df[mask]['det_bpr'].idxmax()\n",
    "        plt.scatter(results_df.loc[idx, 'test_det_bpr'], -results_df.loc[idx, 'test_nll'], \n",
    "                    color=colors[(bw, nw)], marker=markers[(bw, nw)], label=labelslist[(bw, nw)])\n",
    "\n",
    "# Set labels and legend\n",
    "plt.title('Massachusetts')\n",
    "plt.xlabel('Test Deterministic BPR')\n",
    "plt.ylabel('Test Negative Log Likelihood (NLL)')\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(sorted(zip(labels, handles), key=lambda x: x[0]))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick sanity EDA on best datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
