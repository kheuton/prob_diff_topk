{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "# add code directory to path\n",
    "import sys\n",
    "sys.path.append('/cluster/home/kheuto01/code/prob_diff_topk')\n",
    "\n",
    "from metrics import top_k_onehot_indicator\n",
    "from torch_perturb.perturbations import perturbed\n",
    "from torch_models import NegativeBinomialDebug, torch_bpr_uncurried, deterministic_bpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom torch.distributions import NegativeBinomial\\nfrom torch import nn\\ndef single_tensor_to_params(param_size_shape_list, single_tensor):\\n        params = []\\n        idx = 0\\n        for (numel, shape) in param_size_shape_list:\\n            num_elements = numel\\n            param_data = single_tensor[idx:idx+num_elements].view(shape)\\n            params.append(param_data)\\n            idx += num_elements\\n        return params\\n\\ndef build_from_single_tensor(single_tensor, param_size_shape_list, X, time):\\n    beta_0, beta, b_0, b_1, log_sigma_0, log_sigma_1, rho, siginv_theta = single_tensor_to_params(param_size_shape_list, single_tensor)\\n    fixed_effects = beta_0 + torch.einsum('tli,i->tl', X, beta)\\n    random_intercepts = b_0.expand(X.shape[0], -1)\\n    random_slopes = b_1.expand(X.shape[0], -1)\\n    \\n    log_mu = fixed_effects + random_intercepts + random_slopes * time\\n\\n    # Use softplus to ensure mu is positive and grows more slowly\\n    mu = nn.functional.softplus(log_mu)\\n\\n    # Calculate theta probability\\n    theta = torch.nn.functional.sigmoid(siginv_theta)\\n\\n    \\n    return NegativeBinomial(total_count=mu, probs=theta)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from torch.distributions import NegativeBinomial\n",
    "from torch import nn\n",
    "def single_tensor_to_params(param_size_shape_list, single_tensor):\n",
    "        params = []\n",
    "        idx = 0\n",
    "        for (numel, shape) in param_size_shape_list:\n",
    "            num_elements = numel\n",
    "            param_data = single_tensor[idx:idx+num_elements].view(shape)\n",
    "            params.append(param_data)\n",
    "            idx += num_elements\n",
    "        return params\n",
    "\n",
    "def build_from_single_tensor(single_tensor, param_size_shape_list, X, time):\n",
    "    beta_0, beta, b_0, b_1, log_sigma_0, log_sigma_1, rho, siginv_theta = single_tensor_to_params(param_size_shape_list, single_tensor)\n",
    "    fixed_effects = beta_0 + torch.einsum('tli,i->tl', X, beta)\n",
    "    random_intercepts = b_0.expand(X.shape[0], -1)\n",
    "    random_slopes = b_1.expand(X.shape[0], -1)\n",
    "    \n",
    "    log_mu = fixed_effects + random_intercepts + random_slopes * time\n",
    "\n",
    "    # Use softplus to ensure mu is positive and grows more slowly\n",
    "    mu = nn.functional.softplus(log_mu)\n",
    "\n",
    "    # Calculate theta probability\n",
    "    theta = torch.nn.functional.sigmoid(siginv_theta)\n",
    "\n",
    "    \n",
    "    return NegativeBinomial(total_count=mu, probs=theta)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_3d_array(df):\n",
    "    # Ensure the DataFrame has a MultiIndex with 'geoid' and 'timestep'\n",
    "    if not isinstance(df.index, pd.MultiIndex) or set(df.index.names) != {'geoid', 'timestep'}:\n",
    "        raise ValueError(\"DataFrame must have a MultiIndex with levels 'geoid' and 'timestep'\")\n",
    "\n",
    "    # Get unique geoids and timesteps, sorted\n",
    "    geoids = sorted(df.index.get_level_values('geoid').unique())\n",
    "    timesteps = sorted(df.index.get_level_values('timestep').unique())\n",
    "\n",
    "    # Create a mapping of geoids to indices\n",
    "    geoid_to_idx = {geoid: idx for idx, geoid in enumerate(geoids)}\n",
    "\n",
    "    # Initialize the 3D array\n",
    "    num_timesteps = len(timesteps)\n",
    "    num_locations = len(geoids)\n",
    "    num_features = len(df.columns)\n",
    "    X = np.zeros((num_timesteps, num_locations, num_features))\n",
    "\n",
    "    # Fill the 3D array\n",
    "    for (geoid, timestep), row in df.iterrows():\n",
    "        t_idx = timesteps.index(timestep)\n",
    "        g_idx = geoid_to_idx[geoid]\n",
    "        X[t_idx, g_idx, :] = row.values\n",
    "\n",
    "    return X, geoids, timesteps\n",
    "\n",
    "def convert_y_df_to_2d_array(y_df, geoids, timesteps):\n",
    "    # Ensure the DataFrame has a MultiIndex with 'geoid' and 'timestep'\n",
    "    if not isinstance(y_df.index, pd.MultiIndex) or set(y_df.index.names) != {'geoid', 'timestep'}:\n",
    "        raise ValueError(\"DataFrame must have a MultiIndex with levels 'geoid' and 'timestep'\")\n",
    "\n",
    "    # Initialize the 2D array\n",
    "    num_timesteps = len(timesteps)\n",
    "    num_locations = len(geoids)\n",
    "    y = np.zeros((num_timesteps, num_locations))\n",
    "\n",
    "    # Create a mapping of geoids to indices\n",
    "    geoid_to_idx = {geoid: idx for idx, geoid in enumerate(geoids)}\n",
    "\n",
    "    # Fill the 2D array\n",
    "    for (geoid, timestep), value in y_df.iloc[:, 0].items():\n",
    "        t_idx = timesteps.index(timestep)\n",
    "        g_idx = geoid_to_idx[geoid]\n",
    "        y[t_idx, g_idx] = value\n",
    "\n",
    "    return y\n",
    "\n",
    "def evaluate_model(model, X, y, time, K, M_score_func, perturbed_top_K_func):\n",
    "    \"\"\"Evaluate model on given data and return metrics.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        dist = model(X, time)\n",
    "        \n",
    "        # Sample and calculate ratio ratings\n",
    "        y_sample_TMS = dist.sample((M_score_func,)).permute(1, 0, 2)\n",
    "        ratio_rating_TMS = y_sample_TMS/(1+y_sample_TMS.sum(dim=-1, keepdim=True))\n",
    "        ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        nll = -model.log_likelihood(y, X, time)\n",
    "        perturbed_bpr_T = torch_bpr_uncurried(ratio_rating_TS, y, K=K, \n",
    "                                             perturbed_top_K_func=perturbed_top_K_func)\n",
    "        deterministic_bpr_T = deterministic_bpr(ratio_rating_TS, y, K=K)\n",
    "        \n",
    "        metrics = {\n",
    "            'nll': nll.item(),\n",
    "            'perturbed_bpr': torch.mean(perturbed_bpr_T).item(),\n",
    "            'deterministic_bpr': torch.mean(deterministic_bpr_T).item()\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "def train_epoch_neg_binom(model, optimizer, K, threshold,\n",
    "                         M_score_func, feat_TSF,\n",
    "                         time_T, train_y_TS,\n",
    "                         perturbed_top_K_func, bpr_weight, nll_weight, update=True):\n",
    "    \"\"\"Train one epoch of the negative binomial model.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_gradient_P = None\n",
    "    if (nll_weight > 0) and (bpr_weight==0) and (False):\n",
    "        nll = -model.log_likelihood(train_y_TS, feat_TSF, time_T)\n",
    "        nll.backward()\n",
    "        if update: \n",
    "            optimizer.step()\n",
    "        metrics = {\n",
    "            'loss': nll.item(),\n",
    "            'deterministic_bpr': 0,\n",
    "            'perturbed_bpr': 0,\n",
    "            'nll': nll.item()}\n",
    "    else:\n",
    "        for t in range(feat_TSF.shape[0]):\n",
    "            dist = model(feat_TSF[t:t+1], time_T[t:t+1])\n",
    "\n",
    "            y_sample_TMS = dist.sample((M_score_func,)).permute(1, 0, 2)\n",
    "            action_denominator_TM = y_sample_TMS.sum(dim=-1, keepdim=True) + 1 \n",
    "\n",
    "            ratio_rating_TMS = y_sample_TMS / action_denominator_TM\n",
    "            ratio_rating_TS = ratio_rating_TMS.mean(dim=1)\n",
    "            ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "            def get_log_probs_baked(param, y_samples_chunk):\n",
    "                distribution = model.build_from_single_tensor(param, feat_TSF[t:t+1], time_T[t:t+1])\n",
    "                log_probs_chunk = distribution.log_prob(y_samples_chunk.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "                return log_probs_chunk\n",
    "\n",
    "            # Define the chunk size\n",
    "            chunk_size = 1  # Adjust this to fit your memory constraints\n",
    "            jac_TMSP_chunks = []\n",
    "\n",
    "            # Loop through chunks of y_sample_TMS\n",
    "            for i in range(0, M_score_func, chunk_size):\n",
    "                print(f'Chunky: {i}')\n",
    "                y_samples_chunk = y_sample_TMS[:, i:i+chunk_size, :]\n",
    "                jac_chunk = torch.autograd.functional.jacobian(\n",
    "                    lambda param: get_log_probs_baked(param, y_samples_chunk),\n",
    "                    model.params_to_single_tensor(),\n",
    "                    strategy='forward-mode',\n",
    "                    vectorize=True\n",
    "                )\n",
    "                jac_TMSP_chunks.append(jac_chunk)\n",
    "\n",
    "            # Concatenate all chunks along the sample dimension\n",
    "            jac_TMSP = torch.cat(jac_TMSP_chunks, dim=1)  # Adjust dim if needed\n",
    "\n",
    "            score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "            score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)    \n",
    "\n",
    "            positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS[t:t+1]), \n",
    "                                                K=K, perturbed_top_K_func=perturbed_top_K_func)\n",
    "\n",
    "            if nll_weight > 0:\n",
    "                bpr_threshold_diff_T = positive_bpr_T - threshold\n",
    "                violate_threshold_flag = bpr_threshold_diff_T < 0\n",
    "                negative_bpr_loss = torch.mean(-bpr_threshold_diff_T * violate_threshold_flag)\n",
    "            else:\n",
    "                negative_bpr_loss = torch.mean(-positive_bpr_T)\n",
    "\n",
    "            nll = -model.log_likelihood(train_y_TS[t:t+1], feat_TSF[t:t+1], time_T[t:t+1])\n",
    "            loss = bpr_weight * negative_bpr_loss + nll_weight * nll\n",
    "            loss.backward()\n",
    "\n",
    "            loss_grad_TS = ratio_rating_TS.grad\n",
    "            gradient_TSP = score_func_estimator_TSP * torch.unsqueeze(loss_grad_TS, -1)\n",
    "            gradient_P = torch.sum(gradient_TSP, dim=[0, 1])\n",
    "            \n",
    "            if total_gradient_P is None:\n",
    "                total_gradient_P = gradient_P\n",
    "            else:\n",
    "                total_gradient_P += gradient_P\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        gradient_tuple = model.single_tensor_to_params(total_gradient_P)\n",
    "\n",
    "        for param, gradient in zip(model.parameters(), gradient_tuple):\n",
    "            if nll_weight > 0:\n",
    "                gradient = gradient + param.grad\n",
    "            param.grad = gradient\n",
    "\n",
    "        if update:\n",
    "            optimizer.step()\n",
    "\n",
    "        deterministic_bpr_T = deterministic_bpr(ratio_rating_TS, torch.tensor(train_y_TS), K=K)\n",
    "        det_bpr = torch.mean(deterministic_bpr_T)\n",
    "\n",
    "        metrics = {\n",
    "            'loss': total_loss ,\n",
    "            'deterministic_bpr': det_bpr.item(),\n",
    "            'perturbed_bpr': torch.mean(positive_bpr_T).item(),\n",
    "            'nll': nll.item()\n",
    "        }\n",
    "\n",
    "    return metrics, model\n",
    "\n",
    "def main(K=None, step_size=None, epochs=None, bpr_weight=None,\n",
    "         nll_weight=None, seed=None, outdir=None, threshold=None,\n",
    "         perturbed_noise=None, num_score_samples=None, num_pert_samples=None,\n",
    "         data_dir=None, device='cuda', val_freq=10):\n",
    "    \"\"\"Main training loop with command line arguments.\"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Load training data\n",
    "    train_X_df = pd.read_csv(os.path.join(data_dir, 'train_x.csv'), index_col=[0,1])\n",
    "    train_Y_df = pd.read_csv(os.path.join(data_dir, 'train_y.csv'), index_col=[0,1])\n",
    "    \n",
    "    # Load validation data\n",
    "    val_X_df = pd.read_csv(os.path.join(data_dir, 'valid_x.csv'), index_col=[0,1])\n",
    "    val_Y_df = pd.read_csv(os.path.join(data_dir, 'valid_y.csv'), index_col=[0,1])\n",
    "    \n",
    "    # Process training data\n",
    "    train_X, geoids, timesteps = convert_df_to_3d_array(train_X_df)#.drop(columns='timestep.1'))\n",
    "    train_time_arr = np.array([timesteps] * len(geoids)).T\n",
    "    train_y = convert_y_df_to_2d_array(train_Y_df, geoids, timesteps)\n",
    "\n",
    "    # Process validation data\n",
    "    val_X, _, val_timesteps = convert_df_to_3d_array(val_X_df)#.drop(columns='timestep.1'))\n",
    "    val_time_arr = np.array([val_timesteps] * len(geoids)).T\n",
    "    val_y = convert_y_df_to_2d_array(val_Y_df, geoids, val_timesteps)\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    X_train = torch.tensor(train_X, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_y, dtype=torch.float32).to(device)\n",
    "    time_train = torch.tensor(train_time_arr, dtype=torch.float32).to(device)\n",
    "    \n",
    "    X_val = torch.tensor(val_X, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(val_y, dtype=torch.float32).to(device)\n",
    "    time_val = torch.tensor(val_time_arr, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = NegativeBinomialDebug(\n",
    "        num_locations=len(geoids),\n",
    "        num_fixed_effects=train_X.shape[2], device=device\n",
    "    ).to(device)\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "    # Setup top-k function\n",
    "    top_k_func = partial(top_k_onehot_indicator, k=K)\n",
    "    perturbed_top_K_func = perturbed(top_k_func, sigma=perturbed_noise, num_samples=num_pert_samples)\n",
    "\n",
    "    # Initialize metric tracking with separate epoch tracking for validation\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'epochs': [], \n",
    "            'loss': [], \n",
    "            'nll': [], \n",
    "            'perturbed_bpr': [], \n",
    "            'deterministic_bpr': []\n",
    "        },\n",
    "        'val': {\n",
    "            'epochs': [], \n",
    "            'nll': [], \n",
    "            'perturbed_bpr': [], \n",
    "            'deterministic_bpr': []\n",
    "        },\n",
    "        'times': []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'EPOCH: {epoch}')\n",
    "        start = time.time()\n",
    "        \n",
    "        # Training step\n",
    "        train_metrics, model = train_epoch_neg_binom(\n",
    "            model, optimizer, K, threshold,\n",
    "            num_score_samples, X_train, time_train,\n",
    "            y_train, perturbed_top_K_func,\n",
    "            bpr_weight, nll_weight, device\n",
    "        )\n",
    "        \n",
    "        # Update training metrics\n",
    "        metrics['train']['epochs'].append(epoch)\n",
    "        for metric, value in train_metrics.items():\n",
    "            metrics['train'][metric].append(value)\n",
    "        \n",
    "        # Validation step (every val_freq epochs)\n",
    "        if epoch % val_freq == 0:\n",
    "            model.eval()\n",
    "            val_metrics = evaluate_model(\n",
    "                model, X_val, y_val, time_val,\n",
    "                K, num_score_samples, perturbed_top_K_func\n",
    "            )\n",
    "            \n",
    "            # Update validation metrics\n",
    "            metrics['val']['epochs'].append(epoch)\n",
    "            for metric, value in val_metrics.items():\n",
    "                metrics['val'][metric].append(value)\n",
    "            \n",
    "            # Save best model\n",
    "            val_loss = val_metrics['nll'] * nll_weight\n",
    "            if bpr_weight > 0:\n",
    "                val_loss -= val_metrics['perturbed_bpr'] * bpr_weight\n",
    "                \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                if not os.path.exists(outdir):\n",
    "                    os.makedirs(outdir)\n",
    "                torch.save(model.state_dict(), f'{outdir}/best_model.pth')\n",
    "        \n",
    "        end = time.time()\n",
    "        metrics['times'].append(end - start)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Train - Loss: {train_metrics['loss']:.4f}, NLL: {train_metrics['nll']:.4f}, \"\n",
    "              f\"BPR: {train_metrics['deterministic_bpr']:.4f}\")\n",
    "        if epoch % val_freq == 0:\n",
    "            print(f\"Val - NLL: {val_metrics['nll']:.4f}, \"\n",
    "                  f\"BPR: {val_metrics['deterministic_bpr']:.4f}\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if epoch % 100 == 0:\n",
    "            if not os.path.exists(outdir):\n",
    "                os.makedirs(outdir)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'metrics': metrics,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, f'{outdir}/checkpoint.pth')\n",
    "            \n",
    "            # Save metrics separately for easier analysis\n",
    "            # Create DataFrames with proper indexing\n",
    "            train_df = pd.DataFrame(metrics['train']).set_index('epochs')\n",
    "            val_df = pd.DataFrame(metrics['val']).set_index('epochs')\n",
    "            times_df = pd.DataFrame({'times': metrics['times']}, index=range(len(metrics['times'])))\n",
    "            \n",
    "            train_df.to_csv(f'{outdir}/train_metrics.csv')\n",
    "            val_df.to_csv(f'{outdir}/val_metrics.csv')\n",
    "            times_df.to_csv(f'{outdir}/time_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50\n",
    "bpr_weight = 1\n",
    "nll_weight = 1\n",
    "step_size = 0.001\n",
    "num_score_samples = 1\n",
    "num_pert_samples = 100\n",
    "seed = 123\n",
    "perturbed_noise = 0.001\n",
    "threshold = 0.5\n",
    "epochs = 1\n",
    "outdir = '/cluster/tufts/hugheslab/kheuto01/debug'\n",
    "data_dir = '~/'\n",
    "bird = True\n",
    "\n",
    "device = 'cuda'\n",
    "val_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "if seed is not None:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "if bird:\n",
    "    bird_str = 'bird_'\n",
    "else:\n",
    "    bird_str = ''\n",
    "\n",
    "# Load training data\n",
    "train_X_df = pd.read_csv(os.path.join(data_dir, f'{bird_str}train_x.csv'), index_col=[0,1])\n",
    "train_Y_df = pd.read_csv(os.path.join(data_dir, f'{bird_str}train_y.csv'), index_col=[0,1])\n",
    "\n",
    "# Load validation data\n",
    "val_X_df = pd.read_csv(os.path.join(data_dir, f'{bird_str}valid_x.csv'), index_col=[0,1])\n",
    "val_Y_df = pd.read_csv(os.path.join(data_dir, f'{bird_str}valid_y.csv'), index_col=[0,1])\n",
    "\n",
    "# Process training data\n",
    "train_X, geoids, timesteps = convert_df_to_3d_array(train_X_df)#.drop(columns='timestep.1'))\n",
    "train_time_arr = np.array([timesteps] * len(geoids)).T\n",
    "train_y = convert_y_df_to_2d_array(train_Y_df, geoids, timesteps)\n",
    "\n",
    "# Process validation data\n",
    "val_X, _, val_timesteps = convert_df_to_3d_array(val_X_df)#.drop(columns='timestep.1'))\n",
    "val_time_arr = np.array([val_timesteps] * len(geoids)).T\n",
    "val_y = convert_y_df_to_2d_array(val_Y_df, geoids, val_timesteps)\n",
    "\n",
    "# Convert to tensors and move to device\n",
    "X_train = torch.tensor(train_X, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(train_y, dtype=torch.float32).to(device)\n",
    "time_train = torch.tensor(train_time_arr, dtype=torch.float32).to(device)\n",
    "\n",
    "X_val = torch.tensor(val_X, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(val_y, dtype=torch.float32).to(device)\n",
    "time_val = torch.tensor(val_time_arr, dtype=torch.float32).to(device)\n",
    "\n",
    "# Initialize model\n",
    "model = NegativeBinomialDebug(\n",
    "    num_locations=len(geoids),\n",
    "    num_fixed_effects=train_X.shape[2], device=device\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "# Setup top-k function\n",
    "top_k_func = partial(top_k_onehot_indicator, k=K)\n",
    "perturbed_top_K_func = perturbed(top_k_func, sigma=perturbed_noise, num_samples=num_pert_samples)\n",
    "\n",
    "# Initialize metric tracking with separate epoch tracking for validation\n",
    "metrics = {\n",
    "    'train': {\n",
    "        'epochs': [], \n",
    "        'loss': [], \n",
    "        'nll': [], \n",
    "        'perturbed_bpr': [], \n",
    "        'deterministic_bpr': []\n",
    "    },\n",
    "    'val': {\n",
    "        'epochs': [], \n",
    "        'nll': [], \n",
    "        'perturbed_bpr': [], \n",
    "        'deterministic_bpr': []\n",
    "    },\n",
    "    'times': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-97.0381, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.log_likelihood(y_train, X_train, time_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunky: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259927/284826383.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS[t:t+1]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n",
      "Chunky: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259927/284826383.py:165: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  deterministic_bpr_T = deterministic_bpr(ratio_rating_TS, torch.tensor(train_y_TS), K=K)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "epoch = 0\n",
    "\n",
    "# Training step\n",
    "train_metrics, model = train_epoch_neg_binom(\n",
    "    model, optimizer, K, threshold,\n",
    "    num_score_samples, X_train, time_train,\n",
    "    y_train, perturbed_top_K_func,\n",
    "    bpr_weight, nll_weight, update=False\n",
    ")\n",
    "\n",
    "# Update training metrics\n",
    "metrics['train']['epochs'].append(epoch)\n",
    "for metric, value in train_metrics.items():\n",
    "    metrics['train'][metric].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([nan], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([0.], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([0.], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([0.], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([-1.6950], device='cuda:0', grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list = [param.grad for param in model.parameters()]\n",
    "param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'epochs': [0],\n",
       "  'loss': [inf],\n",
       "  'nll': [inf],\n",
       "  'perturbed_bpr': [0.024308940395712852],\n",
       "  'deterministic_bpr': [0.0]},\n",
       " 'val': {'epochs': [],\n",
       "  'nll': [],\n",
       "  'perturbed_bpr': [],\n",
       "  'deterministic_bpr': []},\n",
       " 'times': []}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "torch.save(model.state_dict(), f'{outdir}/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NegativeBinomialRegressionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize model with correct parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model2  \u001b[38;5;241m=\u001b[39m \u001b[43mNegativeBinomialRegressionModel\u001b[49m(\n\u001b[1;32m      3\u001b[0m     num_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(geoids),\n\u001b[1;32m      4\u001b[0m     num_fixed_effects\u001b[38;5;241m=\u001b[39mtrain_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load saved weights\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model2\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NegativeBinomialRegressionModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize model with correct parameters\n",
    "model2  = NegativeBinomialRegressionModel(\n",
    "    num_locations=len(geoids),\n",
    "    num_fixed_effects=train_X.shape[2], device=device\n",
    ").to(device)\n",
    "\n",
    "# Load saved weights\n",
    "model2.load_state_dict(torch.load(f'{outdir}/best_model.pth', map_location=device))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()['beta'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>deaths_sp_lag</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>timestep.1</th>\n",
       "      <th>svi_theme1</th>\n",
       "      <th>svi_theme2</th>\n",
       "      <th>svi_theme3</th>\n",
       "      <th>svi_theme4</th>\n",
       "      <th>svi_total_</th>\n",
       "      <th>prev_deaths_05back</th>\n",
       "      <th>prev_deaths_04back</th>\n",
       "      <th>prev_deaths_03back</th>\n",
       "      <th>prev_deaths_02back</th>\n",
       "      <th>prev_deaths_01back</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geoid</th>\n",
       "      <th>timestep</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17031010100</th>\n",
       "      <th>2</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>42.021255</td>\n",
       "      <td>-87.669830</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7266</td>\n",
       "      <td>0.3552</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>0.7562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031010201</th>\n",
       "      <th>2</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>42.016008</td>\n",
       "      <td>-87.680148</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.5819</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031010202</th>\n",
       "      <th>2</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>42.016048</td>\n",
       "      <td>-87.673326</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7016</td>\n",
       "      <td>0.3417</td>\n",
       "      <td>0.8568</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031010300</th>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>42.015943</td>\n",
       "      <td>-87.666539</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.8379</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031010400</th>\n",
       "      <th>2</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>42.006411</td>\n",
       "      <td>-87.658816</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5516</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.6209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031843700</th>\n",
       "      <th>5</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>41.945560</td>\n",
       "      <td>-87.690034</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.5499</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.3357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031843800</th>\n",
       "      <th>5</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>41.801657</td>\n",
       "      <td>-87.640476</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8369</td>\n",
       "      <td>0.8133</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.6732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031843900</th>\n",
       "      <th>5</th>\n",
       "      <td>1.111111</td>\n",
       "      <td>41.778993</td>\n",
       "      <td>-87.576130</td>\n",
       "      <td>5</td>\n",
       "      <td>0.7841</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>0.8931</td>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031844600</th>\n",
       "      <th>5</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>41.813385</td>\n",
       "      <td>-87.625444</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6935</td>\n",
       "      <td>0.5114</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.7030</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17031844700</th>\n",
       "      <th>5</th>\n",
       "      <td>3.111111</td>\n",
       "      <td>41.854147</td>\n",
       "      <td>-87.711700</td>\n",
       "      <td>5</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>0.6778</td>\n",
       "      <td>0.9103</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.9760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5312 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      deaths_sp_lag   INTPTLAT   INTPTLON  timestep.1  \\\n",
       "geoid       timestep                                                    \n",
       "17031010100 2              0.600000  42.021255 -87.669830           2   \n",
       "17031010201 2              0.142857  42.016008 -87.680148           2   \n",
       "17031010202 2              0.625000  42.016048 -87.673326           2   \n",
       "17031010300 2              0.500000  42.015943 -87.666539           2   \n",
       "17031010400 2              0.750000  42.006411 -87.658816           2   \n",
       "...                             ...        ...        ...         ...   \n",
       "17031843700 5              0.307692  41.945560 -87.690034           5   \n",
       "17031843800 5              0.777778  41.801657 -87.640476           5   \n",
       "17031843900 5              1.111111  41.778993 -87.576130           5   \n",
       "17031844600 5              0.714286  41.813385 -87.625444           5   \n",
       "17031844700 5              3.111111  41.854147 -87.711700           5   \n",
       "\n",
       "                      svi_theme1  svi_theme2  svi_theme3  svi_theme4  \\\n",
       "geoid       timestep                                                   \n",
       "17031010100 2             0.7266      0.3552      0.6652      0.8699   \n",
       "17031010201 2             0.8108      0.5819      0.8880      0.9470   \n",
       "17031010202 2             0.7016      0.3417      0.8568      0.9801   \n",
       "17031010300 2             0.5695      0.1866      0.8379      0.9833   \n",
       "17031010400 2             0.5516      0.0382      0.4726      0.9984   \n",
       "...                          ...         ...         ...         ...   \n",
       "17031843700 5             0.3019      0.2730      0.5499      0.4423   \n",
       "17031843800 5             0.8750      0.8369      0.8133      0.0983   \n",
       "17031843900 5             0.7841      0.5141      0.8993      0.8931   \n",
       "17031844600 5             0.6935      0.5114      0.9146      0.7030   \n",
       "17031844700 5             0.9499      0.6778      0.9103      0.9653   \n",
       "\n",
       "                      svi_total_  prev_deaths_05back  prev_deaths_04back  \\\n",
       "geoid       timestep                                                       \n",
       "17031010100 2             0.7562                 0.0                 0.0   \n",
       "17031010201 2             0.9325                 0.0                 0.0   \n",
       "17031010202 2             0.8619                 0.0                 0.0   \n",
       "17031010300 2             0.7600                 0.0                 0.0   \n",
       "17031010400 2             0.6209                 0.0                 0.0   \n",
       "...                          ...                 ...                 ...   \n",
       "17031843700 5             0.3357                 0.0                 2.0   \n",
       "17031843800 5             0.6732                 0.0                 0.0   \n",
       "17031843900 5             0.8587                 0.0                 0.0   \n",
       "17031844600 5             0.7340                 0.0                 1.0   \n",
       "17031844700 5             0.9760                 0.0                 0.0   \n",
       "\n",
       "                      prev_deaths_03back  prev_deaths_02back  \\\n",
       "geoid       timestep                                           \n",
       "17031010100 2                        0.0                 0.0   \n",
       "17031010201 2                        0.0                 0.0   \n",
       "17031010202 2                        0.0                 0.0   \n",
       "17031010300 2                        0.0                 0.0   \n",
       "17031010400 2                        0.0                 0.0   \n",
       "...                                  ...                 ...   \n",
       "17031843700 5                        1.0                 1.0   \n",
       "17031843800 5                        0.0                 2.0   \n",
       "17031843900 5                        1.0                 1.0   \n",
       "17031844600 5                        0.0                 0.0   \n",
       "17031844700 5                        2.0                 5.0   \n",
       "\n",
       "                      prev_deaths_01back  \n",
       "geoid       timestep                      \n",
       "17031010100 2                        1.0  \n",
       "17031010201 2                        0.0  \n",
       "17031010202 2                        0.0  \n",
       "17031010300 2                        2.0  \n",
       "17031010400 2                        0.0  \n",
       "...                                  ...  \n",
       "17031843700 5                        0.0  \n",
       "17031843800 5                        0.0  \n",
       "17031843900 5                        0.0  \n",
       "17031844600 5                        1.0  \n",
       "17031844700 5                        6.0  \n",
       "\n",
       "[5312 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
