{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# \n",
    "# Modifications from original work\n",
    "# 29-03-2021 (tuero@ualberta.ca) : Convert Tensorflow code to PyTorch\n",
    "#\n",
    "# Copyright 2021 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Introduces differentiation via perturbations.\n",
    "\n",
    "Example of usage:\n",
    "\n",
    "  @perturbed\n",
    "  def sign_or(x, axis=-1):\n",
    "    s = ((torch.sign(x) + 1) / 2.0).type(torch.bool)\n",
    "    result = torch.any(s, dim=-1)\n",
    "    return result.type(torch.float) * 2.0 - 1\n",
    "\n",
    "\n",
    "Then sign_or is differentiable (unlike what it seems).\n",
    "\n",
    "It is possible to specify the parameters of the perturbations using:\n",
    "  @perturbed(num_samples=1000, sigma=0.1, noise='gumbel')\n",
    "  ...\n",
    "\n",
    "The decorator can also be used directly as a function, for example:\n",
    "  soft_argsort = perturbed(torch.argsort, num_samples=200, sigma=0.01)\n",
    "\"\"\"\n",
    "\n",
    "import functools\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.distributions.gumbel import Gumbel\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "_GUMBEL = 'gumbel'\n",
    "_NORMAL = 'normal'\n",
    "SUPPORTED_NOISES = (_GUMBEL, _NORMAL)\n",
    "\n",
    "\n",
    "def sample_noise_with_gradients(noise, shape):\n",
    "    \"\"\"Samples a noise tensor according to a distribution with its gradient.\n",
    "\n",
    "    Args:\n",
    "    noise: (str) a type of supported noise distribution.\n",
    "    shape: torch.tensor<int>, the shape of the tensor to sample.\n",
    "\n",
    "    Returns:\n",
    "    A tuple Tensor<float>[shape], Tensor<float>[shape] that corresponds to the\n",
    "    sampled noise and the gradient of log the underlying probability\n",
    "    distribution function. For instance, for a gaussian noise (normal), the\n",
    "    gradient is equal to the noise itself.\n",
    "\n",
    "    Raises:\n",
    "    ValueError in case the requested noise distribution is not supported.\n",
    "    See perturbations.SUPPORTED_NOISES for the list of supported distributions.\n",
    "    \"\"\"\n",
    "    if noise not in SUPPORTED_NOISES:\n",
    "        raise ValueError('{} noise is not supported. Use one of [{}]'.format(\n",
    "            noise, SUPPORTED_NOISES))\n",
    "\n",
    "    if noise == _GUMBEL:\n",
    "        sampler = Gumbel(0.0, 1.0)\n",
    "        samples = sampler.sample(shape)\n",
    "        gradients = 1 - torch.exp(-samples)\n",
    "    elif noise == _NORMAL:\n",
    "        sampler = Normal(0.0, 1.0)\n",
    "        samples = sampler.sample(shape)\n",
    "        gradients = samples\n",
    "\n",
    "    return samples, gradients\n",
    "\n",
    "\n",
    "def perturbed(func=None,\n",
    "              num_samples = 1000,\n",
    "              sigma = 0.05,\n",
    "              noise = _NORMAL,\n",
    "              batched = True,\n",
    "              device=None):\n",
    "    \"\"\"Turns a function into a differentiable one via perturbations.\n",
    "\n",
    "    The input function has to be the solution to a linear program for the trick\n",
    "    to work. For instance the maximum function, the logical operators or the ranks\n",
    "    can be expressed as solutions to some linear programs on some polytopes.\n",
    "    If this condition is violated though, the result would not hold and there is\n",
    "    no guarantee on the validity of the obtained gradients.\n",
    "\n",
    "    This function can be used directly or as a decorator.\n",
    "\n",
    "    Args:\n",
    "    func: the function to be turned into a perturbed and differentiable one.\n",
    "    Four I/O signatures for func are currently supported:\n",
    "        If batched is True,\n",
    "        (1) input [B, D1, ..., Dk], output [B, D1, ..., Dk], k >= 1\n",
    "        (2) input [B, D1, ..., Dk], output [B], k >= 1\n",
    "        If batched is False,\n",
    "        (3) input [D1, ..., Dk], output [D1, ..., Dk], k >= 1\n",
    "        (4) input [D1, ..., Dk], output [], k >= 1.\n",
    "    num_samples: the number of samples to use for the expectation computation.\n",
    "    sigma: the scale of the perturbation.\n",
    "    noise: a string representing the noise distribution to be used to sample\n",
    "    perturbations.\n",
    "    batched: whether inputs to the perturbed function will have a leading batch\n",
    "    dimension (True) or consist of a single example (False). Defaults to True.\n",
    "    device: The device to create tensors on (cpu/gpu). If None given, it will\n",
    "    default to gpu:0 if available, cpu otherwise.\n",
    "\n",
    "    Returns:\n",
    "    a function has the same signature as func but that can be back propagated.\n",
    "    \"\"\"\n",
    "    # If device not supplied, auto detect\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # This is a trick to have the decorator work both with and without arguments.\n",
    "    if func is None:\n",
    "        return functools.partial(\n",
    "            perturbed, num_samples=num_samples, sigma=sigma, noise=noise,\n",
    "            batched=batched, device=device)\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(input_tensor, *args):\n",
    "        class PerturbedFunc(torch.autograd.Function):\n",
    "\n",
    "            @staticmethod\n",
    "            def forward(ctx, input_tensor, *args):\n",
    "                original_input_shape = input_tensor.shape\n",
    "                if batched:\n",
    "                    if not input_tensor.dim() >= 2:\n",
    "                        raise ValueError('Batched inputs must have at least rank two')\n",
    "                else:  # Adds dummy batch dimension internally.\n",
    "                    input_tensor = input_tensor.unsqueeze(0)\n",
    "                input_shape = input_tensor.shape  # [B, D1, ... Dk], k >= 1\n",
    "                perturbed_input_shape = [num_samples] + list(input_shape)\n",
    "\n",
    "                noises = sample_noise_with_gradients(noise, perturbed_input_shape)\n",
    "                additive_noise, noise_gradient = tuple(\n",
    "                    [noise.type(input_tensor.dtype) for noise in noises])\n",
    "                additive_noise = additive_noise.to(device)\n",
    "                noise_gradient = noise_gradient.to(device)\n",
    "                perturbed_input = input_tensor.unsqueeze(0) + sigma * additive_noise\n",
    "\n",
    "                # [N, B, D1, ..., Dk] -> [NB, D1, ..., Dk].\n",
    "                flat_batch_dim_shape = [-1] + list(input_shape)[1:]\n",
    "                perturbed_input = torch.reshape(perturbed_input, flat_batch_dim_shape)\n",
    "                # Calls user-defined function in a perturbation agnostic manner.\n",
    "                perturbed_output = func(perturbed_input, *args)\n",
    "                # [NB, D1, ..., Dk] ->  [N, B, D1, ..., Dk].\n",
    "                perturbed_input = torch.reshape(perturbed_input, perturbed_input_shape)\n",
    "                # Either\n",
    "                #   (Default case): [NB, D1, ..., Dk] ->  [N, B, D1, ..., Dk]\n",
    "                # or\n",
    "                #   (Full-reduce case) [NB] -> [N, B]\n",
    "                perturbed_output_shape = [num_samples, -1] + list(perturbed_output.shape)[1:]\n",
    "                perturbed_output = torch.reshape(perturbed_output, perturbed_output_shape)\n",
    "\n",
    "                forward_output = torch.mean(perturbed_output, dim=0)\n",
    "                if not batched:  # Removes dummy batch dimension.\n",
    "                    forward_output = forward_output[0]\n",
    "\n",
    "                # Save context for backward pass\n",
    "                ctx.save_for_backward(perturbed_input, perturbed_output, noise_gradient)\n",
    "                ctx.original_input_shape = original_input_shape\n",
    "\n",
    "                return forward_output\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, dy):\n",
    "                # Pull saved tensors\n",
    "                original_input_shape = ctx.original_input_shape\n",
    "                perturbed_input, perturbed_output, noise_gradient = ctx.saved_tensors\n",
    "                output, noise_grad = perturbed_output, noise_gradient\n",
    "                # Adds dummy feature/channel dimension internally.\n",
    "                if perturbed_input.dim() > output.dim():\n",
    "                    dy = dy.unsqueeze(-1)\n",
    "                    output = output.unsqueeze(-1)\n",
    "                # Adds dummy batch dimension internally.\n",
    "                if not batched:\n",
    "                    dy = dy.unsqueeze(0)\n",
    "                # Flattens [D1, ..., Dk] to a single feat dim [D].\n",
    "                flatten = lambda t: torch.reshape(t, (list(t.shape)[0], list(t.shape)[1], -1))\n",
    "                dy = torch.reshape(dy, (list(dy.shape)[0], -1))  # (B, D)\n",
    "                output = flatten(output)  # (N, B, D)\n",
    "                noise_grad = flatten(noise_grad)  # (N, B, D)\n",
    "                print(noise_grad.dtype)\n",
    "                print(output.dtype)\n",
    "                print(dy.dtype)\n",
    "                g = torch.einsum('nbd,nb->bd', noise_grad, torch.einsum('nbd,bd->nb', output, dy))\n",
    "                g /= sigma * num_samples\n",
    "                return torch.reshape(g, original_input_shape)\n",
    "\n",
    "        return PerturbedFunc.apply(input_tensor, *args)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_hot_indicator(x):\n",
    "    k=100\n",
    "    topk = torch.topk(x, k=k, dim=-1, sorted=False)\n",
    "    indices = topk.indices\n",
    "    # convert to k-hot indicator with onehot function\n",
    "    one_hot = torch.nn.functional.one_hot(indices, num_classes=x.shape[-1]).float()\n",
    "    khot = torch.sum(one_hot, dim=-2)\n",
    "    return khot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_khot = perturbed(top_k_hot_indicator, num_samples=1000, sigma=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "top_k_hot_indicator() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]])\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoft_khot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[2], line 203\u001b[0m, in \u001b[0;36mperturbed.<locals>.wrapper\u001b[0;34m(input_tensor, *args)\u001b[0m\n\u001b[1;32m    200\u001b[0m         g \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m*\u001b[39m num_samples\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mreshape(g, original_input_shape)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPerturbedFunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[2], line 158\u001b[0m, in \u001b[0;36mperturbed.<locals>.wrapper.<locals>.PerturbedFunc.forward\u001b[0;34m(ctx, input_tensor, *args)\u001b[0m\n\u001b[1;32m    156\u001b[0m perturbed_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(perturbed_input, flat_batch_dim_shape)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Calls user-defined function in a perturbation agnostic manner.\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m perturbed_output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperturbed_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# [NB, D1, ..., Dk] ->  [N, B, D1, ..., Dk].\u001b[39;00m\n\u001b[1;32m    160\u001b[0m perturbed_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(perturbed_input, perturbed_input_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: top_k_hot_indicator() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,3,2],[5,3,4]])\n",
    "print(soft_khot(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform top-k pooling on the perturbed tensor\n",
    "topk_results = torch.topk(x, k=2, dim=-1, sorted=False)\n",
    "\n",
    "# Get the indices of the top k elements\n",
    "indices = topk_results.indices # b, nS, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [0, 2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(indices, num_classes=3).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.sort(indices,dim=-1).values, num_classes=3).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/cluster/home/kheuto01/code/prob_diff_topk'\n",
    "deaths = pd.read_csv(os.path.join(data_dir,'deaths_band.csv'))\n",
    "deaths_TS = deaths.pivot(index='time', columns='geoid', values='death').values\n",
    "perturbed_noise=0.1\n",
    "num_pert_samples=100\n",
    "\n",
    "perturbed_top_K_func = perturbed(top_k_hot_indicator, num_samples=num_pert_samples, sigma=perturbed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths = torch.tensor(deaths_TS)#, dtype=torch.float32)\n",
    "param = torch.tensor([0.2])\n",
    "param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-09-11 11:56:36 87816:87816 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CPUAllocator.cpp:249] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "STAGE:2024-09-11 11:56:43 87816:87816 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-09-11 11:56:43 87816:87816 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m deaths \u001b[38;5;241m-\u001b[39m top_K_ids\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/function.py:301\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 199\u001b[0m, in \u001b[0;36mperturbed.<locals>.wrapper.<locals>.PerturbedFunc.backward\u001b[0;34m(ctx, dy)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(dy\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 199\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnbd,nb->bd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnbd,bd->nb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m g \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m*\u001b[39m num_samples\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mreshape(g, original_input_shape)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "from torch.autograd import profiler\n",
    "\n",
    "with torch.profiler.profile(profile_memory=True,\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "    ]\n",
    ") as p:\n",
    "    top_K_ids = perturbed_top_K_func(torch.ones_like(deaths)*param)\n",
    "\n",
    "    top_K_ids = top_K_ids.sum(-2)\n",
    "    loss = deaths - top_K_ids\n",
    "    loss = loss.sum()\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-644930.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0865, 0.0482, 0.1269,  ..., 0.1657, 0.1492, 0.1586],\n",
       "        [0.0326, 0.0820, 0.0783,  ..., 0.1588, 0.0380, 0.0788],\n",
       "        [0.0099, 0.1627, 0.0144,  ..., 0.1459, 0.0746, 0.2378],\n",
       "        ...,\n",
       "        [0.1502, 0.6156, 1.7503,  ..., 0.1954, 0.2495, 0.2577],\n",
       "        [0.5638, 1.9193, 3.6524,  ..., 0.1098, 0.2285, 0.1249],\n",
       "        [2.1141, 3.9075, 4.0316,  ..., 0.1666, 0.1483, 0.1566]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_top_K_func(deaths,2).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                            aten::empty         0.00%       1.035ms         0.00%       1.035ms      86.250us      78.41 Gb      78.41 Gb            12  \n",
      "                                    aten::empty_strided         0.00%      71.000us         0.00%      71.000us       7.889us      38.62 Gb      38.62 Gb             9  \n",
      "                                              aten::mul         0.15%      99.509ms         0.15%      99.535ms      33.178ms     396.30 Mb     396.30 Mb             3  \n",
      "                                              aten::sum         9.61%        6.209s         9.77%        6.314s        1.052s     395.52 Mb     395.52 Mb             6  \n",
      "                                              aten::add         0.17%     109.593ms         0.17%     109.593ms     109.593ms     395.51 Mb     395.51 Mb             1  \n",
      "                                             aten::topk         1.06%     685.906ms         1.06%     685.906ms     685.906ms      73.24 Mb      73.24 Mb             1  \n",
      "                                              aten::bmm         0.20%     129.232ms         0.20%     129.232ms      64.616ms     655.00 Kb     655.00 Kb             2  \n",
      "                                             aten::mean         0.00%      78.000us         0.11%      73.962ms      73.962ms     405.00 Kb     405.00 Kb             1  \n",
      "                                              aten::sub         0.00%     249.000us         0.00%     249.000us     249.000us     405.00 Kb     405.00 Kb             1  \n",
      "                                              aten::neg         0.00%     169.000us         0.00%     169.000us     169.000us     405.00 Kb     405.00 Kb             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 64.609s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(p.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
