{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 18:25:37.715545: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-08 18:25:37.779106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 18:25:37.779142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 18:25:37.780249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 18:25:37.788536: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 18:25:48.675553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, Poisson, MixtureSameFamily\n",
    "# Cd to code\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/cluster/home/kheuto01/code/prob_diff_topk')\n",
    "sys.path.append('/cluster/home/kheuto01/code/prob_diff_topk')\n",
    "\n",
    "from datasets import example_datasets, to_numpy\n",
    "from torch_perturb.torch_pert_topk import PerturbedTopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=360\n",
    "# tracts/distributions\n",
    "S=12\n",
    "# history/features\n",
    "H = 3\n",
    "# total timepoints\n",
    "T= 500\n",
    "num_components=4\n",
    "K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 18:26:23.937274: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-07-08 18:26:23.937312: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: s1cmp008.pax.tufts.edu\n",
      "2024-07-08 18:26:23.937317: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: s1cmp008.pax.tufts.edu\n",
      "2024-07-08 18:26:23.937353: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.129.3\n",
      "2024-07-08 18:26:23.937380: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.129.3\n",
      "2024-07-08 18:26:23.937386: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.129.3\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = example_datasets(H, T, seed=seed)\n",
    "train_X_THS, train_y_TS = to_numpy(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MixtureOfPoissonsModel(nn.Module):\n",
    "    def __init__(self, num_components=4, S=12):\n",
    "        super(MixtureOfPoissonsModel, self).__init__()\n",
    "        self.num_components = num_components\n",
    "        self.S = S\n",
    "        \n",
    "        # Initialize the log rates and mixture probabilities as learnable parameters\n",
    "        self.log_poisson_rates = nn.Parameter(torch.rand(num_components))  # Initialize log rates\n",
    "        self.mixture_probs = nn.Parameter(torch.rand(S, num_components))  # Initialize probabilities\n",
    "\n",
    "    def params_to_single_tensor(self):\n",
    "        return torch.cat([self.log_poisson_rates, self.mixture_probs.view(-1)])\n",
    "    \n",
    "    def single_tensor_to_params(self, single_tensor):\n",
    "        log_poisson_rates = single_tensor[:self.num_components]\n",
    "        mixture_probs = single_tensor[self.num_components:].view(self.S, self.num_components)\n",
    "        return log_poisson_rates, mixture_probs\n",
    "    \n",
    "    def update_params(self, single_tensor):\n",
    "        log_poisson_rates, mixture_probs = self.single_tensor_to_params(single_tensor)\n",
    "        self.log_poisson_rates = nn.Parameter(log_poisson_rates)\n",
    "        self.mixture_probs = nn.Parameter(mixture_probs)\n",
    "        return\n",
    "    \n",
    "    def build_from_single_tensor(self, single_tensor):\n",
    "        log_poisson_rates, mixture_probs = self.single_tensor_to_params(single_tensor)\n",
    "        poisson_rates = torch.exp(log_poisson_rates)\n",
    "        mixture_probs_normalized = torch.nn.functional.softmax(mixture_probs, dim=1)\n",
    "        categorical_dist = Categorical(mixture_probs_normalized)\n",
    "        expanded_rates = poisson_rates.expand(self.S, self.num_components)\n",
    "        poisson_dist = Poisson(expanded_rates, validate_args=False)\n",
    "        mixture_dist = MixtureSameFamily(categorical_dist, poisson_dist)\n",
    "        return mixture_dist\n",
    "        \n",
    "    def forward(self):\n",
    "        # Transform log rates to rates\n",
    "        poisson_rates = torch.exp(self.log_poisson_rates)\n",
    "        \n",
    "        # Normalize mixture_probs to sum to 1 across the components\n",
    "        mixture_probs_normalized = torch.nn.functional.softmax(self.mixture_probs, dim=1)\n",
    "        \n",
    "        # Create the Categorical distribution with the normalized probabilities\n",
    "        categorical_dist = Categorical(mixture_probs_normalized)\n",
    "        \n",
    "        # Expand the Poisson rates to match the number of samples\n",
    "        expanded_rates = poisson_rates.expand(self.S, self.num_components)\n",
    "        \n",
    "        # Create the Poisson distribution with the expanded rates\n",
    "        poisson_dist = Poisson(expanded_rates, validate_args=False)\n",
    "        \n",
    "        # Create the MixtureSameFamily distribution\n",
    "        mixture_dist = MixtureSameFamily(categorical_dist, poisson_dist,  validate_args=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return mixture_dist\n",
    "    \n",
    "def torch_bpr_uncurried(y_pred, y_true, K=4, perturbed_top_K_func=None):\n",
    "\n",
    "    top_K_ids = perturbed_top_K_func(y_pred)\n",
    "    # Sum over k dim\n",
    "    top_K_ids = top_K_ids.sum(dim=-2)\n",
    "\n",
    "    true_top_K_val, _  = torch.topk(y_true, K) \n",
    "    denominator = torch.sum(true_top_K_val, dim=-1)\n",
    "    numerator = torch.sum(top_K_ids * y_true, dim=-1)\n",
    "    bpr = numerator/denominator\n",
    "\n",
    "    return bpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_log_rates = torch.log(torch.tensor([0+1e-8, 7, 10, 100]))\n",
    "ideal_mix_weights = torch.log(1e-13 + torch.tensor(\n",
    "                                [[0,1,0,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.9,0,0,0.1],\n",
    "                                 [0.9,0,0,0.1],\n",
    "                                 [0.9,0,0,0.1],\n",
    "                                 [0.9,0,0,0.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_log_rates = torch.log(torch.tensor([0+1e-8, 7, 10, 100]))\n",
    "bpr_mix_weights = torch.log(1e-13 + torch.tensor(\n",
    "                                [[0,0,0,1],\n",
    "                                 [0,0,0,1],\n",
    "                                 [0,0,0,1],\n",
    "                                 [0,0,0,1],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0],\n",
    "                                 [1,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MixtureOfPoissonsModel()\n",
    "#model.update_params(torch.cat([ideal_log_rates, ideal_mix_weights.view(-1)]))\n",
    "model.update_params(torch.cat([bpr_log_rates, bpr_mix_weights.view(-1)]))\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_score_func =  100\n",
    "M_action = 100\n",
    "train_T = train_y_TS.shape[0]\n",
    "perturbed_top_K_func = PerturbedTopK(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([106., 115., 101., 114.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_model = model()\n",
    "mix_model.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-29.9336, -29.9336, -29.9336,   0.0000],\n",
       "        [-29.9336, -29.9336, -29.9336,   0.0000],\n",
       "        [-29.9336, -29.9336, -29.9336,   0.0000],\n",
       "        [-29.9336, -29.9336, -29.9336,   0.0000],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336],\n",
       "        [  0.0000, -29.9336, -29.9336, -29.9336]], requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mixture_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-13, 1.0000e-13, 1.0000e-13, 1.0000e+00],\n",
       "        [1.0000e-13, 1.0000e-13, 1.0000e-13, 1.0000e+00],\n",
       "        [1.0000e-13, 1.0000e-13, 1.0000e-13, 1.0000e+00],\n",
       "        [1.0000e-13, 1.0000e-13, 1.0000e-13, 1.0000e+00],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13],\n",
       "        [1.0000e+00, 1.0000e-13, 1.0000e-13, 1.0000e-13]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(model.mixture_probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg bpr: -0.5936790704727173\n",
      "Log prob: -1.0909003019332886\n",
      "Loss: -1.6845793724060059\n",
      "Neg bpr: -0.5936261415481567\n",
      "Log prob: -1.0909314155578613\n",
      "Loss: -1.684557557106018\n",
      "Neg bpr: -0.593609631061554\n",
      "Log prob: -1.0920900106430054\n",
      "Loss: -1.685699701309204\n",
      "Neg bpr: -0.5935839414596558\n",
      "Log prob: -1.0907151699066162\n",
      "Loss: -1.684299111366272\n",
      "Neg bpr: -0.5935624241828918\n",
      "Log prob: -1.0903267860412598\n",
      "Loss: -1.6838891506195068\n",
      "Neg bpr: -0.5936207175254822\n",
      "Log prob: -1.0907315015792847\n",
      "Loss: -1.684352159500122\n",
      "Neg bpr: -0.5935508608818054\n",
      "Log prob: -1.0908491611480713\n",
      "Loss: -1.6844000816345215\n",
      "Neg bpr: -0.5935357213020325\n",
      "Log prob: -1.0912786722183228\n",
      "Loss: -1.684814453125\n",
      "Neg bpr: -0.5936082601547241\n",
      "Log prob: -1.0910122394561768\n",
      "Loss: -1.6846204996109009\n",
      "Neg bpr: -0.5935199856758118\n",
      "Log prob: -1.0908797979354858\n",
      "Loss: -1.6843998432159424\n",
      "Neg bpr: -0.5935778021812439\n",
      "Log prob: -1.090424656867981\n",
      "Loss: -1.68400239944458\n",
      "Neg bpr: -0.5936400294303894\n",
      "Log prob: -1.0910803079605103\n",
      "Loss: -1.6847202777862549\n",
      "Neg bpr: -0.5935747027397156\n",
      "Log prob: -1.0919876098632812\n",
      "Loss: -1.6855623722076416\n",
      "Neg bpr: -0.5936239361763\n",
      "Log prob: -1.0902137756347656\n",
      "Loss: -1.683837652206421\n",
      "Neg bpr: -0.5935901999473572\n",
      "Log prob: -1.0917563438415527\n",
      "Loss: -1.6853466033935547\n",
      "Neg bpr: -0.5935729146003723\n",
      "Log prob: -1.0914472341537476\n",
      "Loss: -1.6850202083587646\n",
      "Neg bpr: -0.5935602188110352\n",
      "Log prob: -1.0905399322509766\n",
      "Loss: -1.6841001510620117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     log_probs_TMS \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(y_sample_TMS)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_probs_TMS\n\u001b[0;32m---> 20\u001b[0m jac_TMSP \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_log_probs_baked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_to_single_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward-mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m score_func_estimator_TMSP \u001b[38;5;241m=\u001b[39m jac_TMSP \u001b[38;5;241m*\u001b[39m ratio_rating_TMS\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m score_func_estimator_TSP \u001b[38;5;241m=\u001b[39m score_func_estimator_TMSP\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)    \n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:668\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_graph:\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.autograd.functional.jacobian: `create_graph=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand `strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` are not supported together (yet). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either set `create_graph=False` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jacfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    671\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:545\u001b[0m, in \u001b[0;36m_jacfwd\u001b[0;34m(func, inputs, strict, vectorize)\u001b[0m\n\u001b[1;32m    542\u001b[0m         output_info\u001b[38;5;241m.\u001b[39mappend(primal_outs)\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(jv)\n\u001b[0;32m--> 545\u001b[0m outputs_before_split \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m output_info\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Step 3: for each of the output tangents, split along dim 0\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/_vmap_internals.py:223\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    221\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    225\u001b[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:530\u001b[0m, in \u001b[0;36m_jacfwd.<locals>.jvp\u001b[0;34m(tangents)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fwAD\u001b[38;5;241m.\u001b[39mdual_level():\n\u001b[1;32m    525\u001b[0m     dual_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    526\u001b[0m         fwAD\u001b[38;5;241m.\u001b[39mmake_dual(\u001b[38;5;28minput\u001b[39m, tangent\u001b[38;5;241m.\u001b[39mview_as(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, tangent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, tangents)\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m     _is_outputs_tuple, dual_outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[0;32m--> 530\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdual_inputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m     output_info\u001b[38;5;241m.\u001b[39mappend(_is_outputs_tuple)\n\u001b[1;32m    533\u001b[0m     jv \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[80], line 16\u001b[0m, in \u001b[0;36mget_log_probs_baked\u001b[0;34m(param)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_log_probs_baked\u001b[39m(param):\n\u001b[1;32m     15\u001b[0m     distribution \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbuild_from_single_tensor(param)\n\u001b[0;32m---> 16\u001b[0m     log_probs_TMS \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_sample_TMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_probs_TMS\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/mixture_same_family.py:163\u001b[0m, in \u001b[0;36mMixtureSameFamily.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(x)\n\u001b[1;32m    162\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad(x)\n\u001b[0;32m--> 163\u001b[0m log_prob_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponent_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [S, B, k]\u001b[39;00m\n\u001b[1;32m    164\u001b[0m log_mix_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixture_distribution\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    166\u001b[0m )  \u001b[38;5;66;03m# [B, k]\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogsumexp(log_prob_x \u001b[38;5;241m+\u001b[39m log_mix_prob, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/poisson.py:70\u001b[0m, in \u001b[0;36mPoisson.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[1;32m     69\u001b[0m rate, value \u001b[38;5;241m=\u001b[39m broadcast_all(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate, value)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mxlogy(rate) \u001b[38;5;241m-\u001b[39m rate \u001b[38;5;241m-\u001b[39m \u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlgamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    mix_model = model()\n",
    "    \n",
    "    y_sample_TMS = mix_model.sample((train_T, M_score_func))\n",
    "    y_sample_action_TMS = mix_model.sample((train_T, M_action))\n",
    "\n",
    "    ratio_rating_TMS = y_sample_TMS/y_sample_TMS.sum(dim=-1, keepdim=True)\n",
    "    ratio_rating_TS =  ratio_rating_TMS.mean(dim=1)\n",
    "    ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "    pred_y_TS = torch.mean(y_sample_action_TMS, dim=1)\n",
    "    pred_y_TS.requires_grad_(True)\n",
    "\n",
    "    def get_log_probs_baked(param):\n",
    "        distribution = model.build_from_single_tensor(param)\n",
    "        log_probs_TMS = distribution.log_prob(y_sample_TMS)\n",
    "\n",
    "        return log_probs_TMS\n",
    "\n",
    "    jac_TMSP = torch.autograd.functional.jacobian(get_log_probs_baked, (model.params_to_single_tensor()), strategy='forward-mode', vectorize=True)\n",
    "\n",
    "    score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "    score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)    \n",
    "\n",
    "    # get gradient of negative bpr_t  with respect to ratio rating_TS\n",
    "    positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS), K=4, perturbed_top_K_func=perturbed_top_K_func)\n",
    "    negative_bpr = torch.mean(-positive_bpr_T)\n",
    "    \n",
    "    log_prob = torch.mean(mix_model.log_prob(pred_y_TS))\n",
    "\n",
    "    print(f'Neg bpr: {negative_bpr}')\n",
    "    print(f'Log prob: {log_prob}')\n",
    "\n",
    "    loss = negative_bpr + log_prob\n",
    "    print(f'Loss: {loss}')\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    loss_grad_TS = ratio_rating_TS.grad\n",
    "\n",
    "    gradient_TSP = score_func_estimator_TSP * torch.unsqueeze(loss_grad_TS, -1)\n",
    "    gradient_P = torch.sum(gradient_TSP, dim=[0,1])\n",
    "    model.update_params(model.params_to_single_tensor() - 0.0005 * gradient_P)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg bpr: -0.48453548550605774\n",
      "Log prob: -3.0042552947998047\n",
      "Loss: -3.48879075050354\n"
     ]
    }
   ],
   "source": [
    "    mix_model = model()\n",
    "    \n",
    "    y_sample_TMS = mix_model.sample((train_T, M_score_func))\n",
    "    y_sample_action_TMS = mix_model.sample((train_T, M_action))\n",
    "\n",
    "    ratio_rating_TMS = y_sample_action_TMS/y_sample_action_TMS.sum(dim=-1, keepdim=True)\n",
    "    ratio_rating_TS =  ratio_rating_TMS.mean(dim=1)\n",
    "    ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "    pred_y_TS = torch.mean(y_sample_action_TMS, dim=1)\n",
    "    pred_y_TS.requires_grad_(True)\n",
    "\n",
    "    def get_log_probs_baked(param):\n",
    "        distribution = model.build_from_single_tensor(param)\n",
    "        log_probs_TMS = distribution.log_prob(y_sample_TMS)\n",
    "\n",
    "        return log_probs_TMS\n",
    "\n",
    "    get_log_probs_baked(model.params_to_single_tensor())\n",
    "    jac_TMSP = torch.autograd.functional.jacobian(get_log_probs_baked, (model.params_to_single_tensor()), strategy='forward-mode', vectorize=True)\n",
    "\n",
    "    score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "    score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)    \n",
    "\n",
    "    # get gradient of negative bpr_t  with respect to ratio rating_TS\n",
    "    positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS), K=4, perturbed_top_K_func=perturbed_top_K_func)\n",
    "    negative_bpr = torch.mean(-positive_bpr_T)\n",
    "    \n",
    "    log_prob = torch.mean(mix_model.log_prob(pred_y_TS))\n",
    "\n",
    "    print(f'Neg bpr: {negative_bpr}')\n",
    "    print(f'Log prob: {log_prob}')\n",
    "\n",
    "    loss = negative_bpr + log_prob\n",
    "    print(f'Loss: {loss}')\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    loss_grad_TS = ratio_rating_TS.grad\n",
    "\n",
    "    gradient_TSP = score_func_estimator_TSP * torch.unsqueeze(loss_grad_TS, -1)\n",
    "    gradient_P = torch.sum(gradient_TSP, dim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2520)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch_bpr_uncurried(torch.tensor([[0,0,0,0,0,0,0,0,1,1,1,1]])*torch.ones_like(torch.tensor(train_y_TS)),torch.tensor(train_y_TS),K=4, perturbed_top_K_func=perturbed_top_K_func))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2153846153846154"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28/130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "M =  1000\n",
    "train_T = train_y_TS.shape[0]\n",
    "y_sample_TMS = mix_model.sample((train_T, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0829, 0.0823, 0.0825,  ..., 0.0791, 0.0794, 0.0867],\n",
       "        [0.0837, 0.0819, 0.0831,  ..., 0.0869, 0.0860, 0.0828],\n",
       "        [0.0862, 0.0787, 0.0771,  ..., 0.0796, 0.0818, 0.0868],\n",
       "        ...,\n",
       "        [0.0798, 0.0823, 0.0776,  ..., 0.0822, 0.0822, 0.0849],\n",
       "        [0.0867, 0.0820, 0.0799,  ..., 0.0804, 0.0839, 0.0822],\n",
       "        [0.0847, 0.0793, 0.0790,  ..., 0.0841, 0.0791, 0.0862]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_rating_TMS = y_sample_TMS/y_sample_TMS.sum(dim=-1, keepdim=True)\n",
    "ratio_rating_TS =  ratio_rating_TMS.mean(dim=1)\n",
    "ratio_rating_TS.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs_baked(param):\n",
    "    distribution = model.build_from_single_tensor(param)\n",
    "    log_probs_TMS = distribution.log_prob(y_sample_TMS)\n",
    "\n",
    "    return log_probs_TMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac_TMSP = torch.autograd.functional.jacobian(get_log_probs_baked, (model.params_to_single_tensor()), strategy='forward-mode', vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_top_K_func = PerturbedTopK(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gradient of negative bpr_t  with respect to ratio rating_TS\n",
    "positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS), K=4, perturbed_top_K_func=perturbed_top_K_func)\n",
    "negative_bpr = torch.sum(-positive_bpr_T)\n",
    "negative_bpr.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_TS = ratio_rating_TS.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_TSP = score_func_estimator_TSP * torch.unsqueeze(loss_grad_TS, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_P = torch.sum(gradient_TSP, dim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_params(model.params_to_single_tensor() - 0.05 * gradient_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OptimizerPostHook',\n",
       " 'OptimizerPreHook',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cuda_graph_capture_health_check',\n",
       " '_group_tensors_by_device_and_dtype',\n",
       " '_init_group',\n",
       " '_optimizer_load_state_dict_post_hooks',\n",
       " '_optimizer_load_state_dict_pre_hooks',\n",
       " '_optimizer_state_dict_post_hooks',\n",
       " '_optimizer_state_dict_pre_hooks',\n",
       " '_optimizer_step_code',\n",
       " '_optimizer_step_post_hooks',\n",
       " '_optimizer_step_pre_hooks',\n",
       " '_patch_step_function',\n",
       " '_process_value_according_to_param_policy',\n",
       " '_warned_capturable_if_run_uncaptured',\n",
       " '_zero_grad_profile_name',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'profile_hook_step',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'register_step_post_hook',\n",
       " 'register_step_pre_hook',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
