{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_K(y_pred_BD, y_true_BD, K=4, perturbed_top_K_func=None):\n",
    "\n",
    "    top_K_ids_BKD = perturbed_top_K_func(y_pred_BD)\n",
    "    # Sum over k dim\n",
    "    top_K_ids_BD = top_K_ids_BKD.sum(dim=-2)\n",
    "\n",
    "    true_top_K_val_BD, _  = torch.topk(y_true_BD, K) \n",
    "    denominator_B = torch.sum(true_top_K_val_BD, dim=-1)\n",
    "    numerator_B = torch.sum(top_K_ids_BD * y_true_BD, dim=-1)\n",
    "    bpr_B = numerator_B/denominator_B\n",
    "\n",
    "    return bpr_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    b, d = x.shape\n",
    "    \n",
    "    # Generate Gaussian noise with specified number of samples and standard deviation\n",
    "    noise = torch.normal(mean=0.0, std=1.0, size=(b, num_samples, d)).to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbedTopK(nn.Module):\n",
    "\n",
    "    def __init__(self, k: int, noise: torch.tensor, sigma: float = 0.05, num_samples: int = 500):\n",
    "        super(PerturbedTopK, self).__init__()\n",
    "    \n",
    "        self.noise = noise\n",
    "        self.k = k\n",
    "        self.sigma = sigma\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Return the output of the PerturbedTopKFunction, applied to the input tensor\n",
    "        # using the k, num_samples, and sigma attributes as arguments\n",
    "        return PerturbedTopKFunction.apply(x, self.k, self.noise, self.sigma, self.num_samples)\n",
    "\n",
    "class PerturbedTopKFunction(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, k: int, noise: torch.tensor, sigma: float = 0.05, num_samples: int = 500):\n",
    "    \n",
    "        b, d = x.shape\n",
    " \n",
    "        # Add noise to the input tensor\n",
    "        perturbed_x = x[:, None, :] + noise * sigma # b, nS, d\n",
    "        \n",
    "        # Perform top-k pooling on the perturbed tensor\n",
    "        topk_results = torch.topk(perturbed_x, k=k, dim=-1, sorted=False)\n",
    "        \n",
    "        # Get the indices of the top k elements\n",
    "        indices = topk_results.indices # b, nS, k\n",
    "        \n",
    "        # Sort the indices in ascending order\n",
    "        indices = torch.sort(indices, dim=-1).values # b, nS, k\n",
    "\n",
    "        # Convert the indices to one-hot tensors\n",
    "        perturbed_output = torch.nn.functional.one_hot(indices, num_classes=d).float()\n",
    "        \n",
    "        # Average the one-hot tensors to get the final output\n",
    "        indicators = perturbed_output.mean(dim=1) # b, k, d\n",
    "\n",
    "        # Save constants and tensors for backward pass\n",
    "        ctx.k = k\n",
    "        ctx.num_samples = num_samples\n",
    "        ctx.sigma = sigma\n",
    "\n",
    "        ctx.perturbed_output = perturbed_output\n",
    "        ctx.noise = noise\n",
    "\n",
    "        return indicators\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # If there is no gradient to backpropagate, return tuple of None values\n",
    "        if grad_output is None:\n",
    "            return tuple([None] * 5)\n",
    "\n",
    "        noise_gradient = ctx.noise\n",
    "        \n",
    "        # Calculate expected gradient\n",
    "        expected_gradient = (\n",
    "            torch.einsum(\"bnkd,bne->bkde\", ctx.perturbed_output, noise_gradient)\n",
    "            / ctx.num_samples\n",
    "            / ctx.sigma\n",
    "        ) * float(ctx.k)\n",
    "        \n",
    "        grad_input = torch.einsum(\"bkd,bkde->be\", grad_output, expected_gradient)\n",
    "        \n",
    "        return (grad_input,) + tuple([None] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch dimension\n",
    "B = 100\n",
    "# feature dimension\n",
    "D = 10\n",
    "# top-K\n",
    "K=4\n",
    "rating_BD = torch.rand(B, D)\n",
    "labels_BD = torch.rand(B, D)\n",
    "\n",
    "\n",
    "sigma = 0.05\n",
    "num_samples=500\n",
    "\n",
    "    \n",
    "# Generate Gaussian noise with specified number of samples and standard deviation\n",
    "noise = torch.normal(mean=0.0, std=1.0, size=(B, num_samples, D)).to(rating_BD.device)\n",
    "\n",
    "perturbed_top_K_func = PerturbedTopK(k=K, noise=noise, num_samples=num_samples, sigma=sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(rating_BD):\n",
    "    \"\"\"Compute loss. Labels are not an input so we can just get gradient of rating\"\"\"\n",
    "    return -bpr_K(rating_BD, labels_BD, perturbed_top_K_func=perturbed_top_K_func, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "You must implement the jvp function for custom autograd.Function to use it with forward mode AD.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Goal: gradient of loss w.r.t. rating_BD for each B separately\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m grad_loss_TD \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrating_BD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward-mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:668\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_graph:\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.autograd.functional.jacobian: `create_graph=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand `strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` are not supported together (yet). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either set `create_graph=False` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jacfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    671\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:545\u001b[0m, in \u001b[0;36m_jacfwd\u001b[0;34m(func, inputs, strict, vectorize)\u001b[0m\n\u001b[1;32m    542\u001b[0m         output_info\u001b[38;5;241m.\u001b[39mappend(primal_outs)\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(jv)\n\u001b[0;32m--> 545\u001b[0m outputs_before_split \u001b[38;5;241m=\u001b[39m \u001b[43m_vmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m is_outputs_tuple, outputs \u001b[38;5;241m=\u001b[39m output_info\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m# Step 3: for each of the output tangents, split along dim 0\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/_vmap_internals.py:223\u001b[0m, in \u001b[0;36m_vmap.<locals>.wrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     batched_inputs, batch_size \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    221\u001b[0m         in_dims, args, vmap_level, func\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_none_pass_through:\n\u001b[1;32m    225\u001b[0m         _validate_outputs(batched_outputs, func)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:530\u001b[0m, in \u001b[0;36m_jacfwd.<locals>.jvp\u001b[0;34m(tangents)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fwAD\u001b[38;5;241m.\u001b[39mdual_level():\n\u001b[1;32m    525\u001b[0m     dual_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    526\u001b[0m         fwAD\u001b[38;5;241m.\u001b[39mmake_dual(\u001b[38;5;28minput\u001b[39m, tangent\u001b[38;5;241m.\u001b[39mview_as(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, tangent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, tangents)\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m     _is_outputs_tuple, dual_outputs \u001b[38;5;241m=\u001b[39m _as_tuple(\n\u001b[0;32m--> 530\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdual_inputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m     output_info\u001b[38;5;241m.\u001b[39mappend(_is_outputs_tuple)\n\u001b[1;32m    533\u001b[0m     jv \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(rating_BD)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(rating_BD):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss. Labels are not an input so we can just get gradient of rating\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[43mbpr_K\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrating_BD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_BD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbed_top_K_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturbed_top_K_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mbpr_K\u001b[0;34m(y_pred_BD, y_true_BD, K, perturbed_top_K_func)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbpr_K\u001b[39m(y_pred_BD, y_true_BD, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, perturbed_top_K_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     top_K_ids_BKD \u001b[38;5;241m=\u001b[39m \u001b[43mperturbed_top_K_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_BD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Sum over k dim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     top_K_ids_BD \u001b[38;5;241m=\u001b[39m top_K_ids_BKD\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mPerturbedTopK.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Return the output of the PerturbedTopKFunction, applied to the input tensor\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# using the k, num_samples, and sigma attributes as arguments\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPerturbedTopKFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/function.py:308\u001b[0m, in \u001b[0;36mBackwardCFunction.apply_jvp\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03mApply method used when executing forward mode AD during the forward\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/function.py:477\u001b[0m, in \u001b[0;36m_SingleLevelFunction.jvp\u001b[0;34m(ctx, *grad_inputs)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjvp\u001b[39m(ctx: Any, \u001b[38;5;241m*\u001b[39mgrad_inputs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    461\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define a formula for differentiating the operation with forward mode automatic differentiation.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    This function is to be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m    functions.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must implement the jvp function for custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautograd.Function to use it with forward mode AD.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: You must implement the jvp function for custom autograd.Function to use it with forward mode AD."
     ]
    }
   ],
   "source": [
    "# Goal: gradient of loss w.r.t. rating_BD for each B separately\n",
    "grad_loss_TD = torch.autograd.functional.jacobian(loss, (rating_BD), vectorize=True,  strategy='forward-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 15:20:17.475711: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-07-05 15:20:17.475745: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: p1cmp078.pax.tufts.edu\n",
      "2024-07-05 15:20:17.475754: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: p1cmp078.pax.tufts.edu\n",
      "2024-07-05 15:20:17.475804: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.129.3\n",
      "2024-07-05 15:20:17.475830: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.129.3\n",
      "2024-07-05 15:20:17.475835: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.129.3\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 15:19:46.649358: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-05 15:19:46.706224: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-05 15:19:46.706251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-05 15:19:46.707330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-05 15:19:46.714907: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 15:19:56.184029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape (2,) must have rank at least 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m      4\u001b[0m   y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m [\u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m4.\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1273\u001b[0m, in \u001b[0;36mGradientTape.batch_jacobian\u001b[0;34m(self, target, source, unconnected_gradients, parallel_iterations, experimental_use_pfor)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m   dim \u001b[38;5;241m=\u001b[39m target_shape\u001b[38;5;241m.\u001b[39mdims[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_rank_at_least\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m         source\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mwith_rank_at_least(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m         dim\u001b[38;5;241m.\u001b[39mis_compatible_with(source\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m   1276\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1277\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed first dimension of target shape (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1278\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource shape (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) to match.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (target\u001b[38;5;241m.\u001b[39mshape, source\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_shape\u001b[38;5;241m.\u001b[39mis_fully_defined():\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:1144\u001b[0m, in \u001b[0;36mTensorShape.with_rank_at_least\u001b[0;34m(self, rank)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a shape based on `self` with at least the given rank.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m    `rank`.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m<\u001b[39m rank:\n\u001b[0;32m-> 1144\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have rank at least \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, rank))\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape (2,) must have rank at least 2"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2.,3.])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.batch_jacobian(y, x,).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
