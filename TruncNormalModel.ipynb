{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, Poisson, MixtureSameFamily\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Cd to code\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/cluster/home/kheuto01/code/prob_diff_topk')\n",
    "sys.path.append('/cluster/home/kheuto01/code/prob_diff_topk')\n",
    "\n",
    "from datasets import example_datasets, to_numpy\n",
    "from torch_perturb.torch_pert_topk import PerturbedTopK\n",
    "from torch_models import MixtureOfPoissonsModel, torch_bpr_uncurried, deterministic_bpr\n",
    "from torch_distributions import TruncatedNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=360\n",
    "# tracts/distributions\n",
    "S=12\n",
    "# history/features\n",
    "H = 3\n",
    "# total timepoints\n",
    "T= 500\n",
    "num_components=4\n",
    "K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = example_datasets(H, T, seed=seed)\n",
    "train_X_THS, train_y_TS = to_numpy(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take inverse softplus\n",
    "ideal_means = torch.tensor([0+1e-8, 7, 10, 100])\n",
    "ideal_softinv_means = ideal_means + torch.log(-torch.expm1(-ideal_means))\n",
    "ideal_scales = torch.tensor([0.2, 0.2, 0.2, 0.2])\n",
    "ideal_softinv_scales = ideal_scales + torch.log(-torch.expm1(-ideal_scales)) \n",
    "ideal_mix_weights = torch.log(1e-13 + torch.tensor(\n",
    "                                [[0,1,0,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0,1,0,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.3,0,0.7,0],\n",
    "                                 [0.9,0,0,0.1],\n",
    "                                 [0.9,0,0,0.1],\n",
    "                                 [0.9,0,0,0.1],\n",
    "                                 [0.9,0,0,0.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixtureOfTruncNormModel()\n",
    "step_size = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=step_size)\n",
    "\n",
    "model.update_params(torch.cat([ideal_softinv_means, ideal_softinv_scales, ideal_mix_weights.view(-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_score_func =  200\n",
    "M_action = 200\n",
    "train_T = train_y_TS.shape[0]\n",
    "perturbed_top_K_func = PerturbedTopK(k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_model = model()\n",
    "sample = mix_model.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.1, 7.22, 7.17, 6.96, 9.98, 0.1, 9.45, 10.12, 0.04, 0.04, 100.17, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# print sample as floats rounded to 2 decimal places\n",
    "print([samp.round(2) for samp in sample.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3683, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mix_model.log_prob(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2408.5791, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mix_model.log_prob(torch.tensor(train_y_TS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "bprs = []\n",
    "nlls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-18.4207,   6.9991,  10.0000, 100.0000,  -1.5078,  -1.5078,  -1.5078,\n",
       "         -1.5078, -29.9336,   0.0000, -29.9336, -29.9336, -29.9336,   0.0000,\n",
       "        -29.9336, -29.9336, -29.9336,   0.0000, -29.9336, -29.9336, -29.9336,\n",
       "          0.0000, -29.9336, -29.9336,  -1.2040, -29.9336,  -0.3567, -29.9336,\n",
       "         -1.2040, -29.9336,  -0.3567, -29.9336,  -1.2040, -29.9336,  -0.3567,\n",
       "        -29.9336,  -1.2040, -29.9336,  -0.3567, -29.9336,  -0.1054, -29.9336,\n",
       "        -29.9336,  -2.3026,  -0.1054, -29.9336, -29.9336,  -2.3026,  -0.1054,\n",
       "        -29.9336, -29.9336,  -2.3026,  -0.1054, -29.9336, -29.9336,  -2.3026],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params_to_single_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg bpr: -0.5302702188491821\n",
      "nll: -2408.5791015625\n",
      "Loss: -2673.714111328125\n",
      "Neg bpr: -0.5286932587623596\n",
      "nll: -2322.26123046875\n",
      "Loss: -2586.60791015625\n",
      "Neg bpr: -0.5222402215003967\n",
      "nll: -2304.0625\n",
      "Loss: -2565.1826171875\n",
      "Neg bpr: -0.5265845656394958\n",
      "nll: -2260.807861328125\n",
      "Loss: -2524.10009765625\n",
      "Neg bpr: -0.5275028347969055\n",
      "nll: -2238.63720703125\n",
      "Loss: -2502.388671875\n",
      "Neg bpr: -0.5284132361412048\n",
      "nll: -2202.7490234375\n",
      "Loss: -2466.95556640625\n",
      "Neg bpr: -0.5229679346084595\n",
      "nll: -2192.816650390625\n",
      "Loss: -2454.300537109375\n",
      "Neg bpr: -0.5284321308135986\n",
      "nll: -2195.840087890625\n",
      "Loss: -2460.05615234375\n",
      "Neg bpr: -0.5273725390434265\n",
      "nll: -2148.55419921875\n",
      "Loss: -2412.240478515625\n",
      "Neg bpr: -0.5310458540916443\n",
      "nll: -2115.156494140625\n",
      "Loss: -2380.679443359375\n",
      "Neg bpr: -0.5344506502151489\n",
      "nll: -2143.5654296875\n",
      "Loss: -2410.790771484375\n",
      "Neg bpr: -0.5283373594284058\n",
      "nll: -2153.397216796875\n",
      "Loss: -2417.56591796875\n",
      "Neg bpr: -0.5284692645072937\n",
      "nll: -2194.61767578125\n",
      "Loss: -2458.852294921875\n",
      "Neg bpr: -0.5296937823295593\n",
      "nll: -2255.803466796875\n",
      "Loss: -2520.650390625\n",
      "Neg bpr: -0.529887855052948\n",
      "nll: -2342.90087890625\n",
      "Loss: -2607.8447265625\n",
      "Neg bpr: -0.5277930498123169\n",
      "nll: -2408.099609375\n",
      "Loss: -2671.99609375\n",
      "Neg bpr: -0.5302034020423889\n",
      "nll: -2455.29736328125\n",
      "Loss: -2720.399169921875\n",
      "Neg bpr: -0.528718113899231\n",
      "nll: -2483.371337890625\n",
      "Loss: -2747.73046875\n",
      "Neg bpr: -0.5201334953308105\n",
      "nll: -2525.596923828125\n",
      "Loss: -2785.66357421875\n",
      "Neg bpr: -0.5237563252449036\n",
      "nll: -2516.2685546875\n",
      "Loss: -2778.146728515625\n",
      "Neg bpr: -0.5207377076148987\n",
      "nll: -2479.802734375\n",
      "Loss: -2740.171630859375\n",
      "Neg bpr: -0.5189472436904907\n",
      "nll: -2419.636962890625\n",
      "Loss: -2679.110595703125\n",
      "Neg bpr: -0.5185962319374084\n",
      "nll: -2381.07177734375\n",
      "Loss: -2640.369873046875\n",
      "Neg bpr: -0.5116984844207764\n",
      "nll: -2345.471435546875\n",
      "Loss: -2601.32080078125\n",
      "Neg bpr: -0.5175447463989258\n",
      "nll: -2305.68359375\n",
      "Loss: -2564.4560546875\n",
      "Neg bpr: -0.5115858316421509\n",
      "nll: -2271.599365234375\n",
      "Loss: -2527.392333984375\n",
      "Neg bpr: -0.511690616607666\n",
      "nll: -2228.1806640625\n",
      "Loss: -2484.02587890625\n",
      "Neg bpr: -0.5178512930870056\n",
      "nll: -2170.743408203125\n",
      "Loss: -2429.6689453125\n",
      "Neg bpr: -0.520149827003479\n",
      "nll: -2110.88427734375\n",
      "Loss: -2370.959228515625\n",
      "Neg bpr: -0.5127045512199402\n",
      "nll: -2052.197265625\n",
      "Loss: -2308.549560546875\n",
      "Neg bpr: -0.5187575817108154\n",
      "nll: -1991.441650390625\n",
      "Loss: -2250.8203125\n",
      "Neg bpr: -0.5148267149925232\n",
      "nll: -1926.03515625\n",
      "Loss: -2183.448486328125\n",
      "Neg bpr: -0.5171974897384644\n",
      "nll: -1870.978759765625\n",
      "Loss: -2129.57763671875\n",
      "Neg bpr: -0.5137465000152588\n",
      "nll: -1840.407470703125\n",
      "Loss: -2097.28076171875\n",
      "Neg bpr: -0.5185577869415283\n",
      "nll: -1806.79736328125\n",
      "Loss: -2066.076171875\n",
      "Neg bpr: -0.5170272588729858\n",
      "nll: -1786.10546875\n",
      "Loss: -2044.619140625\n",
      "Neg bpr: -0.5136502385139465\n",
      "nll: -1767.5865478515625\n",
      "Loss: -2024.41162109375\n",
      "Neg bpr: -0.5165811777114868\n",
      "nll: -1751.35009765625\n",
      "Loss: -2009.640625\n",
      "Neg bpr: -0.5121572017669678\n",
      "nll: -1717.412109375\n",
      "Loss: -1973.49072265625\n",
      "Neg bpr: -0.5098668932914734\n",
      "nll: -1691.5220947265625\n",
      "Loss: -1946.45556640625\n",
      "Neg bpr: -0.5119603276252747\n",
      "nll: -1671.2503662109375\n",
      "Loss: -1927.23046875\n",
      "Neg bpr: -0.5125595927238464\n",
      "nll: -1667.557373046875\n",
      "Loss: -1923.837158203125\n",
      "Neg bpr: -0.5095205903053284\n",
      "nll: -1683.86376953125\n",
      "Loss: -1938.6240234375\n",
      "Neg bpr: -0.5132497549057007\n",
      "nll: -1677.40380859375\n",
      "Loss: -1934.0286865234375\n",
      "Neg bpr: -0.5144469141960144\n",
      "nll: -1677.705078125\n",
      "Loss: -1934.928466796875\n",
      "Neg bpr: -0.5171222686767578\n",
      "nll: -1684.244384765625\n",
      "Loss: -1942.8055419921875\n",
      "Neg bpr: -0.5176454782485962\n",
      "nll: -1690.99658203125\n",
      "Loss: -1949.8193359375\n",
      "Neg bpr: -0.517909049987793\n",
      "nll: -1694.3548583984375\n",
      "Loss: -1953.309326171875\n",
      "Neg bpr: -0.5212342739105225\n",
      "nll: -1696.02490234375\n",
      "Loss: -1956.64208984375\n",
      "Neg bpr: -0.5227965116500854\n",
      "nll: -1715.6905517578125\n",
      "Loss: -1977.0888671875\n",
      "Neg bpr: -0.5231888890266418\n",
      "nll: -1741.3955078125\n",
      "Loss: -2002.989990234375\n",
      "Neg bpr: -0.5218344330787659\n",
      "nll: -1772.076904296875\n",
      "Loss: -2032.994140625\n",
      "Neg bpr: -0.5254641175270081\n",
      "nll: -1786.6351318359375\n",
      "Loss: -2049.3671875\n",
      "Neg bpr: -0.5229476094245911\n",
      "nll: -1797.841552734375\n",
      "Loss: -2059.3154296875\n",
      "Neg bpr: -0.5190916657447815\n",
      "nll: -1796.0438232421875\n",
      "Loss: -2055.589599609375\n",
      "Neg bpr: -0.5212498307228088\n",
      "nll: -1797.9471435546875\n",
      "Loss: -2058.572021484375\n",
      "Neg bpr: -0.5229141116142273\n",
      "nll: -1806.5555419921875\n",
      "Loss: -2068.0126953125\n",
      "Neg bpr: -0.5253879427909851\n",
      "nll: -1823.5474853515625\n",
      "Loss: -2086.241455078125\n",
      "Neg bpr: -0.5219170451164246\n",
      "nll: -1811.1126708984375\n",
      "Loss: -2072.0712890625\n",
      "Neg bpr: -0.5163354277610779\n",
      "nll: -1785.42724609375\n",
      "Loss: -2043.594970703125\n",
      "Neg bpr: -0.5161866545677185\n",
      "nll: -1773.9595947265625\n",
      "Loss: -2032.052978515625\n",
      "Neg bpr: -0.5144082903862\n",
      "nll: -1737.0804443359375\n",
      "Loss: -1994.2845458984375\n",
      "Neg bpr: -0.513745903968811\n",
      "nll: -1701.759765625\n",
      "Loss: -1958.6326904296875\n",
      "Neg bpr: -0.5155357718467712\n",
      "nll: -1650.8839111328125\n",
      "Loss: -1908.65185546875\n",
      "Neg bpr: -0.508880078792572\n",
      "nll: -1593.970703125\n",
      "Loss: -1848.4107666015625\n",
      "Neg bpr: -0.509856641292572\n",
      "nll: -1504.3828125\n",
      "Loss: -1759.3111572265625\n",
      "Neg bpr: -0.5065751075744629\n",
      "nll: -1432.62255859375\n",
      "Loss: -1685.91015625\n",
      "Neg bpr: -0.5011164546012878\n",
      "nll: -1309.9981689453125\n",
      "Loss: -1560.556396484375\n",
      "Neg bpr: -0.5057973265647888\n",
      "nll: -1207.5106201171875\n",
      "Loss: -1460.4093017578125\n",
      "Neg bpr: -0.5063557028770447\n",
      "nll: -1115.3046875\n",
      "Loss: -1368.4825439453125\n",
      "Neg bpr: -0.5118299126625061\n",
      "nll: -975.2637939453125\n",
      "Loss: -1231.1787109375\n",
      "Neg bpr: -0.5121815800666809\n",
      "nll: -803.761962890625\n",
      "Loss: -1059.852783203125\n",
      "Neg bpr: -0.5070170760154724\n",
      "nll: -613.301513671875\n",
      "Loss: -866.81005859375\n",
      "Neg bpr: -0.5075768828392029\n",
      "nll: -496.19775390625\n",
      "Loss: -749.9862060546875\n",
      "Neg bpr: -0.5077910423278809\n",
      "nll: -357.786376953125\n",
      "Loss: -611.681884765625\n",
      "Neg bpr: -0.5047196745872498\n",
      "nll: -162.98788452148438\n",
      "Loss: -415.34771728515625\n",
      "Neg bpr: -0.49732106924057007\n",
      "nll: -67.69338989257812\n",
      "Loss: -316.35394287109375\n",
      "Neg bpr: -0.5034728050231934\n",
      "nll: 24.058815002441406\n",
      "Loss: -227.67758178710938\n",
      "Neg bpr: -0.4972952902317047\n",
      "nll: 128.12112426757812\n",
      "Loss: -120.52651977539062\n",
      "Neg bpr: -0.4988430142402649\n",
      "nll: 54.871334075927734\n",
      "Loss: -194.5501708984375\n",
      "Neg bpr: -0.49247634410858154\n",
      "nll: -142.44625854492188\n",
      "Loss: -388.6844482421875\n",
      "Neg bpr: -0.49399128556251526\n",
      "nll: -331.20391845703125\n",
      "Loss: -578.1995849609375\n",
      "Neg bpr: -0.494416743516922\n",
      "nll: -427.6280517578125\n",
      "Loss: -674.83642578125\n",
      "Neg bpr: -0.4815978407859802\n",
      "nll: -543.1900634765625\n",
      "Loss: -783.989013671875\n",
      "Neg bpr: -0.4856324791908264\n",
      "nll: -608.264892578125\n",
      "Loss: -851.0811157226562\n",
      "Neg bpr: -0.48875677585601807\n",
      "nll: -662.4341430664062\n",
      "Loss: -906.8125\n",
      "Neg bpr: -0.486152708530426\n",
      "nll: -695.3154296875\n",
      "Loss: -938.3917846679688\n",
      "Neg bpr: -0.48416346311569214\n",
      "nll: -764.1591796875\n",
      "Loss: -1006.2409057617188\n",
      "Neg bpr: -0.4824041724205017\n",
      "nll: -823.2855834960938\n",
      "Loss: -1064.4876708984375\n",
      "Neg bpr: -0.4746498167514801\n",
      "nll: -854.3142700195312\n",
      "Loss: -1091.63916015625\n",
      "Neg bpr: -0.4751339256763458\n",
      "nll: -853.455078125\n",
      "Loss: -1091.02197265625\n",
      "Neg bpr: -0.4717392921447754\n",
      "nll: -742.6346435546875\n",
      "Loss: -978.5042724609375\n",
      "Neg bpr: -0.47175872325897217\n",
      "nll: -562.6609497070312\n",
      "Loss: -798.540283203125\n",
      "Neg bpr: -0.467342734336853\n",
      "nll: -285.99224853515625\n",
      "Loss: -519.6636352539062\n",
      "Neg bpr: -0.4688279628753662\n",
      "nll: 45.2879524230957\n",
      "Loss: -189.12603759765625\n",
      "Neg bpr: -0.47758808732032776\n",
      "nll: 409.24444580078125\n",
      "Loss: 170.45040893554688\n",
      "Neg bpr: -0.47056257724761963\n",
      "nll: 1020.4139404296875\n",
      "Loss: 785.1326293945312\n",
      "Neg bpr: -0.4770967662334442\n",
      "nll: 1792.678466796875\n",
      "Loss: 1554.130126953125\n",
      "Neg bpr: -0.4743218421936035\n",
      "nll: 2550.2529296875\n",
      "Loss: 2313.092041015625\n",
      "Neg bpr: -0.4752185642719269\n",
      "nll: 3337.71875\n",
      "Loss: 3100.109375\n",
      "Neg bpr: -0.4801669418811798\n",
      "nll: 4005.476318359375\n",
      "Loss: 3765.392822265625\n",
      "Neg bpr: -0.48059967160224915\n",
      "nll: 4872.2216796875\n",
      "Loss: 4631.921875\n",
      "Neg bpr: -0.48465844988822937\n",
      "nll: 5656.20458984375\n",
      "Loss: 5413.87548828125\n",
      "Neg bpr: -0.48341435194015503\n",
      "nll: 6247.0048828125\n",
      "Loss: 6005.2978515625\n",
      "Neg bpr: -0.48061156272888184\n",
      "nll: 7312.85546875\n",
      "Loss: 7072.5498046875\n",
      "Neg bpr: -0.48086172342300415\n",
      "nll: 8314.5322265625\n",
      "Loss: 8074.1015625\n",
      "Neg bpr: -0.48125600814819336\n",
      "nll: 8994.0478515625\n",
      "Loss: 8753.419921875\n",
      "Neg bpr: -0.47613325715065\n",
      "nll: 9434.6103515625\n",
      "Loss: 9196.5439453125\n",
      "Neg bpr: -0.4786202907562256\n",
      "nll: 9633.765625\n",
      "Loss: 9394.455078125\n",
      "Neg bpr: -0.48199263215065\n",
      "nll: 10112.080078125\n",
      "Loss: 9871.083984375\n",
      "Neg bpr: -0.4757789075374603\n",
      "nll: 10662.2744140625\n",
      "Loss: 10424.384765625\n",
      "Neg bpr: -0.4793740212917328\n",
      "nll: 11406.5869140625\n",
      "Loss: 11166.900390625\n",
      "Neg bpr: -0.4789424538612366\n",
      "nll: 11930.4912109375\n",
      "Loss: 11691.01953125\n",
      "Neg bpr: -0.48182275891304016\n",
      "nll: 12016.015625\n",
      "Loss: 11775.1044921875\n",
      "Neg bpr: -0.48096978664398193\n",
      "nll: 12168.9609375\n",
      "Loss: 11928.4755859375\n",
      "Neg bpr: -0.480654239654541\n",
      "nll: 11882.12109375\n",
      "Loss: 11641.7939453125\n",
      "Neg bpr: -0.48086726665496826\n",
      "nll: 11909.439453125\n",
      "Loss: 11669.005859375\n",
      "Neg bpr: -0.48019835352897644\n",
      "nll: 12222.9609375\n",
      "Loss: 11982.861328125\n",
      "Neg bpr: -0.47961366176605225\n",
      "nll: 12512.6953125\n",
      "Loss: 12272.888671875\n",
      "Neg bpr: -0.48086318373680115\n",
      "nll: 12912.44921875\n",
      "Loss: 12672.017578125\n",
      "Neg bpr: -0.4814002513885498\n",
      "nll: 13447.9521484375\n",
      "Loss: 13207.251953125\n",
      "Neg bpr: -0.4816146790981293\n",
      "nll: 13950.8173828125\n",
      "Loss: 13710.009765625\n",
      "Neg bpr: -0.48423880338668823\n",
      "nll: 14480.5\n",
      "Loss: 14238.380859375\n",
      "Neg bpr: -0.483555406332016\n",
      "nll: 15126.228515625\n",
      "Loss: 14884.451171875\n",
      "Neg bpr: -0.4837748110294342\n",
      "nll: 15859.76953125\n",
      "Loss: 15617.8818359375\n",
      "Neg bpr: -0.48313167691230774\n",
      "nll: 15857.8701171875\n",
      "Loss: 15616.3046875\n",
      "Neg bpr: -0.48495611548423767\n",
      "nll: 15918.279296875\n",
      "Loss: 15675.80078125\n",
      "Neg bpr: -0.4823368787765503\n",
      "nll: 16089.8212890625\n",
      "Loss: 15848.6533203125\n",
      "Neg bpr: -0.4872349500656128\n",
      "nll: 16106.4326171875\n",
      "Loss: 15862.8154296875\n",
      "Neg bpr: -0.4888130724430084\n",
      "nll: 16440.626953125\n",
      "Loss: 16196.220703125\n",
      "Neg bpr: -0.4887596070766449\n",
      "nll: 16966.255859375\n",
      "Loss: 16721.876953125\n",
      "Neg bpr: -0.49041783809661865\n",
      "nll: 17661.888671875\n",
      "Loss: 17416.6796875\n",
      "Neg bpr: -0.4881778061389923\n",
      "nll: 18521.759765625\n",
      "Loss: 18277.669921875\n",
      "Neg bpr: -0.48965463042259216\n",
      "nll: 19107.10546875\n",
      "Loss: 18862.27734375\n",
      "Neg bpr: -0.4882350265979767\n",
      "nll: 19482.892578125\n",
      "Loss: 19238.775390625\n",
      "Neg bpr: -0.48875895142555237\n",
      "nll: 19678.603515625\n",
      "Loss: 19434.224609375\n",
      "Neg bpr: -0.49090489745140076\n",
      "nll: 20209.22265625\n",
      "Loss: 19963.76953125\n",
      "Neg bpr: -0.48675286769866943\n",
      "nll: 21444.05078125\n",
      "Loss: 21200.673828125\n",
      "Neg bpr: -0.4882339537143707\n",
      "nll: 22667.958984375\n",
      "Loss: 22423.841796875\n",
      "Neg bpr: -0.49017369747161865\n",
      "nll: 23390.890625\n",
      "Loss: 23145.8046875\n",
      "Neg bpr: -0.49085336923599243\n",
      "nll: 23879.46875\n",
      "Loss: 23634.04296875\n",
      "Neg bpr: -0.49055755138397217\n",
      "nll: 24426.50390625\n",
      "Loss: 24181.224609375\n",
      "Neg bpr: -0.4906337559223175\n",
      "nll: 24518.5625\n",
      "Loss: 24273.24609375\n",
      "Neg bpr: -0.4918311536312103\n",
      "nll: 24732.55078125\n",
      "Loss: 24486.634765625\n",
      "Neg bpr: -0.4925934374332428\n",
      "nll: 24689.861328125\n",
      "Loss: 24443.564453125\n",
      "Neg bpr: -0.4923836886882782\n",
      "nll: 24514.857421875\n",
      "Loss: 24268.666015625\n",
      "Neg bpr: -0.4956153929233551\n",
      "nll: 24188.0546875\n",
      "Loss: 23940.24609375\n",
      "Neg bpr: -0.49455392360687256\n",
      "nll: 23922.6953125\n",
      "Loss: 23675.41796875\n",
      "Neg bpr: -0.49214619398117065\n",
      "nll: 23467.69140625\n",
      "Loss: 23221.619140625\n",
      "Neg bpr: -0.4918980896472931\n",
      "nll: 23041.986328125\n",
      "Loss: 22796.037109375\n",
      "Neg bpr: -0.4912792444229126\n",
      "nll: 23291.66015625\n",
      "Loss: 23046.021484375\n",
      "Neg bpr: -0.4927993714809418\n",
      "nll: 23120.09375\n",
      "Loss: 22873.693359375\n",
      "Neg bpr: -0.4899348020553589\n",
      "nll: 23007.5234375\n",
      "Loss: 22762.556640625\n",
      "Neg bpr: -0.49610209465026855\n",
      "nll: 22835.353515625\n",
      "Loss: 22587.302734375\n",
      "Neg bpr: -0.4923225939273834\n",
      "nll: 22479.65625\n",
      "Loss: 22233.494140625\n",
      "Neg bpr: -0.49109578132629395\n",
      "nll: 21969.326171875\n",
      "Loss: 21723.77734375\n",
      "Neg bpr: -0.49756601452827454\n",
      "nll: 22089.640625\n",
      "Loss: 21840.857421875\n",
      "Neg bpr: -0.4971284568309784\n",
      "nll: 22031.494140625\n",
      "Loss: 21782.9296875\n",
      "Neg bpr: -0.49565672874450684\n",
      "nll: 21621.541015625\n",
      "Loss: 21373.712890625\n",
      "Neg bpr: -0.4944181740283966\n",
      "nll: 21127.560546875\n",
      "Loss: 20880.3515625\n",
      "Neg bpr: -0.496925413608551\n",
      "nll: 20267.46875\n",
      "Loss: 20019.005859375\n",
      "Neg bpr: -0.4981064796447754\n",
      "nll: 19664.958984375\n",
      "Loss: 19415.90625\n",
      "Neg bpr: -0.5000098943710327\n",
      "nll: 19773.943359375\n",
      "Loss: 19523.9375\n",
      "Neg bpr: -0.4982554614543915\n",
      "nll: 19686.294921875\n",
      "Loss: 19437.16796875\n",
      "Neg bpr: -0.49727553129196167\n",
      "nll: 19639.611328125\n",
      "Loss: 19390.97265625\n",
      "Neg bpr: -0.4994444251060486\n",
      "nll: 19245.15234375\n",
      "Loss: 18995.4296875\n",
      "Neg bpr: -0.4980240762233734\n",
      "nll: 18305.908203125\n",
      "Loss: 18056.896484375\n",
      "Neg bpr: -0.49990418553352356\n",
      "nll: 17061.353515625\n",
      "Loss: 16811.40234375\n",
      "Neg bpr: -0.49755197763442993\n",
      "nll: 15778.1728515625\n",
      "Loss: 15529.396484375\n",
      "Neg bpr: -0.5008559226989746\n",
      "nll: 14516.17578125\n",
      "Loss: 14265.748046875\n",
      "Neg bpr: -0.5036255121231079\n",
      "nll: 13195.291015625\n",
      "Loss: 12943.478515625\n",
      "Neg bpr: -0.5028235912322998\n",
      "nll: 12035.1953125\n",
      "Loss: 11783.783203125\n",
      "Neg bpr: -0.5070042610168457\n",
      "nll: 11155.443359375\n",
      "Loss: 10901.94140625\n",
      "Neg bpr: -0.5022973418235779\n",
      "nll: 10224.7880859375\n",
      "Loss: 9973.6396484375\n",
      "Neg bpr: -0.5064365863800049\n",
      "nll: 9289.953125\n",
      "Loss: 9036.734375\n",
      "Neg bpr: -0.5055379271507263\n",
      "nll: 8656.974609375\n",
      "Loss: 8404.2060546875\n",
      "Neg bpr: -0.5085565447807312\n",
      "nll: 8154.03076171875\n",
      "Loss: 7899.75244140625\n",
      "Neg bpr: -0.5151380300521851\n",
      "nll: 7912.421875\n",
      "Loss: 7654.85302734375\n",
      "Neg bpr: -0.5125709176063538\n",
      "nll: 7729.3486328125\n",
      "Loss: 7473.06298828125\n",
      "Neg bpr: -0.5214574337005615\n",
      "nll: 7342.5009765625\n",
      "Loss: 7081.7724609375\n",
      "Neg bpr: -0.5232769846916199\n",
      "nll: 7033.9892578125\n",
      "Loss: 6772.3505859375\n",
      "Neg bpr: -0.5229435563087463\n",
      "nll: 6735.87255859375\n",
      "Loss: 6474.40087890625\n",
      "Neg bpr: -0.5275694131851196\n",
      "nll: 6587.43798828125\n",
      "Loss: 6323.6533203125\n",
      "Neg bpr: -0.5290995240211487\n",
      "nll: 6759.1943359375\n",
      "Loss: 6494.64453125\n",
      "Neg bpr: -0.5328249931335449\n",
      "nll: 6874.982421875\n",
      "Loss: 6608.56982421875\n",
      "Neg bpr: -0.5379985570907593\n",
      "nll: 6809.28515625\n",
      "Loss: 6540.2861328125\n",
      "Neg bpr: -0.5390629172325134\n",
      "nll: 6742.16064453125\n",
      "Loss: 6472.62939453125\n",
      "Neg bpr: -0.5433574318885803\n",
      "nll: 6714.27978515625\n",
      "Loss: 6442.60107421875\n",
      "Neg bpr: -0.5434155464172363\n",
      "nll: 6770.978515625\n",
      "Loss: 6499.2705078125\n",
      "Neg bpr: -0.544890820980072\n",
      "nll: 6801.4736328125\n",
      "Loss: 6529.0283203125\n",
      "Neg bpr: -0.5455455183982849\n",
      "nll: 6990.41845703125\n",
      "Loss: 6717.6455078125\n",
      "Neg bpr: -0.5489164590835571\n",
      "nll: 7311.1171875\n",
      "Loss: 7036.6591796875\n",
      "Neg bpr: -0.5483684539794922\n",
      "nll: 7650.3662109375\n",
      "Loss: 7376.18212890625\n",
      "Neg bpr: -0.5493178367614746\n",
      "nll: 7882.95703125\n",
      "Loss: 7608.29833984375\n",
      "Neg bpr: -0.5512065887451172\n",
      "nll: 7899.20556640625\n",
      "Loss: 7623.60205078125\n",
      "Neg bpr: -0.5504186153411865\n",
      "nll: 8184.9912109375\n",
      "Loss: 7909.78173828125\n",
      "Neg bpr: -0.5501776337623596\n",
      "nll: 8425.55078125\n",
      "Loss: 8150.4619140625\n",
      "Neg bpr: -0.5503960847854614\n",
      "nll: 8764.4755859375\n",
      "Loss: 8489.27734375\n",
      "Neg bpr: -0.551267683506012\n",
      "nll: 9333.8876953125\n",
      "Loss: 9058.25390625\n",
      "Neg bpr: -0.5541400909423828\n",
      "nll: 9757.265625\n",
      "Loss: 9480.1953125\n",
      "Neg bpr: -0.5520027875900269\n",
      "nll: 10187.126953125\n",
      "Loss: 9911.1259765625\n",
      "Neg bpr: -0.5562631487846375\n",
      "nll: 10723.615234375\n",
      "Loss: 10445.4833984375\n",
      "Neg bpr: -0.5495402216911316\n",
      "nll: 11558.4560546875\n",
      "Loss: 11283.685546875\n",
      "Neg bpr: -0.5521543622016907\n",
      "nll: 12383.85546875\n",
      "Loss: 12107.7783203125\n",
      "Neg bpr: -0.5548497438430786\n",
      "nll: 13127.74609375\n",
      "Loss: 12850.3212890625\n",
      "Neg bpr: -0.5505727529525757\n",
      "nll: 13791.7548828125\n",
      "Loss: 13516.46875\n",
      "Neg bpr: -0.5555405616760254\n",
      "nll: 13979.2470703125\n",
      "Loss: 13701.4765625\n",
      "Neg bpr: -0.551693856716156\n",
      "nll: 14121.18359375\n",
      "Loss: 13845.3369140625\n",
      "Neg bpr: -0.5556843280792236\n",
      "nll: 14387.3525390625\n",
      "Loss: 14109.5107421875\n",
      "Neg bpr: -0.5556711554527283\n",
      "nll: 14960.525390625\n",
      "Loss: 14682.689453125\n",
      "Neg bpr: -0.5568196177482605\n",
      "nll: 15630.0693359375\n",
      "Loss: 15351.6591796875\n",
      "Neg bpr: -0.5545645356178284\n",
      "nll: 15906.6171875\n",
      "Loss: 15629.3349609375\n",
      "Neg bpr: -0.5544954538345337\n",
      "nll: 16359.5146484375\n",
      "Loss: 16082.2666015625\n",
      "Neg bpr: -0.5556110739707947\n",
      "nll: 16759.01171875\n",
      "Loss: 16481.20703125\n",
      "Neg bpr: -0.5542465448379517\n",
      "nll: 17119.87890625\n",
      "Loss: 16842.755859375\n",
      "Neg bpr: -0.5577196478843689\n",
      "nll: 17530.75390625\n",
      "Loss: 17251.89453125\n",
      "Neg bpr: -0.5563297271728516\n",
      "nll: 17644.939453125\n",
      "Loss: 17366.775390625\n",
      "Neg bpr: -0.5553696751594543\n",
      "nll: 17676.107421875\n",
      "Loss: 17398.421875\n",
      "Neg bpr: -0.5574228167533875\n",
      "nll: 17580.0625\n",
      "Loss: 17301.3515625\n",
      "Neg bpr: -0.557042121887207\n",
      "nll: 17173.58203125\n",
      "Loss: 16895.060546875\n",
      "Neg bpr: -0.5562544465065002\n",
      "nll: 17333.3125\n",
      "Loss: 17055.185546875\n",
      "Neg bpr: -0.5550661683082581\n",
      "nll: 17627.732421875\n",
      "Loss: 17350.19921875\n",
      "Neg bpr: -0.5557811260223389\n",
      "nll: 18104.869140625\n",
      "Loss: 17826.978515625\n",
      "Neg bpr: -0.55464768409729\n",
      "nll: 18477.6796875\n",
      "Loss: 18200.35546875\n",
      "Neg bpr: -0.5561670660972595\n",
      "nll: 18836.19921875\n",
      "Loss: 18558.115234375\n",
      "Neg bpr: -0.5544316172599792\n",
      "nll: 18955.583984375\n",
      "Loss: 18678.3671875\n",
      "Neg bpr: -0.5593442916870117\n",
      "nll: 19049.9609375\n",
      "Loss: 18770.2890625\n",
      "Neg bpr: -0.5551048517227173\n",
      "nll: 19117.166015625\n",
      "Loss: 18839.61328125\n",
      "Neg bpr: -0.5546681880950928\n",
      "nll: 19232.064453125\n",
      "Loss: 18954.73046875\n",
      "Neg bpr: -0.5550143122673035\n",
      "nll: 19886.11328125\n",
      "Loss: 19608.60546875\n",
      "Neg bpr: -0.5552793145179749\n",
      "nll: 20449.0234375\n",
      "Loss: 20171.3828125\n",
      "Neg bpr: -0.5555166006088257\n",
      "nll: 20820.431640625\n",
      "Loss: 20542.673828125\n",
      "Neg bpr: -0.5542808771133423\n",
      "nll: 21536.9296875\n",
      "Loss: 21259.7890625\n",
      "Neg bpr: -0.5555962920188904\n",
      "nll: 22306.94921875\n",
      "Loss: 22029.150390625\n",
      "Neg bpr: -0.554977536201477\n",
      "nll: 22966.134765625\n",
      "Loss: 22688.646484375\n",
      "Neg bpr: -0.5517613887786865\n",
      "nll: 23915.83203125\n",
      "Loss: 23639.951171875\n",
      "Neg bpr: -0.5519010424613953\n",
      "nll: 24945.23046875\n",
      "Loss: 24669.279296875\n",
      "Neg bpr: -0.5476738214492798\n",
      "nll: 26192.943359375\n",
      "Loss: 25919.10546875\n",
      "Neg bpr: -0.5501948595046997\n",
      "nll: 27653.041015625\n",
      "Loss: 27377.943359375\n",
      "Neg bpr: -0.5505539178848267\n",
      "nll: 28716.2421875\n",
      "Loss: 28440.96484375\n",
      "Neg bpr: -0.5503902435302734\n",
      "nll: 30541.240234375\n",
      "Loss: 30266.044921875\n",
      "Neg bpr: -0.546511173248291\n",
      "nll: 32071.625\n",
      "Loss: 31798.369140625\n",
      "Neg bpr: -0.5465849041938782\n",
      "nll: 34048.328125\n",
      "Loss: 33775.03515625\n",
      "Neg bpr: -0.5489622950553894\n",
      "nll: 35537.78125\n",
      "Loss: 35263.30078125\n",
      "Neg bpr: -0.5507911443710327\n",
      "nll: 36803.59375\n",
      "Loss: 36528.19921875\n",
      "Neg bpr: -0.5506842732429504\n",
      "nll: 37647.01953125\n",
      "Loss: 37371.67578125\n",
      "Neg bpr: -0.5541355609893799\n",
      "nll: 38685.4375\n",
      "Loss: 38408.37109375\n",
      "Neg bpr: -0.5526540875434875\n",
      "nll: 39867.9609375\n",
      "Loss: 39591.6328125\n",
      "Neg bpr: -0.5522210001945496\n",
      "nll: 41030.7265625\n",
      "Loss: 40754.6171875\n",
      "Neg bpr: -0.555208683013916\n",
      "nll: 42406.390625\n",
      "Loss: 42128.78515625\n",
      "Neg bpr: -0.5557520985603333\n",
      "nll: 43719.53125\n",
      "Loss: 43441.65625\n",
      "Neg bpr: -0.5535414218902588\n",
      "nll: 44950.99609375\n",
      "Loss: 44674.2265625\n",
      "Neg bpr: -0.5568194389343262\n",
      "nll: 46444.60546875\n",
      "Loss: 46166.1953125\n",
      "Neg bpr: -0.5572015643119812\n",
      "nll: 49069.8359375\n",
      "Loss: 48791.234375\n",
      "Neg bpr: -0.5558058619499207\n",
      "nll: 51208.4296875\n",
      "Loss: 50930.52734375\n",
      "Neg bpr: -0.5551570057868958\n",
      "nll: 52144.32421875\n",
      "Loss: 51866.74609375\n",
      "Neg bpr: -0.5571836829185486\n",
      "nll: 53602.0625\n",
      "Loss: 53323.46875\n",
      "Neg bpr: -0.5567157864570618\n",
      "nll: 54514.234375\n",
      "Loss: 54235.875\n",
      "Neg bpr: -0.5549928545951843\n",
      "nll: 55373.87890625\n",
      "Loss: 55096.3828125\n",
      "Neg bpr: -0.5590586066246033\n",
      "nll: 55589.7578125\n",
      "Loss: 55310.2265625\n",
      "Neg bpr: -0.5589662790298462\n",
      "nll: 55954.94140625\n",
      "Loss: 55675.45703125\n",
      "Neg bpr: -0.5585970282554626\n",
      "nll: 56218.515625\n",
      "Loss: 55939.21875\n",
      "Neg bpr: -0.5556084513664246\n",
      "nll: 56344.96875\n",
      "Loss: 56067.1640625\n",
      "Neg bpr: -0.555734395980835\n",
      "nll: 56871.6328125\n",
      "Loss: 56593.765625\n",
      "Neg bpr: -0.5565474629402161\n",
      "nll: 57641.8359375\n",
      "Loss: 57363.5625\n",
      "Neg bpr: -0.5576184391975403\n",
      "nll: 58077.0625\n",
      "Loss: 57798.25390625\n",
      "Neg bpr: -0.5565014481544495\n",
      "nll: 58782.22265625\n",
      "Loss: 58503.97265625\n",
      "Neg bpr: -0.5603191256523132\n",
      "nll: 59062.92578125\n",
      "Loss: 58782.765625\n",
      "Neg bpr: -0.5588852763175964\n",
      "nll: 59726.9140625\n",
      "Loss: 59447.47265625\n",
      "Neg bpr: -0.5601793527603149\n",
      "nll: 60625.12109375\n",
      "Loss: 60345.03125\n",
      "Neg bpr: -0.5578732490539551\n",
      "nll: 60914.34765625\n",
      "Loss: 60635.41015625\n",
      "Neg bpr: -0.5586333274841309\n",
      "nll: 61270.25\n",
      "Loss: 60990.93359375\n",
      "Neg bpr: -0.5591950416564941\n",
      "nll: 61608.50390625\n",
      "Loss: 61328.90625\n",
      "Neg bpr: -0.5571266412734985\n",
      "nll: 61824.1328125\n",
      "Loss: 61545.5703125\n",
      "Neg bpr: -0.5587843060493469\n",
      "nll: 61850.54296875\n",
      "Loss: 61571.15234375\n",
      "Neg bpr: -0.5573215484619141\n",
      "nll: 62017.5078125\n",
      "Loss: 61738.84765625\n",
      "Neg bpr: -0.5580441951751709\n",
      "nll: 61777.87109375\n",
      "Loss: 61498.84765625\n",
      "Neg bpr: -0.5574745535850525\n",
      "nll: 61277.5390625\n",
      "Loss: 60998.80078125\n",
      "Neg bpr: -0.5574405193328857\n",
      "nll: 60413.54296875\n",
      "Loss: 60134.82421875\n",
      "Neg bpr: -0.5543738603591919\n",
      "nll: 59686.4609375\n",
      "Loss: 59409.2734375\n",
      "Neg bpr: -0.5520767569541931\n",
      "nll: 59500.953125\n",
      "Loss: 59224.9140625\n",
      "Neg bpr: -0.5483695864677429\n",
      "nll: 59117.9765625\n",
      "Loss: 58843.79296875\n",
      "Neg bpr: -0.546345591545105\n",
      "nll: 58633.19140625\n",
      "Loss: 58360.01953125\n",
      "Neg bpr: -0.5410990715026855\n",
      "nll: 58233.01953125\n",
      "Loss: 57962.46875\n",
      "Neg bpr: -0.535751461982727\n",
      "nll: 57296.765625\n",
      "Loss: 57028.890625\n",
      "Neg bpr: -0.530012845993042\n",
      "nll: 56236.62890625\n",
      "Loss: 55971.62109375\n",
      "Neg bpr: -0.5270564556121826\n",
      "nll: 55162.05859375\n",
      "Loss: 54898.53125\n",
      "Neg bpr: -0.5281052589416504\n",
      "nll: 54240.26953125\n",
      "Loss: 53976.21875\n",
      "Neg bpr: -0.525812566280365\n",
      "nll: 53759.6484375\n",
      "Loss: 53496.7421875\n",
      "Neg bpr: -0.5202093124389648\n",
      "nll: 53716.75\n",
      "Loss: 53456.64453125\n",
      "Neg bpr: -0.5182154178619385\n",
      "nll: 53535.8984375\n",
      "Loss: 53276.7890625\n",
      "Neg bpr: -0.5114006400108337\n",
      "nll: 53119.76171875\n",
      "Loss: 52864.0625\n",
      "Neg bpr: -0.5114755034446716\n",
      "nll: 52871.765625\n",
      "Loss: 52616.02734375\n",
      "Neg bpr: -0.4985622763633728\n",
      "nll: 53100.875\n",
      "Loss: 52851.59375\n",
      "Neg bpr: -0.49265873432159424\n",
      "nll: 53191.92578125\n",
      "Loss: 52945.59765625\n",
      "Neg bpr: -0.4878683388233185\n",
      "nll: 53444.2734375\n",
      "Loss: 53200.33984375\n",
      "Neg bpr: -0.4880429208278656\n",
      "nll: 53989.7890625\n",
      "Loss: 53745.76953125\n",
      "Neg bpr: -0.48663172125816345\n",
      "nll: 53960.15625\n",
      "Loss: 53716.83984375\n",
      "Neg bpr: -0.4848443567752838\n",
      "nll: 54150.26171875\n",
      "Loss: 53907.83984375\n",
      "Neg bpr: -0.48401060700416565\n",
      "nll: 54629.8125\n",
      "Loss: 54387.80859375\n",
      "Neg bpr: -0.4841735363006592\n",
      "nll: 55204.33984375\n",
      "Loss: 54962.25390625\n",
      "Neg bpr: -0.4823639392852783\n",
      "nll: 55605.61328125\n",
      "Loss: 55364.4296875\n",
      "Neg bpr: -0.48040053248405457\n",
      "nll: 55774.0625\n",
      "Loss: 55533.86328125\n",
      "Neg bpr: -0.4801063537597656\n",
      "nll: 55694.921875\n",
      "Loss: 55454.8671875\n",
      "Neg bpr: -0.4783778488636017\n",
      "nll: 55456.36328125\n",
      "Loss: 55217.17578125\n",
      "Neg bpr: -0.4774986207485199\n",
      "nll: 55750.55078125\n",
      "Loss: 55511.80078125\n",
      "Neg bpr: -0.4767325222492218\n",
      "nll: 56367.2421875\n",
      "Loss: 56128.875\n",
      "Neg bpr: -0.47537150979042053\n",
      "nll: 57009.94921875\n",
      "Loss: 56772.26171875\n",
      "Neg bpr: -0.474187970161438\n",
      "nll: 57863.01953125\n",
      "Loss: 57625.92578125\n",
      "Neg bpr: -0.47280487418174744\n",
      "nll: 58851.84375\n",
      "Loss: 58615.44140625\n",
      "Neg bpr: -0.47109559178352356\n",
      "nll: 60174.0625\n",
      "Loss: 59938.515625\n",
      "Neg bpr: -0.47159332036972046\n",
      "nll: 61293.625\n",
      "Loss: 61057.828125\n",
      "Neg bpr: -0.46889054775238037\n",
      "nll: 62135.609375\n",
      "Loss: 61901.1640625\n",
      "Neg bpr: -0.4687628149986267\n",
      "nll: 63003.703125\n",
      "Loss: 62769.3203125\n",
      "Neg bpr: -0.4682970643043518\n",
      "nll: 63857.91015625\n",
      "Loss: 63623.76171875\n",
      "Neg bpr: -0.4686633348464966\n",
      "nll: 64721.62109375\n",
      "Loss: 64487.2890625\n",
      "Neg bpr: -0.46624159812927246\n",
      "nll: 65123.1171875\n",
      "Loss: 64889.99609375\n",
      "Neg bpr: -0.46849167346954346\n",
      "nll: 65801.1953125\n",
      "Loss: 65566.953125\n",
      "Neg bpr: -0.46562865376472473\n",
      "nll: 66631.4375\n",
      "Loss: 66398.625\n",
      "Neg bpr: -0.46821969747543335\n",
      "nll: 67433.375\n",
      "Loss: 67199.265625\n",
      "Neg bpr: -0.4666661024093628\n",
      "nll: 67968.015625\n",
      "Loss: 67734.6796875\n",
      "Neg bpr: -0.4673331081867218\n",
      "nll: 67529.5546875\n",
      "Loss: 67295.890625\n",
      "Neg bpr: -0.46790772676467896\n",
      "nll: 67299.703125\n",
      "Loss: 67065.75\n",
      "Neg bpr: -0.4675480127334595\n",
      "nll: 66962.5390625\n",
      "Loss: 66728.765625\n",
      "Neg bpr: -0.469759464263916\n",
      "nll: 66726.2421875\n",
      "Loss: 66491.359375\n",
      "Neg bpr: -0.4690844714641571\n",
      "nll: 66725.8359375\n",
      "Loss: 66491.296875\n",
      "Neg bpr: -0.47084787487983704\n",
      "nll: 66586.4140625\n",
      "Loss: 66350.9921875\n",
      "Neg bpr: -0.4690268039703369\n",
      "nll: 66719.3984375\n",
      "Loss: 66484.8828125\n",
      "Neg bpr: -0.469221293926239\n",
      "nll: 67526.8515625\n",
      "Loss: 67292.2421875\n",
      "Neg bpr: -0.46992361545562744\n",
      "nll: 68389.734375\n",
      "Loss: 68154.7734375\n",
      "Neg bpr: -0.46880391240119934\n",
      "nll: 68799.96875\n",
      "Loss: 68565.5703125\n",
      "Neg bpr: -0.47041055560112\n",
      "nll: 68950.96875\n",
      "Loss: 68715.765625\n",
      "Neg bpr: -0.47097036242485046\n",
      "nll: 69337.3671875\n",
      "Loss: 69101.8828125\n",
      "Neg bpr: -0.47012802958488464\n",
      "nll: 69068.96875\n",
      "Loss: 68833.90625\n",
      "Neg bpr: -0.4720326364040375\n",
      "nll: 68936.046875\n",
      "Loss: 68700.03125\n",
      "Neg bpr: -0.4712145924568176\n",
      "nll: 69071.2109375\n",
      "Loss: 68835.6015625\n",
      "Neg bpr: -0.4727851450443268\n",
      "nll: 68742.9296875\n",
      "Loss: 68506.5390625\n",
      "Neg bpr: -0.47168055176734924\n",
      "nll: 68823.3828125\n",
      "Loss: 68587.5390625\n",
      "Neg bpr: -0.4731014370918274\n",
      "nll: 69231.5703125\n",
      "Loss: 68995.0234375\n",
      "Neg bpr: -0.47359979152679443\n",
      "nll: 69363.625\n",
      "Loss: 69126.828125\n",
      "Neg bpr: -0.4730219542980194\n",
      "nll: 68866.140625\n",
      "Loss: 68629.6328125\n",
      "Neg bpr: -0.47250548005104065\n",
      "nll: 68526.6484375\n",
      "Loss: 68290.3984375\n",
      "Neg bpr: -0.4732121229171753\n",
      "nll: 67586.2578125\n",
      "Loss: 67349.6484375\n",
      "Neg bpr: -0.4735866189002991\n",
      "nll: 65672.28125\n",
      "Loss: 65435.48828125\n",
      "Neg bpr: -0.474618136882782\n",
      "nll: 64445.671875\n",
      "Loss: 64208.36328125\n",
      "Neg bpr: -0.4749329090118408\n",
      "nll: 63800.2734375\n",
      "Loss: 63562.80859375\n",
      "Neg bpr: -0.4747387766838074\n",
      "nll: 63226.171875\n",
      "Loss: 62988.80078125\n",
      "Neg bpr: -0.47413766384124756\n",
      "nll: 62543.8671875\n",
      "Loss: 62306.796875\n",
      "Neg bpr: -0.4737306237220764\n",
      "nll: 62039.55078125\n",
      "Loss: 61802.68359375\n",
      "Neg bpr: -0.4763774573802948\n",
      "nll: 61118.515625\n",
      "Loss: 60880.328125\n",
      "Neg bpr: -0.4761196970939636\n",
      "nll: 59960.203125\n",
      "Loss: 59722.14453125\n",
      "Neg bpr: -0.4759330153465271\n",
      "nll: 58801.7109375\n",
      "Loss: 58563.74609375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1000):\n",
    "    mix_model = model()\n",
    "    \n",
    "    y_sample_TMS = mix_model.sample((train_T, M_score_func))\n",
    "    y_sample_action_TMS = mix_model.sample((train_T, M_action))\n",
    "\n",
    "    ratio_rating_TMS = y_sample_action_TMS/y_sample_action_TMS.sum(dim=-1, keepdim=True)\n",
    "    ratio_rating_TS =  ratio_rating_TMS.mean(dim=1)\n",
    "    ratio_rating_TS.requires_grad_(True)\n",
    "\n",
    "    #pred_y_TS = torch.mean(y_sample_action_TMS, dim=1)\n",
    "    #pred_y_TS.requires_grad_(True)\n",
    "\n",
    "    def get_log_probs_baked(param):\n",
    "        distribution = model.build_from_single_tensor(param)\n",
    "        log_probs_TMS = distribution.log_prob(y_sample_TMS)\n",
    "\n",
    "        return log_probs_TMS\n",
    "\n",
    "    jac_TMSP = torch.autograd.functional.jacobian(get_log_probs_baked, (model.params_to_single_tensor()), strategy='forward-mode', vectorize=True)\n",
    "\n",
    "    score_func_estimator_TMSP = jac_TMSP * ratio_rating_TMS.unsqueeze(-1)\n",
    "    score_func_estimator_TSP = score_func_estimator_TMSP.mean(dim=1)    \n",
    "\n",
    "    # get gradient of negative bpr_t  with respect to ratio rating_TS\n",
    "    positive_bpr_T = torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS), K=4, perturbed_top_K_func=perturbed_top_K_func)\n",
    "    negative_bpr = torch.mean(-positive_bpr_T)\n",
    "    \n",
    "    nll = torch.sum(-mix_model.log_prob( torch.tensor(train_y_TS)))\n",
    "\n",
    "    print(f'Neg bpr: {negative_bpr}')\n",
    "    print(f'nll: {nll}')\n",
    "\n",
    "    loss = 500*negative_bpr + nll\n",
    "    print(f'Loss: {loss}')\n",
    "    losses.append(loss)\n",
    "    bprs.append(negative_bpr)\n",
    "    nlls.append(nll)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    loss_grad_TS = ratio_rating_TS.grad\n",
    "\n",
    "    gradient_TSP = score_func_estimator_TSP * torch.unsqueeze(loss_grad_TS, -1)\n",
    "    gradient_P = torch.sum(gradient_TSP, dim=[0,1])\n",
    "\n",
    "    gradient_tuple = model.single_tensor_to_params(gradient_P)\n",
    "\n",
    "    for param, gradient in zip(model.parameters(), gradient_tuple):\n",
    "        param.grad = gradient\n",
    "    optimizer.step()\n",
    "        \n",
    "    #model.update_params(model.params_to_single_tensor() - step_size * gradient_P)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 200, 12])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample_TMS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_single_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_to_single_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_sample_TMS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/mixture_same_family.py:161\u001b[0m, in \u001b[0;36mMixtureSameFamily.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad(x)\n\u001b[1;32m    163\u001b[0m     log_prob_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_distribution\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# [S, B, k]\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/distribution.py:310\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/constraints.py:400\u001b[0m, in \u001b[0;36m_Interval.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_bound\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m) \u001b[38;5;241m&\u001b[39m (value \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupper_bound)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    " model.build_from_single_tensor(model.params_to_single_tensor()).log_prob(y_sample_TMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_single_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_to_single_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_sample_TMS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/mixture_same_family.py:161\u001b[0m, in \u001b[0;36mMixtureSameFamily.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad(x)\n\u001b[1;32m    163\u001b[0m     log_prob_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_distribution\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# [S, B, k]\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/distribution.py:310\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/constraints.py:400\u001b[0m, in \u001b[0;36m_Interval.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_bound\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m) \u001b[38;5;241m&\u001b[39m (value \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupper_bound)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    " model.build_from_single_tensor(model.params_to_single_tensor()).log_prob(y_sample_TMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution= model.build_from_single_tensor(model.params_to_single_tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_score_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/mixture_same_family.py:161\u001b[0m, in \u001b[0;36mMixtureSameFamily.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad(x)\n\u001b[1;32m    163\u001b[0m     log_prob_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_distribution\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# [S, B, k]\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/distribution.py:310\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m support \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43msupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m     )\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/distributions/constraints.py:400\u001b[0m, in \u001b[0;36m_Interval.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_bound\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m) \u001b[38;5;241m&\u001b[39m (value \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupper_bound)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "distribution.log_prob(distribution.sample((300, M_score_func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0963, 0.0965, 0.0968,  ..., 0.0605, 0.0558, 0.0600],\n",
       "        [0.1012, 0.1004, 0.1007,  ..., 0.0532, 0.0384, 0.0672],\n",
       "        [0.0903, 0.0905, 0.0902,  ..., 0.0771, 0.0702, 0.0654],\n",
       "        ...,\n",
       "        [0.1042, 0.1037, 0.1038,  ..., 0.0390, 0.0443, 0.0439],\n",
       "        [0.0997, 0.0997, 0.0996,  ..., 0.0672, 0.0556, 0.0592],\n",
       "        [0.0946, 0.0949, 0.0946,  ..., 0.0579, 0.0533, 0.0592]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_model = model()\n",
    "\n",
    "y_sample_TMS = mix_model.sample((train_T, M_score_func))\n",
    "y_sample_action_TMS = mix_model.sample((train_T, M_action))\n",
    "\n",
    "ratio_rating_TMS = y_sample_action_TMS/y_sample_action_TMS.sum(dim=-1, keepdim=True)\n",
    "ratio_rating_TS =  ratio_rating_TMS.mean(dim=1)\n",
    "ratio_rating_TS.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0979, 0.0979, 0.0979, 0.0979, 0.0935, 0.0929, 0.0932, 0.0932, 0.0599,\n",
       "        0.0588, 0.0582, 0.0589], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(ratio_rating_TS, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 12])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_rating_TS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4100, 0.4620, 0.4440, 0.4140, 0.3800, 0.4360, 0.3800, 0.3980, 0.1840,\n",
       "         0.1760, 0.1680, 0.1480]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_top_K_func(torch.mean(ratio_rating_TS, dim=0, keepdim=True)).sum(dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[3, 2],\n",
       "        [6, 5]]),\n",
       "indices=tensor([[2, 1],\n",
       "        [0, 1]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.tensor([[1,2,3],[6,5,4]]), k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_top_K_func(torch.tensor([[1,2,3,4,5],[6,5,4,3,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5286, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch_bpr_uncurried(ratio_rating_TS, torch.tensor(train_y_TS), K=4, perturbed_top_K_func=perturbed_top_K_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5899)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(deterministic_bpr(ratio_rating_TS, torch.tensor(train_y_TS), K=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_topk = torch.topk(torch.tensor(train_y_TS), K)\n",
    "pred_value_at_topk  = torch.gather(ratio_rating_TS, 1, true_topk.indices)\n",
    "numerator = torch.sum(pred_value_at_topk, dim=-1)\n",
    "denominator = torch.sum(true_topk.values, dim=-1)\n",
    "bpr = numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([130.,  31.,  37.,  37.,  34.,  34.,  31.,  34.,  34.,  34.,  37.,  37.,\n",
       "         40.,  37.,  37.,  37., 130., 124.,  34.,  37.,  40., 124.,  37., 130.,\n",
       "        130.,  34., 130., 130., 130.,  37., 130., 130., 130.,  37.,  37., 124.,\n",
       "         34., 121.,  34.,  37.,  40.,  40.,  31.,  37.,  31.,  37.,  37., 130.,\n",
       "         40.,  37., 130.,  37.,  40.,  37.,  34., 127., 124., 127.,  34.,  37.,\n",
       "         34.,  37.,  40.,  40., 130.,  37.,  40.,  37.,  40.,  31.,  37., 220.,\n",
       "         37.,  37., 130.,  37.,  31., 220.,  40.,  40., 220.,  31.,  37.,  34.,\n",
       "         37.,  37.,  31.,  40.,  37., 130.,  34.,  40., 130.,  34., 220.,  37.,\n",
       "        127.,  37.,  40.,  40., 130.,  37.,  37., 130.,  37., 130.,  37.,  37.,\n",
       "        130., 124., 220.,  31., 130., 127.,  34.,  37.,  40.,  34.,  34.,  40.,\n",
       "         31., 130.,  37.,  37.,  40.,  40., 127.,  31.,  34., 220.,  37.,  37.,\n",
       "         34.,  37.,  40.,  40.,  34., 130., 130.,  37.,  40.,  37.,  37.,  37.,\n",
       "         40.,  37., 130.,  37., 124.,  40.,  37., 130.,  37.,  31.,  37., 130.,\n",
       "         31.,  37., 130.,  37., 130.,  34.,  31., 127.,  40.,  37.,  40., 130.,\n",
       "        130.,  34.,  40.,  40., 130.,  34., 130.,  34.,  37., 130.,  34.,  34.,\n",
       "         40., 130., 130.,  40., 130.,  34.,  37.,  37.,  40.,  31.,  37.,  37.,\n",
       "         37., 130.,  34.,  37.,  40., 130.,  37., 130.,  40.,  34., 127., 220.,\n",
       "        127.,  34.,  37.,  40., 130.,  40., 130.,  37.,  31.,  37., 130.,  34.,\n",
       "         34.,  37., 130.,  37., 130.,  34., 130.,  37., 130.,  37., 130.,  34.,\n",
       "         34.,  40., 130.,  40., 130.,  37., 130.,  31., 220.,  37.,  31., 127.,\n",
       "        124., 130.,  28.,  34., 130.,  37.,  34.,  37.,  31.,  40.,  37.,  37.,\n",
       "         37.,  34.,  37.,  34., 130.,  34.,  37., 127.,  37., 130., 130.,  37.,\n",
       "         34.,  34., 130.,  37.,  40.,  40.,  40.,  34.,  34., 130., 121.,  34.,\n",
       "         37., 124., 130.,  40.,  37.,  34., 130., 130.,  40., 130., 130., 127.,\n",
       "        130., 127., 130.,  37.,  37.,  40.,  37.,  37.,  34., 127.,  37.,  40.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0558, 0.0907, 0.0929, 0.0958],\n",
       "        [0.0937, 0.1004, 0.1012, 0.1007],\n",
       "        [0.0848, 0.0831, 0.0919, 0.0905],\n",
       "        ...,\n",
       "        [0.0439, 0.0996, 0.0986, 0.1037],\n",
       "        [0.0942, 0.0815, 0.0952, 0.0997],\n",
       "        [0.0937, 0.1033, 0.0858, 0.0900]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_value_at_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0963, 0.0965, 0.0968, 0.0965, 0.0907, 0.0939, 0.0929, 0.0958, 0.0644,\n",
       "        0.0605, 0.0558, 0.0600], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_rating_TS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.,   7.,   7.,   7.,  10.,   0.,  10.,  10.,   0.,   0., 100.,\n",
       "         0.], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_TS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
