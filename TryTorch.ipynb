{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:40:54.455675: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-19 10:40:54.509747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 10:40:54.509777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 10:40:54.510820: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 10:40:54.518904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 10:41:01.568098: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "# use torch backend\n",
    "import torch\n",
    "from functorch import jacrev, jacfwd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution Settings\n",
    "scale = 1\n",
    "mean = 0\n",
    "\n",
    "normal_dist = scipy.stats.norm(loc=mean, scale=scale)\n",
    "\n",
    "# Number of independent datapoints \n",
    "T = 1000\n",
    "\n",
    "seed=360\n",
    "rng = np.random.RandomState(seed)\n",
    "y_T = torch.Tensor(normal_dist.rvs(size=T, random_state=rng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=100000\n",
    "param = torch.tensor([1.], requires_grad=True)\n",
    "param_sized = param*torch.ones(T)\n",
    "distribution = torch.distributions.Normal(loc=param_sized, scale=1)\n",
    "samples_MT = distribution.sample(sample_shape=(M,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_MT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_log_probs(param, samples_MT):\n",
    "    param_sized = param*torch.ones(T)\n",
    "    distribution = torch.distributions.Normal(loc=param_sized, scale=1)\n",
    "    log_probs_MT = distribution.log_prob(samples_MT)\n",
    "\n",
    "    return log_probs_MT\n",
    "\n",
    "def get_log_probs_baked(param):\n",
    "    param_sized = param*torch.ones(T)\n",
    "    distribution = torch.distributions.Normal(loc=param_sized, scale=1)\n",
    "    log_probs_MT = distribution.log_prob(samples_MT)\n",
    "\n",
    "    return log_probs_MT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = torch.autograd.functional.jacobian(get_log_probs, (param, samples_MT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.13 s ± 159 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# use time magic with 3 runs \n",
    "%timeit jac = torch.autograd.functional.jacobian(get_log_probs, (param, samples_MT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.22 s ± 81.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# use time magic with 3 runs \n",
    "%timeit jac = torch.autograd.functional.jacobian(get_log_probs_baked, (param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 40000000400000000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use time magic with 3 runs \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjac = torch.autograd.functional.jacobian(get_log_probs, (param, samples_MT), strategy=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforward-mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, vectorize=True)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2456\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2456\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2459\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/IPython/core/magics/execution.py:1185\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1184\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[0;32m-> 1185\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/IPython/core/magics/execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:668\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_graph:\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.autograd.functional.jacobian: `create_graph=True` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand `strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` are not supported together (yet). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either set `create_graph=False` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`strategy=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreverse-mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jacfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    671\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m _as_tuple(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:520\u001b[0m, in \u001b[0;36m_jacfwd\u001b[0;34m(func, inputs, strict, vectorize)\u001b[0m\n\u001b[1;32m    517\u001b[0m input_numels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Step 1: Prepare tangents\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m tangents \u001b[38;5;241m=\u001b[39m \u001b[43m_construct_standard_basis_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_numels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# Step 2: Compute vmap over computation with dual tensors\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjvp\u001b[39m(tangents):\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:493\u001b[0m, in \u001b[0;36m_construct_standard_basis_for\u001b[0;34m(tensors, tensor_numels)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    492\u001b[0m total_numel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(tensor_numels)\n\u001b[0;32m--> 493\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    494\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mnew_zeros(total_numel, tensor_numel)\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tensor, tensor_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, tensor_numels)\n\u001b[1;32m    496\u001b[0m )\n\u001b[1;32m    497\u001b[0m diag_start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, tensor_numels):\n",
      "File \u001b[0;32m/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/pip_k3/lib/python3.11/site-packages/torch/autograd/functional.py:494\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensors) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    492\u001b[0m total_numel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(tensor_numels)\n\u001b[1;32m    493\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 494\u001b[0m     \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_numel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_numel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tensor, tensor_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, tensor_numels)\n\u001b[1;32m    496\u001b[0m )\n\u001b[1;32m    497\u001b[0m diag_start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, tensor_numels):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 40000000400000000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# use time magic with 3 runs \n",
    "%timeit jac = torch.autograd.functional.jacobian(get_log_probs, (param, samples_MT), strategy='forward-mode', vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14 s ± 52.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# use time magic with 3 runs \n",
    "%timeit jac = torch.autograd.functional.jacobian(get_log_probs_baked, (param), strategy='forward-mode', vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " jac = torch.autograd.functional.jacobian(get_log_probs_baked, (param), strategy='forward-mode', vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   1.,   0.,   1.,   0.,   0.,   0.,   0.,   1.,   3.,   1.,\n",
       "          2.,   8.,   6.,   9.,   7.,   9.,  10.,  24.,  22.,  29.,  21.,\n",
       "         30.,  33.,  37.,  59.,  54.,  69.,  70.,  72.,  96., 110., 124.,\n",
       "        156., 156., 183., 184., 233., 193., 225., 245., 224., 265., 290.,\n",
       "        279., 299., 294., 305., 294., 325., 306., 281., 301., 322., 275.,\n",
       "        292., 288., 267., 272., 247., 220., 218., 190., 198., 145., 150.,\n",
       "        119., 134.,  95.,  82.,  80.,  72.,  62.,  50.,  42.,  40.,  27.,\n",
       "         33.,  20.,  24.,  24.,  11.,  14.,   5.,   9.,   4.,   6.,   5.,\n",
       "          3.,   1.,   1.,   3.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.]),\n",
       " array([-3.92970634, -3.85055661, -3.77140689, -3.6922574 , -3.61310768,\n",
       "        -3.53395796, -3.45480824, -3.37565851, -3.29650903, -3.2173593 ,\n",
       "        -3.13820958, -3.05905986, -2.97991014, -2.90076041, -2.82161093,\n",
       "        -2.7424612 , -2.66331148, -2.58416176, -2.50501204, -2.42586255,\n",
       "        -2.34671283, -2.2675631 , -2.18841338, -2.10926366, -2.03011417,\n",
       "        -1.95096445, -1.87181473, -1.792665  , -1.7135154 , -1.63436568,\n",
       "        -1.55521595, -1.47606635, -1.39691663, -1.31776702, -1.2386173 ,\n",
       "        -1.15946758, -1.08031797, -1.00116825, -0.92201859, -0.84286892,\n",
       "        -0.7637192 , -0.68456954, -0.60541987, -0.52627021, -0.44712052,\n",
       "        -0.36797082, -0.28882116, -0.20967148, -0.1305218 , -0.05137213,\n",
       "         0.02777755,  0.10692723,  0.18607691,  0.26522657,  0.34437627,\n",
       "         0.42352593,  0.50267559,  0.58182532,  0.66097498,  0.74012464,\n",
       "         0.81927431,  0.89842403,  0.97757369,  1.05672336,  1.13587308,\n",
       "         1.21502268,  1.29417241,  1.37332213,  1.45247173,  1.53162146,\n",
       "         1.61077106,  1.68992078,  1.76907051,  1.84822011,  1.92736983,\n",
       "         2.00651956,  2.08566928,  2.16481876,  2.24396849,  2.32311821,\n",
       "         2.40226793,  2.48141766,  2.56056714,  2.63971686,  2.71886659,\n",
       "         2.79801631,  2.87716603,  2.95631552,  3.03546524,  3.11461496,\n",
       "         3.19376469,  3.27291441,  3.35206413,  3.43121362,  3.51036334,\n",
       "         3.58951306,  3.66866279,  3.74781251,  3.82696199,  3.90611172,\n",
       "         3.98526144]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(jac[1].flatten().detach().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distribution = torch.distributions.Normal(loc=yhat, scale=scale,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch layer that just has 1 variable and returns that variable\n",
    "# this is equivalent to a linear regression model with no features\n",
    "linear = torch.nn.Linear(0,1, bias=True)\n",
    "\n",
    "# call linear with a batchsize of 1000\n",
    "yhat = linear(torch.zeros(1000,0, requires_grad=True))\n",
    "distribution = torch.distributions.Normal(loc=yhat, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set linear intercept to 1\n",
    "linear.bias.data = torch.tensor([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62829/2382041253.py:8: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss_T = torch.nn.functional.mse_loss(expected_action_T, y_T,)\n"
     ]
    }
   ],
   "source": [
    "samples_MT = distribution.sample(sample_shape=(M,))\n",
    "action_samples_MT = distribution.sample(sample_shape=(M,))\n",
    "action_samples_MT.requires_grad_(True)\n",
    "expected_action_T = torch.mean(action_samples_MT, dim=0, )\n",
    "log_probs_MT = distribution.log_prob(samples_MT)\n",
    "\n",
    "# loss is MSE between y_T and expected action\n",
    "loss_T = torch.nn.functional.mse_loss(expected_action_T, y_T,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = torch.autograd.functional.jacobian(get_log_probs, (samples_MT, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000, 1, 1000, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_k3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
